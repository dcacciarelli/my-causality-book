{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "320b85bf-d38f-4ba1-97a5-1d6df42c0c09",
   "metadata": {},
   "source": [
    "# Double Machine Learning (DML)\n",
    "\n",
    "Double Machine Learning (DML) is a framework, initially proposed by Chernozhukov et al. {cite}`chernozhukov2018double`, for estimating causal effects when many confounding variables are present. It combines machine learning techniques with econometric methods to control for these confounders and obtain unbiased estimates of treatment effects. For example, let's assume we are interested in unveiling the effect of wind power (WP) production or solar power (SP) production on electricity prices. In this case, WP and SP production would be our **treatment variables**, while the electricity price would be our **response**. We know that these factors have an effect in reducing prices due to their low marginal costs. However, we also know that there are many other factors affecting electricity prices (e.g., demand, gas prices, macroeconomic trends). These are the **confounders**. We can also assume that some these confounders have an effect both on our treatment and our response variable. For example, the season we are in or the specific hour of the day will certainly affect the generation of SP, but will also have an effect on the demand (hence, the prices) because of the well-known daily and season consumption profiles. Ignoring the effect of these confounders might lead to biased estimates. \n",
    "\n",
    "With DML, we are trying to isolate the effect of the treatment variables on the response. This framework assumes that the response $y$ (e.g., the prices) is a function of the treatment $w$ and other confounding variables $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = g(w, x) + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "where $g$ is an arbitrary function (linear or nonlinear) and $\\epsilon$ is the error term.\n",
    "\n",
    "Similarly, since we assumed that the treatment is also affected by other confounding variables, we have that $w$ can be modeled as a function of $x$:\n",
    "\n",
    "\\begin{equation}\n",
    "    w = m(x) + \\nu\n",
    "\\end{equation}\n",
    "\n",
    "where $m$ is an arbitrary function (linear or nonlinear) and $\\nu$ is the error term.\n",
    "\n",
    "Now, the DML framework involves two main stages:\n",
    "1. **Nuisance parameter estimation**: use a machine learning model to estimate the functions $\\hat{g}(w, x)$ and $\\hat{m}(x)$.\n",
    "2. **Orthogonalization and estimation**: use the estimated functions to \"remove\" the effect of the confounding variables from both $w$ and $y$. Then, we estimate the causal effects by regressing the residuals of the response on the residuals of the treatment.\n",
    "\n",
    "The **key intuition** is that if we remove the effect of other confounders from the tratment and the response, the variation that remains in the residuals is only due to the treatment itself. It should be noted that this approach assumes we already know the causal graph, and that there are no omitted variables.\n",
    "\n",
    "\n",
    "## The Partially Linear Case\n",
    "\n",
    "For simplicity, we now consider a partially linear case where the relationship between the outcome $y$ and the treatment $w$ can be expressed linearly, while allowing for a potentially complex, nonlinear relationship between the confounders $x$ and both the treatment and outcome.\n",
    "\n",
    "In this case, the model is specified as follows:\n",
    "\n",
    "\\begin{equation}\n",
    "    y = \\beta w + g(x) + \\epsilon\n",
    "\\end{equation}\n",
    "\n",
    "\\begin{equation}\n",
    "    w = m(x) + \\nu\n",
    "\\end{equation}\n",
    "\n",
    "Here:\n",
    "- $\\beta$ is the coefficient capturing the causal effect of the treatment $w$ on the outcome $y$. This is what we are trying to estimate!\n",
    "- $g(x)$ is an unknown function capturing the effect of the confounders $x$ on the outcome.\n",
    "- $m(x)$ is an unknown function capturing the effect of the confounders $x$ on the treatment.\n",
    "\n",
    "\n",
    "The **key steps** to implement the DML  in the partially linear case are:\n",
    "\n",
    "1. **Split the Data**: Randomly split the data into $K$ folds.\n",
    "2. **Train Predictive Models**:\n",
    "    - For each fold $k$ (where $k \\in \\{1, 2, ..., K\\}$):\n",
    "        1. **Treatment Model**: Train a machine learning model $\\hat{m}_{-k}(x)$ using $K-1$ folds to predict $w$ from $x$.\n",
    "        2. **Outcome Model**: Train a machine learning model $\\hat{g}_{-k}(x)$ using $K-1$ folds to predict $y$ from $x$.\n",
    "3. **Generate Residuals**:\n",
    "    - Use the models trained on $K-1$ folds to predict the held-out fold $k$.\n",
    "    - Compute the residuals for the treatment and outcome models:\n",
    "        \\begin{equation}\n",
    "            \\hat{V}_W = W - \\hat{W}, \\quad \\hat{V}_Y = Y - \\hat{Y}\n",
    "        \\end{equation}\n",
    "4. **Regress Residuals**:\n",
    "    - Regress the residualized outcome $\\hat{V}_Y$ on the residualized treatment $\\hat{V}_W$ to estimate the causal effect $\\beta$:\n",
    "        \\begin{equation}\n",
    "        \\hat{\\beta}_k = \\text{coef}\\left( \\hat{V}_Y \\sim \\hat{V}_W \\right)\n",
    "        \\end{equation}\n",
    "5. **Average Estimates**:\n",
    "    - Repeat steps 2-4 for each fold and average the resulting $K$ estimates to obtain the final causal estimate:\n",
    "        \\begin{equation}\n",
    "            \\hat{\\beta} = \\frac{1}{K} \\sum_{k=1}^{K} \\hat{\\beta}_k\n",
    "        \\end{equation}\n",
    "6. **Robustness**:\n",
    "    - For more robustness with respect to random partitioning in finite samples, repeat the algorithm multiple times (e.g., 100 times) with different random splits and report the median estimate.\n",
    "\n",
    "This algorithm ensures that the estimation of the treatment effect is orthogonal to the nuisance parameters (the confounders), thereby removing bias due to overfitting and ensuring that the estimated treatment effect is unbiased.\n",
    "\n",
    "The **key advantages** of DML in the partially linear case are:\n",
    "1. **Flexibility**: allows the use of flexible machine learning models to estimate complex, nonlinear relationships between confounders and the treatment/outcome.\n",
    "2. **Bias Reduction**: the orthogonalization step ensures that the estimation of the causal effect is unbiased by the confounders.\n",
    "3. **Robustness**: cross-fitting and multiple repetitions provide robustness against overfitting and ensure stable estimates.\n",
    "\n",
    "\n",
    "## Electricity example\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
