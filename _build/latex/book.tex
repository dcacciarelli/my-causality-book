%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{booktabs}{sphinx}
\PassOptionsToPackage{colorrows}{sphinx}

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}

\addto\captionsenglish{\renewcommand{\contentsname}{Crash course on Stats and ML}}

\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Applied Causal Inference with Examples from Electricity Markets}
\date{Jul 07, 2024}
\release{}
\author{Davide Cacciarelli}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}


\sphinxAtStartPar
This work\sphinxhyphen{}in\sphinxhyphen{}progress book is designed to provide comprehensive tutorials on causal inference methodologies. This book explains key concepts in this rapidly growing research area and showcases potential applications within electricity markets.

\sphinxAtStartPar
\sphinxstylestrong{Prerequisites}

\sphinxAtStartPar
To get the most out of this book, it is recommended that readers have a basic understanding of:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistics and probability}: familiarity with basic statistical concepts and probability theory.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Python programming}: basic knowledge of Python and possibly experience with libraries such as NumPy, pandas, statsmodels, and scikit\sphinxhyphen{}learn.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Machine learning}: an understanding of linear regression models, and an overview of machine learning concepts.

\end{itemize}

\sphinxAtStartPar
If you feel like you need a refresher, at the beginning of this book you can find a \sphinxstylestrong{crash course} to review these essential concepts. These chapters are designed to help you brush up on the foundational knowledge required to fully engage with the material in this book. However, if you are already familiar with these concepts or prefer to go directly to the applied causal inference part, feel free to skip those chapters.

\sphinxAtStartPar
\sphinxstylestrong{Learning objectives}

\sphinxAtStartPar
By the end of this book, readers will be able to:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Understand and apply causal inference techniques to analyse relationships between variables.

\item {} 
\sphinxAtStartPar
Use directed acyclic graphs (DAGs) to represent and reason about causal structures.

\item {} 
\sphinxAtStartPar
Implement and interpret various causal discovery algorithms.

\item {} 
\sphinxAtStartPar
Estimate causal effects using methods like instrumental variables, double machine learning, and difference\sphinxhyphen{}in\sphinxhyphen{}differences.

\item {} 
\sphinxAtStartPar
Interpret results from complex machine learning models.

\item {} 
\sphinxAtStartPar
Design experiments to gather data that supports causal analysis.

\end{itemize}

\begin{sphinxadmonition}{note}{Note}

\sphinxAtStartPar
The examples are provided in \sphinxstylestrong{Python}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Each chapter is designed to be as self\sphinxhyphen{}contained as possible, enabling you to focus on specific topics without needing prior chapters.

\item {} 
\sphinxAtStartPar
Chapters can be run independently, allowing you to generate data and apply statistical methods on their own.

\item {} 
\sphinxAtStartPar
Each chapter is available for download as a \sphinxstylestrong{Jupyter Notebook}, facilitating hands\sphinxhyphen{}on learning and enabling you to reproduce the results effortlessly.

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{References}

\sphinxAtStartPar
This book is highly applied, aimed at providing essential intuitions and practical examples of how to apply causal inference methods to real\sphinxhyphen{}world problems. For additional theoretical and technical explanations, we suggest:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxhref{https://dl.acm.org/doi/book/10.5555/1642718}{Causality: Models, Reasoning and Inference}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://mitpress.mit.edu/9780262037310/elements-of-causal-inference/}{Elements of Causal Inference}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://link.springer.com/book/10.1007/978-4-431-55784-5}{Statistical Causal Discovery: LiNGAM Approach}

\item {} 
\sphinxAtStartPar
\sphinxhref{https://www.bradyneal.com/causal-inference-course}{Introduction to Causal Inference from a Machine Learning Perspective}

\end{itemize}
\end{sphinxadmonition}

\begin{sphinxadmonition}{note}{\protect\(~\protect\)}

\sphinxAtStartPar
Built with \sphinxhref{https://beta.jupyterbook.org/intro.html}{Jupyter Book
2.0} tool set, as part of the
\sphinxhref{https://ebp.jupyterbook.org/en/latest/}{ExecutableBookProject}.
\end{sphinxadmonition}

\sphinxAtStartPar
If you have any inquiries, please contact {\hyperref[\detokenize{intro:d.cacciarelli@imperial.ac.uk}]{\sphinxcrossref{\DUrole{xref,myst}{d.cacciarelli@imperial.ac.uk}}}}.

\sphinxstepscope


\part{Crash course on Stats and ML}

\sphinxstepscope


\chapter{Probability Theory and Statistics}
\label{\detokenize{notebooks/review_stats:probability-theory-and-statistics}}\label{\detokenize{notebooks/review_stats::doc}}

\section{Basic concepts}
\label{\detokenize{notebooks/review_stats:basic-concepts}}
\sphinxAtStartPar
A \sphinxstylestrong{random variable} is a fundamental concept in probability and statistics. It represents a variable whose values are determined by the outcomes of a random phenomenon. A \sphinxstylestrong{discrete random variable} can take on a finite or countable number of distinct values. For example, the roll of a fair six\sphinxhyphen{}sided die is a discrete random variable, as it can result in one of six possible outcomes (1 through 6). The probability distribution of this random variable is uniform, meaning each outcome has an equal probability of occurring.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Simulate rolling a die 10000 times}
\PYG{n}{rolls} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10000}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the results}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{rolls}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Die Face}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Histogram of Die Rolls}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{473824e3ebc6a1fd78bed35d20c9081c43637d9c03a42a9e1a1271f466f2ae5a}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
A \sphinxstylestrong{continuous random variable} can take any value within a given range. The normal distribution (or Gaussian distribution) is a common continuous distribution characterized by its bell\sphinxhyphen{}shaped curve. It is defined by its mean (expected value) and standard deviation (a measure of variability).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}

\PYG{c+c1}{\PYGZsh{} Generate data from a normal distribution}
\PYG{n}{mean} \PYG{o}{=} \PYG{l+m+mi}{0}
\PYG{n}{std\PYGZus{}dev} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{mean}\PYG{p}{,} \PYG{n}{std\PYGZus{}dev}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the results}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the normal distribution}
\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{xmin}\PYG{p}{,} \PYG{n}{xmax}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{p} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{norm}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{mean}\PYG{p}{,} \PYG{n}{std\PYGZus{}dev}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{p}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Histogram of Normal Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{ea659595f4c8c7e34cdf139ac15fa2ca8f7aba7f9ae0174d9a72fe0a93f1440d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
A \sphinxstylestrong{random process} (or stochastic process) is a collection of random variables indexed by time or another variable, used to model systems that evolve randomly over time or space. A random process is a function that assigns a random variable to each point in a time or space domain (examples include stock market prices, weather patterns, and noise signals in electrical engineering). We can distinguish between discrete\sphinxhyphen{}time processes (whose indices are countable), and continuous\sphinxhyphen{}time processes (whose indices are an interval). We can also distinguish between stationary processes (whose statistical properties are constant over time), and nonstationary processes.

\sphinxAtStartPar
A random walk is a simple example of a discrete\sphinxhyphen{}time random process, where each step is determined randomly, leading to a path that evolves over time.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate a random walk}
\PYG{n}{n\PYGZus{}steps} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{steps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}steps}\PYG{p}{)}
\PYG{n}{random\PYGZus{}walk} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{steps}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the random walk}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{random\PYGZus{}walk}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time Step}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Position}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Walk}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{416a7cedcc526ce2c6d8b5a6d731bf4b480ee677c4b176c7ec83ba194b9cdf63}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Statistical indepencence} refers to the lack of a relationship between two or more random variables. More formally, we can distinguish between two types of statistical independence:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Marginal independence} refers to the lack of a relationship between two random variables, without considering the effect of any other variables. Mathematically, two random variables \(X\) and \(Y\) are marginally independent (\(X \perp Y\)) if their joint probability distribution can be expressed as the product of their marginal probability
distributions, as in

\end{enumerate}
\label{equation:notebooks/review_stats:89e7ea3d-e4be-4d91-acbd-b29aa5e8c640}\begin{equation}
    P(X, Y) = P(X)P(Y)
\end{equation}
\sphinxAtStartPar
To illustrate this concept, let’s generate two independent random variables \(X\) and \(Y\) from a normal distribution with mean 0 and standard deviation 1. We then plot their joint distribution using a scatter plot. If \(X\) and \(Y\) are truly independent, the scatter plot will show no discernible pattern.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Generate two independent random variables}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{)}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{10000}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot their joint distribution}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatter Plot of Marginally Independent Variables X and Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{ebed95ae1ef18122b8a0b8e87113bbb8e6d99ce26f58b53d71a87483bbf08605}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\setcounter{enumi}{1}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conditional independence} refers to the lack of a relationship between two random variables, given the value of one or more other variables. Mathematically, two random variables \(X\) and \(Y\) are conditionally independent given a variable (\(X \perp Y | Z\)) if their conditional probability distribution satisfies

\end{enumerate}
\label{equation:notebooks/review_stats:6667aae2-07a3-4efa-b046-8f22959f23a2}\begin{equation}
    P(X, Y | Z) = P(X | Z)P(Y | Z)
\end{equation}
\sphinxAtStartPar
To illustrate this, let’s generate three random variables \(X\), \(Y\), and \(Z\) such that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(X\) and \(Y\) are dependent on \(Z\)

\item {} 
\sphinxAtStartPar
\(X\) and \(Y\) are conditionally independent given \(Z\)

\end{itemize}

\sphinxAtStartPar
This is done by adding noise to \(Z\) to generate \(X\) and \(Y\).

\sphinxAtStartPar
We first plot the scatter plot of \(X\) and \(Y\) without considering \(Z\), which may show some correlation.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate three random variables such that X and Y are independent given Z}
\PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100000}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{Z} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100000}\PYG{p}{)}
\PYG{n}{Y} \PYG{o}{=} \PYG{n}{Z} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100000}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot X and Y}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatter Plot of Variables X and Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{ca5a58f3017161055dff1eb83d55f904ec1c04c2dbe667e31f6af72accbe099f}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, we plot \(X\) and \(Y\) for different ranges of \(Z\). If \(X\) and \(Y\) are conditionally independent given \(Z\), the scatter plots for different values of \(Z\) should show no pattern, demonstrating that knowing \(Z\) removes the dependence between \(X\) and \(Y\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Now, plot X and Y given Z}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{z\PYGZus{}value} \PYG{o+ow}{in} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{:}
    \PYG{n}{mask} \PYG{o}{=} \PYG{p}{(}\PYG{n}{Z} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{z\PYGZus{}value} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.1}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{Z} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{n}{z\PYGZus{}value} \PYG{o}{+} \PYG{l+m+mf}{0.1}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{mask}\PYG{p}{]}\PYG{p}{,} \PYG{n}{Y}\PYG{p}{[}\PYG{n}{mask}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z \PYGZti{} }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{z\PYGZus{}value}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Scatter Plot of X and Y given different values of Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{6690af5c95620c8d8a836e4b08d4441b6bdc9aba58865b9b71dd4dec11113eff}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Expectation, variance, and covariance}
\label{\detokenize{notebooks/review_stats:expectation-variance-and-covariance}}
\sphinxAtStartPar
The expectation (or expected value) of a continuous random variable \(X\) with probability density function \(p(x)\) is
\label{equation:notebooks/review_stats:2c141f97-f155-43a4-9b49-0af19049808c}\begin{equation}
    \mathbb{E}[X] = \int xp(x)dx
\end{equation}
\sphinxAtStartPar
while the expectation of a discrete random variable with probability mass function \(p(x)\) is
\label{equation:notebooks/review_stats:c1a1788c-d405-4f93-b0c1-c1c34836266c}\begin{equation}
    \mathbb{E}[X] = \sum_{x} xp(x)
\end{equation}
\sphinxAtStartPar
The expectation of any function of a random variable, \(f(X)\), is given by
\label{equation:notebooks/review_stats:738b6ec0-a1c6-4711-ab1d-1c15949e6e43}\begin{equation}
    \mathbb{E}[X] = \int f(x)p(x)dx
\end{equation}
\sphinxAtStartPar
The deviation or fluctuation of \(X\) from its expected value is \(X - \mathbb{E}[X]\). The variance of a random variable \(X\) measures the dispersion around its mean, and it is given by
\label{equation:notebooks/review_stats:977d5ab2-550b-436b-a61b-f3d2c091bca3}\begin{equation}
    \operatorname{Var}[X] = \mathbb{E}[(X - \mathbb{E}[X])^2]
\end{equation}
\sphinxAtStartPar
The covariance of two random variables \(X\) and \(Y\) measures the degree to which two random variables change together. If the variables tend to show similar behavior (they tend to be above or below their expected values together), the covariance is positive. If one variable tends to increase when the other decreases, the covariance is negative. It is given by
\label{equation:notebooks/review_stats:b99e717f-56eb-4812-b9dc-4721ef3d04f9}\begin{equation}
    \operatorname{Cov}[X,Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])]
\end{equation}
\sphinxAtStartPar
Some \sphinxstylestrong{algebraic properties} of expectation, variance, and covariance will be extremely useful in manipulating and deriving statistical quantities of interest:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity of expectations}:
\label{equation:notebooks/review_stats:8fe4bca9-b253-4d73-849a-14cb47a06cf3}\begin{equation}
        \mathbb{E}[aX+bY] = a\mathbb{E}[X] + b\mathbb{E}[Y]
    \end{equation}
\sphinxAtStartPar
Expectation is a linear operator. The expectation of a sum of random variables is the sum of their expectations, and the expectation of a scaled random variable is the scale factor times the expectation of the variable.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance identity}:
\label{equation:notebooks/review_stats:ee2ebad7-e82d-40cf-9ae8-41c0d7b3cbab}\begin{equation}
        \operatorname{Var}[X] = \mathbb{E}[(X-\mathbb{E}[X])^2] = \mathbb{E}[X^2]-(\mathbb{E}[X])^2
    \end{equation}
\sphinxAtStartPar
Expanding \(\mathbb{E}[(X-\mathbb{E}[X])^2]\) we get \(\mathbb{E}[X^2 -2X \mathbb{E}[X] + (\mathbb{E}[X])^2]\), which is equal to \(\mathbb{E}[X^2] -2\mathbb{E}[X \mathbb{E}[X]] + \mathbb{E}[(\mathbb{E}[X])^2]\). However, \(\mathbb{E}[X]\) is a constant (because it is the expected value of a random variable, it is not random anymore), so \(\mathbb{E}[\mathbb{E}[X]]\) is just \(\mathbb{E}[X]\). So that becomes \(\mathbb{E}[X^2] -2\mathbb{E}[X] \mathbb{E}[X] + (\mathbb{E}[X])^2 = \mathbb{E}[X^2] -2(\mathbb{E}[X])^2 + (\mathbb{E}[X])^2\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Covariance identity}:
\label{equation:notebooks/review_stats:e8172b3c-7140-448e-9480-21ffff2a670f}\begin{equation}
        \operatorname{Cov}[X,Y] = \mathbb{E}[(X - \mathbb{E}[X])(Y - \mathbb{E}[Y])] = \mathbb{E}[XY]-\mathbb{E}[X]\mathbb{E}[Y]
    \end{equation}
\sphinxAtStartPar
Expanding the product \((X - \mathbb{E}[X])(Y - \mathbb{E}[Y])\) we get \(X - X\mathbb{E}[Y] - Y\mathbb{E}[X] + \mathbb{E}[X]\mathbb{E}[Y]\). Taking the expectation  we get \(\mathbb{E}[XY] - \mathbb{E}[X\mathbb{E}[Y]] - \mathbb{E}[Y\mathbb{E}[X]] + \mathbb{E}[\mathbb{E}[X]\mathbb{E}[Y]]\). Because the expectation is constant, we get to \(\mathbb{E}[XY] - \mathbb{E}[X]\mathbb{E}[Y] - \mathbb{E}[Y]\mathbb{E}[X] + \mathbb{E}[X]\mathbb{E}[Y]\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Covariance is symmetric}:
\label{equation:notebooks/review_stats:be8a3c3e-419b-4641-9224-4c9d78abd9b9}\begin{equation}
        \operatorname{Cov}[X,Y] = \operatorname{Cov}[Y,X]
    \end{equation}
\sphinxAtStartPar
The direction of comparison does not matter for covariance, whether you measure how \(X\) varies with \(Y\) or \(Y\) with \(X\), the result is the same.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance is covariance with itself}:
\label{equation:notebooks/review_stats:9e126b23-1dc3-493b-a045-d9f48f476ac8}\begin{equation}
        \operatorname{Cov}[X,X] = \operatorname{Var}[X]
    \end{equation}
\sphinxAtStartPar
Covariance measures how two variables vary together, and variance is a special case where these two variables are the same.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance is not linear}:
\label{equation:notebooks/review_stats:11f3b112-bd2b-445e-b6f9-0d79baa40d9e}\begin{equation}
        \operatorname{Var}[aX + b] = a^2\operatorname{Var}[X]
    \end{equation}
\sphinxAtStartPar
The square in the variance formula leads to a squared scale factor when a random variable is scaled. The addition of a constant \(b\) does not affect variance, as variance measures dispersion around the mean, which is unaffected by constant shifts. To show why \(a\) becomes \(a^2\), we have \(\operatorname{Var}[aX] = \mathbb{E}[(aX-\mathbb{E}(aX)^2]\), which is equal to \(\mathbb{E}[a^2(X-\mathbb{E}(X)^2] = a^2\mathbb{E}[(X-\mathbb{E}(X)^2]\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Covariance is not linear}:
\label{equation:notebooks/review_stats:8261d268-1c8f-4719-a34b-22d4dcd94beb}\begin{equation}
        \operatorname{Cov}[aX + b,Y] = a\operatorname{Cov}[X,Y]
    \end{equation}
\sphinxAtStartPar
Scaling one variable in a covariance relationship scales the covariance itself but does not affect the relationship’s direction or absence (signified by zero covariance). The addition of a constant does not affect covariance, as it does not change how one variable varies with another.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance of a sum}:
\label{equation:notebooks/review_stats:c1014da1-d738-45f0-beb1-87c42e2eb0c3}\begin{equation}
        \operatorname{Var}[X+Y] = \operatorname{Var}[X] + \operatorname{Var}[Y] + 2\operatorname{Cov}[X,Y]
    \end{equation}
\sphinxAtStartPar
The variance of a sum includes the individual variances and an additional term to account for how the variables co\sphinxhyphen{}vary. This comes from expanding \(\mathbb{E}[(X + Y - \mathbb{E}[X+Y])^2]\) and using the linearity of expectations.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance of a large sum}:
\label{equation:notebooks/review_stats:5acf4256-2f97-4c11-a07c-0f8eaf9a46b2}\begin{equation}
        \operatorname{Var}\left[ \sum_{i=1}^{n}X_i \right] = \sum_{i=1}^{n}\sum_{j=1}^{n}\operatorname{Cov}[X_i,X_j] = \sum_{i=1}^{n}\operatorname{Var}[X_i] + 2\sum_{i=1}^{n-1}\sum_{j>i}\operatorname{Cov}[X_i,X_j]
    \end{equation}
\sphinxAtStartPar
The variance of a sum of multiple random variables includes both their individual variances and the covariance terms for every pair.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Law of total expectations}:
\label{equation:notebooks/review_stats:d0ecd719-f44d-4e4b-b4b2-bafa44a1ba9a}\begin{equation}
        \mathbb{E}[X] = \mathbb{E}[\mathbb{E}[X|Y]]
    \end{equation}
\sphinxAtStartPar
Suppose we have two random variables \(X\) and \(Y\). We want to express the expectation of \(X\) (a marginal expectation) in terms of its conditional expectation given \(Y\). his law states that the overall expectation of \(X\) can be found by taking the expectation of the conditional expectation of \(X\) given \(Y\). In practice, what we are doing is splitting the entire probability space into parts based on the values of \(Y\), calculating the expected value of \(X\) (this is \(\mathbb{E}[X|Y]\)), and then taking the expectation of these conditional expectations over the distribution of \(Y\). Imagine \(Y\) as categorizing or segmenting the probability space into different scenarios or groups. Within each group, you calculate the average value of \(X\) (this gives you \(\mathbb{E}[X|Y=y]\) for each \(y\)). Then, you average these averages over all possible groups (weighted by the probability of each group \(Y=y\), leading back to the overall average of \(X\). This law is particularly useful in scenarios where direct calculation of \(\mathbb{E}[X]\) is complex but where conditional expectations \(\mathbb{E}[X|Y]\) are simpler to compute.
This is the expected value (or mean) of \(X\) given a particular value of \(Y\). In many statistical models, especially in predictive modeling, this conditional mean can be thought of as a ``prediction’’ of  \(X\) based on the knowledge of \(Y\). For example, if \(Y\) represents a set of features or conditions, then \(\mathbb{E}[X|Y]\) is our best guess or prediction of \(X\) under those conditions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Law of total variance}:
\label{equation:notebooks/review_stats:aaade233-953b-400f-8cb4-bae64107d9f3}\begin{equation}
        \operatorname{Var}[X] = \operatorname{Var}[\mathbb{E}[X|Y]] + \mathbb{E}[\operatorname{Var}[X|Y]]
    \end{equation}
\sphinxAtStartPar
This law decomposes the total variance into two parts. The first part can be thought of as between\sphinxhyphen{}group variability and measures how much the conditional means vary as \(Y\) changes. The second term is the within\sphinxhyphen{}group variability and represents the average of the variances within each group defined by \(Y\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Independence implies zero covariance}:
\label{equation:notebooks/review_stats:7e6a6a65-406a-41d5-a241-65256706d3ba}\begin{equation}
        X \perp Y \rightarrow \operatorname{Cov}[X,Y] = 0
    \end{equation}
\sphinxAtStartPar
Independence between two variables means the occurrence of one does not affect the probability distribution of the other. This lack of influence translates mathematically to zero covariance. However, the converse is not necessarily true as zero covariance does not capture nonlinear dependencies.

\end{itemize}


\section{Convergence}
\label{\detokenize{notebooks/review_stats:convergence}}
\sphinxAtStartPar
The \sphinxstylestrong{law of large numbers (LLN)} states that, for a sequence of independent and identically distributed (i.i.d.) random variables \(X_1, X_2 \ldots, X_n\) each with expected value \(\mathbb{E}[X]\), the sample mean converges to the expected value as \(n\) approaches infinity
\label{equation:notebooks/review_stats:a641572a-2eef-483f-90df-5594df0af343}\begin{equation}
    \frac{1}{n}\sum_{i=1}^{n}X_i \rightarrow \mathbb{E}[X] \quad \text{ as } n \rightarrow \infty
\end{equation}
\sphinxAtStartPar
The \sphinxstylestrong{i.i.d. assumption} is a fundamental concept in probability theory and statistics, with significant implications. Independence implies that the occurrence of one event or the value of one variable does not influence the occurrence of another. In the context of random variables, \(X_1, X_2 \ldots, X_n\) being independent means the outcome of \(X_i\) provides no information about the outcome of \(X_j\), \(i\neq j\). Identically distributed means that each of the random variables has the same probability distribution. They do not need to take on the same value, but the rules governing their behavior (i.e., the likelihood of each outcome) are identical. In the context of supervised learning, the i.i.d. assumption assumes that the training and test data are independently drawn from the same underlying probability distribution. This enables the use of statistical tools and techniques, such as maximum likelihood estimation and hypothesis testing, which are based on the assumption of independent and identically distributed data. In real\sphinxhyphen{}world data, this assumption is often violated as data may be dependent and non\sphinxhyphen{}identically distributed due to distribution shifts across geography or time, sampling practices, or the presence of confounding variables and selection bias.

\sphinxAtStartPar
To illustrate the LLN, we can generate a sequence of independent and identically distributed (i.i.d.) random variables and observe how their sample mean converges to the expected value as the number of samples increases.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate a sequence of i.i.d. random variables}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute the cumulative mean}
\PYG{n}{cumulative\PYGZus{}mean} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)} \PYG{o}{/} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the cumulative mean}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{cumulative\PYGZus{}mean}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Expected Value (0)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Number of Samples}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Law of Large Numbers}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{cbe81718c45ca5836d9420e9005d081b9bfabab3968d74812d1c8a709b43e5ee}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The \sphinxstylestrong{central limit theorem (CLT)} states that if \(X_1, X_2, \ldots, X_n\) are i.i.d. random variables with an expected value \(\mathbb{E}[X]\) and a finite variance \(\operatorname{Var}[X]\), the distribution of the sample mean
\label{equation:notebooks/review_stats:bc7d4747-fb59-462c-a1d8-cd185f5168b1}\begin{equation}
    \bar{X}_n = \frac{1}{n} \sum_{i=1}^{n} X_i
\end{equation}
\sphinxAtStartPar
approaches a normal distribution as \(n \rightarrow \infty\). Specifically, the standardized form
\label{equation:notebooks/review_stats:7bfc7aa1-acc6-4465-83b7-15d04aa89a17}\begin{equation}
    \frac{\bar{X}_n - \mathbb{E}[X]}{\sqrt{\operatorname{Var}[X]/n}}
\end{equation}
\sphinxAtStartPar
approaches the standard normal distribution \(N(0, 1)\).

\sphinxAtStartPar
The motivation behind the CLT lies in its ability to provide a predictable and well\sphinxhyphen{}understood behavior (the normal distribution) for averages of random variables, regardless of the original distribution of these variables. This is particularly useful in practical scenarios such as statistical sampling and hypothesis testing, where it is often necessary to make inferences about population parameters. The intuition of the CLT is that as we increase the number of random variables in our sample, the peculiarities and individual randomness of each variable tend to cancel out. This leads to the emergence of the normal distribution, which is symmetric and centered around the mean. The CLT is powerful because it applies to a wide range of distributions, whether they are symmetric, skewed, or even arbitrary, as long as the variables are i.i.d. with a finite variance.


\section{Estimation: bias and variance}
\label{\detokenize{notebooks/review_stats:estimation-bias-and-variance}}
\sphinxAtStartPar
When observing values \(X_1, X_2, \ldots, X_n\) from a distribution, the true nature of this distribution is often unknown. In many cases, we are interested in estimating a parameter \(\theta\) of this distribution, such as the mean or variance. This process involves several key concepts:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Statistic}: a function of the observed data, or the data alone.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimator}: a rule or a function that tells you how to infer or guess the value of a parameter \(\theta\), or some function of it, denoted as \(h(\theta)\). Suppose you want to estimate the population mean. The sample mean (denoted usually as \(\bar{X}\)) is an estimator. It is a function that calculates the mean of your sample data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimand}: the quantity that we want to estimate.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimate}: the actual numerical value obtained by applying the estimator to your data. It is an approximation of some estimand, which we get using data.

\end{itemize}

\sphinxAtStartPar
We typically denote an estimator of \(\theta\) as \(\widehat{\theta}_n\), where the hat symbol signifies that it approximates the true parameter, and the subscript \(n\) indicates its dependence on the sample size. An estimator is itself a random variable because it is a function of random data. Its distribution, known as the sampling distribution, depends on the distribution of the data \(X_i\). A desirable property of an estimator is consistency, which means that \(\hat{\theta}_n\) converges to \(\theta\) as \(n \rightarrow \infty\). An estimator that fails to be consistent is generally not desirable. Two important properties of estimators are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bias}: the difference between the expected value of the estimator and the true parameter value. An estimator is unbiased if \(\mathbb{E}[\widehat{\theta}_n] = \theta\) for all \(\theta\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Variance}: a measure of the spread of the estimator’s sampling distribution. The variance of an estimator indicates how much the estimator varies from sample to sample. The square root of this variance is called the standard error, which provides a measure of how precise our estimate is.

\end{enumerate}

\sphinxAtStartPar
Let’s now provide an example to illustrate the concepts of bias and variance in the context of estimating the mean of a normal distribution. We assume a true mean (\(\mu\)) of 5 and generate 100 samples, each consisting of 30 observations drawn from a normal distribution with this true mean and a standard deviation of 2. For each sample, we compute the \sphinxstylestrong{sample mean}, which serves as our estimator for the population mean.

\sphinxAtStartPar
The sample means are then used to evaluate the bias and variance of the estimator:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The bias is calculated as the difference between the average of the sample means (the expected value of the estimator) and the true mean. A low bias indicates that the estimator is accurate on average

\item {} 
\sphinxAtStartPar
The variance is a measure of the spread of the sample means around their average. A low variance indicates that the estimator is reliable and produces similar estimates across different samples.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} True parameter}
\PYG{n}{true\PYGZus{}mean} \PYG{o}{=} \PYG{l+m+mi}{5}

\PYG{c+c1}{\PYGZsh{} Generate multiple samples}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{30}
\PYG{n}{samples} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{n}{true\PYGZus{}mean}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}size}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Calculate sample means}
\PYG{n}{sample\PYGZus{}means} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample}\PYG{p}{)} \PYG{k}{for} \PYG{n}{sample} \PYG{o+ow}{in} \PYG{n}{samples}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate bias and variance}
\PYG{n}{estimated\PYGZus{}mean} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{)}
\PYG{n}{bias} \PYG{o}{=} \PYG{n}{estimated\PYGZus{}mean} \PYG{o}{\PYGZhy{}} \PYG{n}{true\PYGZus{}mean}
\PYG{n}{variance} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{True Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{true\PYGZus{}mean}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Estimated Mean: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{estimated\PYGZus{}mean}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Bias: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{bias}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Variance: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{variance}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the distribution of sample means}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{true\PYGZus{}mean}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{estimated\PYGZus{}mean}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Estimated Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Sample Means}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
True Mean: 5
Estimated Mean: 5.06400167175167
Bias: 0.0640016717516696
Variance: 0.12043348413823464
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{00a99f51267443bbc0ddb6564b9d22fe41c4a2906f93da00ad95f11ce77bd388}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The histogram of the sample means provides a visual representation of the estimator’s sampling distribution. The true mean is marked by a red dashed line, and the average of the sample means (the estimated mean) is shown with a solid blue line. This visualization helps to see how the sample means are distributed around the true mean, illustrating the concepts of bias (how far the average estimate is from the true value) and variance (how spread out the estimates are).


\subsection{Main Probability Distributions}
\label{\detokenize{notebooks/review_stats:main-probability-distributions}}
\sphinxAtStartPar
\sphinxstylestrong{Normal Distribution}

\sphinxAtStartPar
The normal distribution, also known as the Gaussian distribution, is central in statistics due to its symmetric, bell\sphinxhyphen{}shaped curve. It is characterized by its mean \( \mu \) and standard deviation \( \sigma \), with the probability density function (PDF) given by
\label{equation:notebooks/review_stats:b255b387-7999-4c74-92b3-0627317853bc}\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} e^{-\frac{(x-\mu)^2}{2\sigma^2}}
\end{equation}
\sphinxAtStartPar
The significance of the normal distribution arises from the Central Limit Theorem (CLT), which states that sums of independent random variables converge to a normal distribution, regardless of the original distribution of these variables, making it applicable in a wide array of scenarios. This property underscores the normal distribution’s role in approximating the behavior of various real\sphinxhyphen{}world random processes.

\sphinxAtStartPar
\sphinxstylestrong{Chi\sphinxhyphen{}Squared Distribution}

\sphinxAtStartPar
The chi\sphinxhyphen{}squared distribution is another key distribution in statistics, especially in hypothesis testing and confidence interval estimation. It arises as the sum of the squares of independent standard normal variables. Specifically, if \( Z_1, Z_2, ..., Z_k \) are independent standard normal random variables, then \( \sum_{i=1}^{k} Z_i^2 \) follows a chi\sphinxhyphen{}squared distribution with \( k \) degrees of freedom. Its PDF for \( x > 0 \) and \( k \) degrees of freedom is
\label{equation:notebooks/review_stats:602dbc64-5cab-4f97-b9b8-3f9706a9de62}\begin{equation}
    f(x;k) = \frac{x^{k/2-1}e^{-x/2}}{2^{k/2}\Gamma(k/2)}
\end{equation}
\sphinxAtStartPar
This distribution is asymmetric and skewed to the right, with its shape and spread depending on the degrees of freedom \( k \). It is primarily used in the chi\sphinxhyphen{}squared test for independence and goodness of fit, and in estimating variances of normal distributions.

\sphinxAtStartPar
\sphinxstylestrong{F\sphinxhyphen{}Distribution}

\sphinxAtStartPar
The F\sphinxhyphen{}distribution is crucial in the context of variance analysis and hypothesis testing. It is the ratio of two scaled chi\sphinxhyphen{}squared distributions: if \( U \) follows a chi\sphinxhyphen{}squared distribution with \( d_1 \) degrees of freedom and \( V \) follows an independent chi\sphinxhyphen{}squared distribution with \( d_2 \) degrees of freedom, then the ratio \( \frac{U/d_1}{V/d_2} \) follows an F\sphinxhyphen{}distribution. Its PDF is described by
\label{equation:notebooks/review_stats:b59831eb-c986-4e28-94c0-b9e6cb4998fa}\begin{equation}
    f(x; d_1, d_2) = \frac{\sqrt{\frac{(d_1x)^{d_1}d_2^{d_2}}{(d_1x+d_2)^{d_1+d_2}}}}{x\text{B}\left(\frac{d_1}{2},\frac{d_2}{2}\right)}
\end{equation}
\sphinxAtStartPar
where \( d_1 \) and \( d_2 \) are the degrees of freedom. The distribution is non\sphinxhyphen{}symmetric, bounded at the left by 0, and its shape varies with the degrees of freedom. It is particularly useful in comparing variances between two samples, as in ANOVA and regression analysis.

\sphinxAtStartPar
\sphinxstylestrong{Student’s t\sphinxhyphen{}Distribution}

\sphinxAtStartPar
The Student’s t\sphinxhyphen{}distribution arises when estimating the mean of a normally distributed population in situations where the sample size is small and the population standard deviation is unknown. It is defined by the PDF
\label{equation:notebooks/review_stats:165db52b-ca33-474a-9ee6-624f82ba4e77}\begin{equation}
    f(t) = \frac{\Gamma\left(\frac{\nu + 1}{2}\right)}{\sqrt{\nu\pi}\Gamma\left(\frac{\nu}{2}\right)}\left(1+\frac{t^2}{\nu}\right)^{-\frac{\nu+1}{2}}
\end{equation}
\sphinxAtStartPar
where \( \nu \) denotes degrees of freedom. The t\sphinxhyphen{}distribution resembles the normal distribution but has heavier tails, meaning it is more prone to producing values that fall far from its mean. This property makes it particularly useful in hypothesis testing and constructing confidence intervals when the sample size is small. As the sample size increases, the t\sphinxhyphen{}distribution approaches the normal distribution, illustrating the connection between them.

\sphinxAtStartPar
The Student’s t\sphinxhyphen{}distribution is particularly useful when dealing with small sample sizes or when the population variance is unknown. It is defined as the distribution of the ratio of a standard normal random variable \(Z\) (with mean 0 and variance 1) and the square root of a chi\sphinxhyphen{}square random variable \(X\) divided by its degrees of freedom \(v\), i.e.,
\label{equation:notebooks/review_stats:f88fd2e1-a2e6-459f-8ad5-ab88207a8396}\begin{equation}
T = \frac{Z}{\sqrt{X/v}}
\end{equation}
\sphinxAtStartPar
where \(Z\) follows a standard normal distribution and \(X\) follows a chi\sphinxhyphen{}square distribution with \(v\) degrees of freedom. The resulting \(T\) follows a Student’s t\sphinxhyphen{}distribution with \(v\) degrees of freedom.

\sphinxAtStartPar
This distribution is symmetric and bell\sphinxhyphen{}shaped like the normal distribution but has heavier tails, meaning it is more prone to producing values that fall far from its mean. This property makes the t\sphinxhyphen{}distribution particularly suitable for small sample sizes, as it accounts for the increased uncertainty that comes with fewer observations.

\sphinxAtStartPar
The t\sphinxhyphen{}distribution is central to many statistical tests, including the t\sphinxhyphen{}test for assessing the statistical significance of the difference between two sample means, the construction of confidence intervals for the mean of a normally distributed population when the standard deviation is unknown, and in linear regression analysis.

\sphinxAtStartPar
The relationship between the normal distribution, the chi\sphinxhyphen{}square distribution, and the t\sphinxhyphen{}distribution highlights the importance of understanding how distributions can be related and transformed into each other, providing a powerful framework for statistical inference.

\sphinxAtStartPar
\sphinxstylestrong{Exponential Distribution}

\sphinxAtStartPar
The exponential distribution models the time between events in processes with a constant rate of occurrence and is pivotal in reliability analysis and queuing theory. Its memoryless property implies that the probability of an event occurring in the next instant is independent of how much time has already elapsed. The PDF is
\label{equation:notebooks/review_stats:c1c6e0e5-6c36-4ab8-bf8a-f76f8fab97de}\begin{equation}
    f(x;\lambda) = \lambda e^{-\lambda x}
\end{equation}
\sphinxAtStartPar
for \( x \geq 0 \). This distribution describes the time until an event like failure or arrival occurs and is widely used in survival analysis and reliability engineering.

\sphinxAtStartPar
\sphinxstylestrong{Binomial Distribution}

\sphinxAtStartPar
The binomial distribution is fundamental in modeling binary outcomes and represents the number of successes in a fixed number of independent Bernoulli trials. The PDF is
\label{equation:notebooks/review_stats:65b2ddf1-5d3f-4bf3-9831-2a06568fab30}\begin{equation}
    f(k;n,p) = \binom{n}{k}p^k(1-p)^{n-k}
\end{equation}
\sphinxAtStartPar
where \( k \) is the number of successes, \( n \) the number of trials, and \( p \) the probability of success. The shape of the binomial distribution can be symmetric or skewed depending on the values of \( n \) and \( p \). It is extensively used in scenarios like quality control, survey analysis, and clinical trials, providing a model for situations where outcomes are binary and probabilistically independent.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k}{as} \PYG{n+nn}{stats}

\PYG{c+c1}{\PYGZsh{} Set up the figure}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{9}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Normal Distribution}
\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma} \PYG{o}{=} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{norm}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{N(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mu}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{sigma}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Normal Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Chi\PYGZhy{}Squared Distribution}
\PYG{n}{df} \PYG{o}{=} \PYG{l+m+mi}{2}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{chi2}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{df}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Chi2(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{df}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Chi\PYGZhy{}Squared Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} F\PYGZhy{}Distribution}
\PYG{n}{d1}\PYG{p}{,} \PYG{n}{d2} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{2}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{f}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{d1}\PYG{p}{,} \PYG{n}{d2}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{d1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{d2}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{F\PYGZhy{}Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Student\PYGZsq{}s t\PYGZhy{}Distribution}
\PYG{n}{df} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{t}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{df}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{df}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Student}\PYG{l+s+se}{\PYGZbs{}\PYGZsq{}}\PYG{l+s+s1}{s t\PYGZhy{}Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Exponential Distribution}
\PYG{n}{lambda\PYGZus{}} \PYG{o}{=} \PYG{l+m+mi}{1}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{expon}\PYG{o}{.}\PYG{n}{pdf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{o}{/}\PYG{n}{lambda\PYGZus{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Exp(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{lambda\PYGZus{}}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Exponential Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Binomial Distribution}
\PYG{n}{n}\PYG{p}{,} \PYG{n}{p} \PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mf}{0.5}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{n} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{binom}\PYG{o}{.}\PYG{n}{pmf}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{n}\PYG{p}{,} \PYG{n}{p}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{stem}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{basefmt}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ }\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Binom(}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{n}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linefmt}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Binomial Distribution}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{36cad6439ce78357a00c9e7fe3424def4185512696afe25d7759b2a0eafea23a}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Explanation of plots:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normal distribution:} the plot shows the symmetric bell\sphinxhyphen{}shaped curve of the normal distribution with mean 0 and standard deviation 1.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chi\sphinxhyphen{}Squared distribution:} the plot illustrates the chi\sphinxhyphen{}squared distribution with 2 degrees of freedom, showing its right\sphinxhyphen{}skewed nature.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{F\sphinxhyphen{}distribution:} the plot shows the F\sphinxhyphen{}distribution with degrees of freedom 5 and 2, highlighting its right\sphinxhyphen{}skewed shape.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Student’s t\sphinxhyphen{}distribution:} the plot depicts the t\sphinxhyphen{}distribution with 5 degrees of freedom, showing its similarity to the normal distribution but with heavier tails.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exponential distribution:} the plot displays the exponential distribution with a rate parameter of 1, showing its memoryless property and right\sphinxhyphen{}skewed shape.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Binomial distribution:} the plot illustrates the binomial distribution with 10 trials and a success probability of 0.5, showing the discrete probability mass function.

\end{enumerate}

\sphinxAtStartPar
These visualizations help in understanding the different probability distributions and their properties, which are fundamental in various statistical analyses and applications.


\section{Hypothesis testing framework}
\label{\detokenize{notebooks/review_stats:hypothesis-testing-framework}}
\sphinxAtStartPar
Hypothesis testing is a fundamental framework in statistics used to determine whether there is enough evidence in a sample of data to infer that a certain condition holds for the entire population. In hypothesis testing, two contradictory hypotheses about a population parameter are considered: the null hypothesis (\(H_0\)) and the alternative hypothesis (\(H_1\)). The null hypothesis represents a default position or a statement of no effect or no difference. The alternative hypothesis represents what we want to prove or establish.

\sphinxAtStartPar
A test statistic is calculated from the sample data and is used to assess the truth of the null hypothesis. The choice of test statistic depends on the nature of the data and the hypothesis being tested. The P\sphinxhyphen{}value is the probability of observing a test statistic as extreme as, or more extreme than, the observed value under the assumption that the null hypothesis is true. A smaller P\sphinxhyphen{}value indicates that the observed data is less likely under the null hypothesis. Based on the P\sphinxhyphen{}value and a predetermined significance level (usually denoted as \(\alpha\), commonly set at 0.05), a decision is made: if the P\sphinxhyphen{}value is less than \(\alpha\), the null hypothesis is rejected in favor of the alternative hypothesis; if the P\sphinxhyphen{}value is greater than \(\alpha\), there is not enough evidence to reject the null hypothesis.

\sphinxAtStartPar
In hypothesis testing, two types of errors can occur. Type I error corresponds to rejecting the null hypothesis when it is actually true (false positive). The probability of making a Type I error is \(\alpha\). Type II error is failing to reject the null hypothesis when the alternative hypothesis is true (false negative).

\sphinxAtStartPar
Hypothesis testing is a critical tool in statistics for making inferences about populations based on sample data. It allows researchers to test assumptions and make decisions based on statistical evidence. The goal of hypothesis testing is not to prove the null hypothesis but to assess the strength of evidence against it. Hypothesis testing is a foundational concept in statistics used to infer the properties of a population based on sample data. The choice of distribution for conducting a hypothesis test depends on the nature of the data, the size of the sample, and the assumptions that can be made about the population. Below, we explore various hypothesis tests, the distributions used, and the rationale for their use.


\subsection{Comparing Means}
\label{\detokenize{notebooks/review_stats:comparing-means}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Z\sphinxhyphen{}test:} Used when comparing the mean of a sample to a known population mean, or comparing the means of two large independent samples. The Z\sphinxhyphen{}test is applicable when the population variance is known and the sample size is large (typically \(n > 30\)). The normal distribution is used due to the Central Limit Theorem (CLT), which states that the sampling distribution of the sample mean will approximate a normal distribution as the sample size becomes large, regardless of the population’s distribution.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{T\sphinxhyphen{}test:} Used when the population variance is unknown and the sample size is small, the t\sphinxhyphen{}distribution is used. The t\sphinxhyphen{}test is more accommodating of the uncertainty in the sample estimate of the variance, providing more accurate confidence intervals and P\sphinxhyphen{}values. The t\sphinxhyphen{}distribution converges to the normal distribution as the sample size increases.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{One\sphinxhyphen{}sample t\sphinxhyphen{}test:} Compares the mean of a single sample to a known mean.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Two\sphinxhyphen{}sample t\sphinxhyphen{}test:} Compares the means of two independent samples.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Paired t\sphinxhyphen{}test:} Compares means from the same group at different times or under different conditions.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Let’see an example of a one\sphinxhyphen{}sample t\sphinxhyphen{}test to check if the mean height of a sample of people is different from a known population mean of 170 cm.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sample data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}heights} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{172}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Perform one\PYGZhy{}sample t\PYGZhy{}test}
\PYG{n}{population\PYGZus{}mean} \PYG{o}{=} \PYG{l+m+mi}{170}
\PYG{n}{t\PYGZus{}statistic}\PYG{p}{,} \PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}1samp}\PYG{p}{(}\PYG{n}{sample\PYGZus{}heights}\PYG{p}{,} \PYG{n}{population\PYGZus{}mean}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Significance level}
\PYG{n}{alpha} \PYG{o}{=} \PYG{l+m+mf}{0.05}

\PYG{c+c1}{\PYGZsh{} Output the results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{T\PYGZhy{}statistic: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t\PYGZus{}statistic}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, P\PYGZhy{}value: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}value}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Significance level (alpha): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Automated conclusion based on P\PYGZhy{}value}
\PYG{k}{if} \PYG{n}{p\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{n}{alpha}\PYG{p}{:}
    \PYG{n}{conclusion} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Reject the null hypothesis. There is sufficient evidence to conclude that the mean height of the }\PYG{l+s+s2}{\PYGZdq{}}
                  \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{sample is significantly different from the population mean of 170 cm at the }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ significance level.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{conclusion} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fail to reject the null hypothesis. There is not sufficient evidence to conclude that the mean height }\PYG{l+s+s2}{\PYGZdq{}}
                  \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{of the sample is significantly different from the population mean of 170 cm at the }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ significance level.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{conclusion}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting the distribution of sample means}
\PYG{n}{sample\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{sample\PYGZus{}heights}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{)} \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{sample\PYGZus{}means}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample\PYGZus{}heights}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{population\PYGZus{}mean}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Population Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{One\PYGZhy{}Sample t\PYGZhy{}Test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
T\PYGZhy{}statistic: 1.289, P\PYGZhy{}value: 0.207
Significance level (alpha): 0.05
Fail to reject the null hypothesis. There is not sufficient evidence to conclude that the mean height of the sample is significantly different from the population mean of 170 cm at the 0.05 significance level.
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{ee04420a0645267bc2ef9dc640945f5ee120d044f8a9d9519dedae036e8f8e71}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s also see an example of the two\sphinxhyphen{}sample t\sphinxhyphen{}test to compare the mean heights of two independent samples and determine if there is a statistically significant difference between the two population means. Using a significance level of 0.05, the two\sphinxhyphen{}sample t\sphinxhyphen{}test calculates the t\sphinxhyphen{}statistic and P\sphinxhyphen{}value to assess the null hypothesis that the means of the two samples are equal. If the P\sphinxhyphen{}value is less than 0.05, the null hypothesis is rejected, indicating a significant difference between the sample means.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Sample data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{sample1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{172}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{sample2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{169}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Perform two\PYGZhy{}sample t\PYGZhy{}test}
\PYG{n}{t\PYGZus{}statistic}\PYG{p}{,} \PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}ind}\PYG{p}{(}\PYG{n}{sample1}\PYG{p}{,} \PYG{n}{sample2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Significance level}
\PYG{n}{alpha} \PYG{o}{=} \PYG{l+m+mf}{0.05}

\PYG{c+c1}{\PYGZsh{} Output the results}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{T\PYGZhy{}statistic: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t\PYGZus{}statistic}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, P\PYGZhy{}value: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}value}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Significance level (alpha): }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Automated conclusion based on P\PYGZhy{}value}
\PYG{k}{if} \PYG{n}{p\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{n}{alpha}\PYG{p}{:}
    \PYG{n}{conclusion} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Reject the null hypothesis. There is sufficient evidence to conclude that the mean heights of the }\PYG{l+s+s2}{\PYGZdq{}}
                  \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{two samples are significantly different at the }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ significance level.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n}{conclusion} \PYG{o}{=} \PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Fail to reject the null hypothesis. There is not sufficient evidence to conclude that the mean heights }\PYG{l+s+s2}{\PYGZdq{}}
                  \PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{of the two samples are significantly different at the }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{ significance level.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{conclusion}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting the distribution of sample means}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{sample1}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{sample2}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 1 Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{sample2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{dashed}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample 2 Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Sample Mean}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Two\PYGZhy{}Sample t\PYGZhy{}Test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
T\PYGZhy{}statistic: 3.598, P\PYGZhy{}value: 0.000
Significance level (alpha): 0.05
Reject the null hypothesis. There is sufficient evidence to conclude that the mean heights of the two samples are significantly different at the 0.05 significance level.
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{f8e26f90475f7f1e2bd9b88693050d72799650b6a71f58ac96a9448c2bed01f4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Comparing Variances}
\label{\detokenize{notebooks/review_stats:comparing-variances}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{F\sphinxhyphen{}test:} Used in the analysis of variance (ANOVA) and for comparing the variances of two samples. The F\sphinxhyphen{}distribution arises naturally when comparing the ratio of two variances, each of which follows a chi\sphinxhyphen{}squared distribution when the underlying population is normally distributed. The F\sphinxhyphen{}test assesses whether the groups have the same variance, an assumption often required in ANOVA and regression analysis. ANOVA is used to compare the means of three or more samples. The F\sphinxhyphen{}distribution is used in ANOVA to compare the ratio of the variance explained by the model to the variance within the groups. This test helps to determine if there are significant differences between the means of the groups.

\end{itemize}


\subsection{Regression Analysis}
\label{\detokenize{notebooks/review_stats:regression-analysis}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{T\sphinxhyphen{}tests for regression coefficients:} To determine if individual predictors are significantly related to the dependent variable, t\sphinxhyphen{}tests are used, leveraging the t\sphinxhyphen{}distribution. This is because the estimates of the coefficients have distributions that are best modeled by the t\sphinxhyphen{}distribution, especially with small sample sizes.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{F\sphinxhyphen{}test for overall model significance:} The F\sphinxhyphen{}test is used to assess whether at least one predictor variable has a non\sphinxhyphen{}zero coefficient, indicating that the model provides a better fit to the data than a model with no predictors. This test uses the F\sphinxhyphen{}distribution, comparing the model’s explained variance to the unexplained variance.

\end{itemize}


\subsection{Goodness of Fit and Independence Tests}
\label{\detokenize{notebooks/review_stats:goodness-of-fit-and-independence-tests}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Chi\sphinxhyphen{}squared test:} Used for categorical data to assess how likely it is that an observed distribution is due to chance. It is used in goodness\sphinxhyphen{}of\sphinxhyphen{}fit tests to compare the observed distribution to an expected distribution, and in tests of independence to evaluate the relationship between two categorical variables in a contingency table. The chi\sphinxhyphen{}squared distribution is used because the test statistic follows this distribution under the null hypothesis.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}parametric tests:} Used when the assumptions about the population distribution are not met. These tests do not rely on the normality assumption and often use ranking methods or resampling techniques. Examples include the Mann\sphinxhyphen{}Whitney U test, Wilcoxon signed\sphinxhyphen{}rank test, and Kruskal\sphinxhyphen{}Wallis H test.

\end{itemize}


\section{Bayesian Learning}
\label{\detokenize{notebooks/review_stats:bayesian-learning}}
\sphinxAtStartPar
Statistical methods can be broadly divided into two macro\sphinxhyphen{}categories: frequentist and Bayesian. The frequentist approach views parameters as fixed but unknown quantities, uses data to estimate these parameters, and makes point estimates (i.e., a single best guess) for these parameters. In contrast, the Bayesian approach views parameters as random variables, uses data and prior beliefs (prior distributions) to update our beliefs about these parameters, and results in a probability distribution over the parameters, capturing the uncertainty.

\sphinxAtStartPar
One key difference is that Bayesian statistics treats probability as a measure of belief or certainty rather than frequency. This means probabilities are subjective and can be updated as new information becomes available. In frequentist statistics, probability is interpreted as the long\sphinxhyphen{}run frequency of events, relying on the concept of an infinite sequence of repeated trials. Bayesian methods incorporate prior knowledge or beliefs through the use of prior probability distributions, while frequentist methods make inferences solely from the data at hand.

\sphinxAtStartPar
Bayesian learning is formalized using \sphinxstylestrong{Bayes’ theorem}:
\begin{equation*}
\begin{split}
P(\theta | X) = \frac{P(X | \theta)P(\theta)}{P(X)}
\end{split}
\end{equation*}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(P(\theta | X)\) is the posterior distribution of the parameters given the data.

\item {} 
\sphinxAtStartPar
\(P(X | \theta)\) is the likelihood of the data given the parameters. It represents the probability of observing the data \(X\) given a particular set of parameters \(\theta\).

\item {} 
\sphinxAtStartPar
\(P(\theta)\) is the prior distribution of the parameters (our beliefs before seeing the data).

\item {} 
\sphinxAtStartPar
\(P(X)\) is the evidence or marginal likelihood. It is the probability of the data over all possible parameter values, acting as a normalizing constant to ensure the posterior distribution sums (or integrates) to 1.

\end{itemize}

\sphinxAtStartPar
Bayes’ Theorem allows us to update our initial beliefs or probabilities \(P(\theta)\) in light of new evidence (\(X\)). In other words, it provides a way to revise existing predictions or hypotheses given new or additional information.

\sphinxAtStartPar
\sphinxstylestrong{Estimating the posterior distribution}

\sphinxAtStartPar
Computing the posterior distribution means determining the probability distribution of the parameters of a model given the observed data. This involves updating our beliefs about possible parameter values based on the evidence provided by the data. The main challenge in Bayesian learning is computing the posterior distribution, especially for complex models. This is where methods like Markov chain Monte Carlo (MCMC) come into play. Because of these challenges, we often resort to methods like sampling (e.g., MCMC) or approximations to estimate the posterior distribution, rather than computing it exactly. The goal is to get a representation of the distribution that lets us make informed decisions about the likely values of the parameters given the data.

\sphinxAtStartPar
Estimating the posterior distribution is the core of Bayesian inference. After observing data, we combine our prior beliefs with the likelihood of the observed data to compute the posterior distribution. The shape of this distribution reflects our updated beliefs about the parameters given the data. Once we have the posterior distribution, we can draw samples from it. Each sample represents a plausible value of the parameter(s) given our prior beliefs and the observed data. By looking at the spread and distribution of these samples, we can understand the uncertainty associated with our estimates.

\sphinxAtStartPar
In many situations, especially with complex models, the posterior distribution might not have a simple analytical form. In these cases, we cannot just “look” at the posterior directly. Instead, we use sampling techniques (like MCMC methods) to draw samples from the posterior, even if we cannot describe the posterior in a simple equation. These samples then serve multiple purposes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Uncertainty estimation:} the spread and distribution of the samples give a sense of how uncertain we are about our parameter estimates.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Predictive modeling:} we can use the samples to make predictions for new data and to get a sense of uncertainty in those predictions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Model checking:} we can compare the predictions of our model (using the posterior samples) to the actual observed data to see if our model is a good fit.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Decision making:} in practical scenarios, decisions might be based on the posterior samples, especially when we need to consider the uncertainty in our estimates.

\end{itemize}

\sphinxAtStartPar
In summary, while the posterior distribution encapsulates our updated beliefs after seeing data, sampling from the posterior allows us to quantify, explore, and make decisions based on the uncertainty in those beliefs.


\subsection{Markov chain Monte Carlo (MCMC)}
\label{\detokenize{notebooks/review_stats:markov-chain-monte-carlo-mcmc}}
\sphinxAtStartPar
MCMC algorithms are used to approximate complex probability distributions. They are especially useful in Bayesian statistics when direct computation of the posterior distribution is challenging. The basic idea is to generate samples from a complex distribution, which is too intricate to tackle directly. Instead of computing it exactly, we generate samples that come from that distribution. Over time, the distribution of these samples will closely match the target distribution.

\sphinxAtStartPar
The key concepts of MCMC are two\sphinxhyphen{}fold:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Markov chain:} a sequence of random samples where each sample depends only on the one before it. It is like a random walk where each step is influenced only by the current position.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Monte Carlo:} a technique where random sampling is used to get numerical results for problems that might be deterministic in principle. The name originates from the Monte Carlo Casino, as it relies on randomness.

\end{itemize}

\sphinxAtStartPar
The main steps of MCMC algorithms are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Initialization:} start at a random position (a random parameter value).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Proposal:} at each step, propose a new position based on the current one. This can be a random jump, but it is typically a small move.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Acceptance:} decide whether to move to the proposed position. If the new position is a better fit to the data (higher posterior probability), we will likely accept it. If it is worse, we might still accept it but with a lower probability. This decision process ensures we explore the whole space but spend more time in high\sphinxhyphen{}probability areas.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Iteration:} repeat the proposal and acceptance steps many times. The more steps, the better the approximation will be.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Burn\sphinxhyphen{}in:} the initial samples might not be representative because the chain might start far from a high\sphinxhyphen{}probability area. So, we discard an initial set of samples, a process called “burn\sphinxhyphen{}in”.

\end{itemize}

\sphinxAtStartPar
How do we determine if the posterior distribution is higher if we do not have an analytical form? This is the key idea behind MCMC methods (like the Metropolis\sphinxhyphen{}Hastings algorithm). We do not need to know the exact value of the posterior distribution; we only need to know it up to a constant of proportionality. In many cases, while the full posterior is hard to compute (due to the difficulty in calculating the normalization constant), its unnormalized version is computable.

\sphinxAtStartPar
Remember the basic Bayes’ formula:
\begin{equation*}
\begin{split}
\text{posterior} \propto \text{likelihood} \times \text{prior}
\end{split}
\end{equation*}
\sphinxAtStartPar
In many applications, we can compute the product of the likelihood and the prior for any given set of parameters, but we might not be able to easily normalize it to get a true probability distribution. So, when deciding whether to accept a new proposed position in MCMC, we first compute the unnormalized posterior at the current position (which is the product of the likelihood and the prior). Then we compute the unnormalized posterior at the proposed new position. Finally, we compare these values. If the unnormalized posterior is higher at the new position, then it means the true posterior is also higher there. Even if we cannot say exactly what the posterior value is at that position, we can still determine if it is higher or lower than at the current position. This relative comparison, rather than an absolute value, is what drives the decision to accept or reject the new proposed position. For the case where the proposed position has a lower unnormalized posterior value, the Metropolis\sphinxhyphen{}Hastings algorithm provides a rule to accept it with a probability proportional to the ratio of the unnormalized posteriors (proposed to current). This ensures exploration of the entire parameter space, preventing the algorithm from getting stuck in local modes.

\sphinxAtStartPar
In essence, MCMC is a systematic way to “wander around” in a parameter space to understand a probability distribution, especially when direct computation is difficult or impossible.

\sphinxAtStartPar
Let’s use a simple example to illustrate Bayesian inference. We will use MCMC to estimate the posterior distribution of the mean of a normally distributed data set.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pymc} \PYG{k}{as} \PYG{n+nn}{pm}
\PYG{k+kn}{import} \PYG{n+nn}{logging}

\PYG{c+c1}{\PYGZsh{} Suppress pymc logging}
\PYG{n}{logger} \PYG{o}{=} \PYG{n}{logging}\PYG{o}{.}\PYG{n}{getLogger}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{pymc}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{logger}\PYG{o}{.}\PYG{n}{setLevel}\PYG{p}{(}\PYG{n}{logging}\PYG{o}{.}\PYG{n}{ERROR}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate some data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the data}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{hist}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+m+mi}{30}\PYG{p}{,} \PYG{n}{density}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{g}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Histogram of Generated Data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the Bayesian model}
\PYG{k}{with} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{Model}\PYG{p}{(}\PYG{p}{)} \PYG{k}{as} \PYG{n}{model}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Prior for the mean (μ)}
    \PYG{n}{mu} \PYG{o}{=} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{mu}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{mu}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Prior for the standard deviation (σ)}
    \PYG{n}{sigma} \PYG{o}{=} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{HalfNormal}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{sigma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} Likelihood (sampling distribution) of the data}
    \PYG{n}{likelihood} \PYG{o}{=} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{Normal}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{likelihood}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{mu}\PYG{o}{=}\PYG{n}{mu}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{n}{sigma}\PYG{p}{,} \PYG{n}{observed}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Perform MCMC sampling}
    \PYG{k}{with} \PYG{n}{model}\PYG{p}{:}
        \PYG{n}{trace} \PYG{o}{=} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{sample}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{n}{return\PYGZus{}inferencedata}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{progressbar}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the posterior distributions}
\PYG{n}{pm}\PYG{o}{.}\PYG{n}{plot\PYGZus{}posterior}\PYG{p}{(}\PYG{n}{trace}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Summary of the posterior distributions}
\PYG{n}{summary} \PYG{o}{=} \PYG{n}{pm}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{n}{trace}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{summary}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{e78004dcf0d59724742cd03ad7f6074f6c2ac80218b63064c7b645628911eb84}.png}

\noindent\sphinxincludegraphics{{5a8b4895a2084fcc385020a3f632cd9bfa9018423e660228d62ae63aee994e7e}.png}

\begin{sphinxVerbatim}[commandchars=\\\{\}]
        mean     sd  hdi\PYGZus{}3\PYGZpc{}  hdi\PYGZus{}97\PYGZpc{}  mcse\PYGZus{}mean  mcse\PYGZus{}sd  ess\PYGZus{}bulk  ess\PYGZus{}tail  \PYGZbs{}
mu     5.038  0.063   4.923    5.158      0.001    0.001    4255.0    2864.0   
sigma  1.961  0.045   1.879    2.044      0.001    0.001    3477.0    2738.0   

       r\PYGZus{}hat  
mu       1.0  
sigma    1.0  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Explanation:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generate data:} we generate 100 data points from a normal distribution with a mean of 5 and a standard deviation of 2.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Plot data:} a histogram is plotted to visualize the generated data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Define the Bayesian model:} ee define a Bayesian model using the PyMC library.
\begin{itemize}
\item {} 
\sphinxAtStartPar
The prior distribution for the mean (\(\mu\)) is set to a normal distribution with mean 0 and standard deviation 10.

\item {} 
\sphinxAtStartPar
The prior distribution for the standard deviation (\(\sigma\)) is set to a half\sphinxhyphen{}normal distribution with a standard deviation of 10.

\item {} 
\sphinxAtStartPar
The likelihood of the data is modeled as a normal distribution with mean \(\mu\) and standard deviation \(\sigma\).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Perform MCMC sampling:} e perform MCMC sampling to estimate the posterior distributions of \(\mu\) and \(\sigma\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Plot posterior distributions:} the posterior distributions of the parameters are plotted.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Summary of posterior distributions:} a summary of the posterior distributions is printed, showing the estimated parameters and their uncertainties.

\end{enumerate}

\sphinxAtStartPar
Bayesian learning provides a powerful framework for updating beliefs about parameters in light of new data. By combining prior knowledge with observed data through Bayes’ theorem, we can obtain a posterior distribution that captures our updated beliefs and uncertainties. Methods like MCMC enable us to estimate the posterior distribution even for complex models, allowing for informed decision\sphinxhyphen{}making based on the uncertainty in our estimates.

\sphinxstepscope


\chapter{Linear Regression}
\label{\detokenize{notebooks/review_linear_models:linear-regression}}\label{\detokenize{notebooks/review_linear_models::doc}}

\section{Motivating linear approximations}
\label{\detokenize{notebooks/review_linear_models:motivating-linear-approximations}}
\sphinxAtStartPar
Linear models are extremely useful becase they can be used to approximate complex relationships with easy and interpretable models. When employing linear models to learn a function \(f\) relating \(Y\) to \(X\), we do not need to necessarily assume that the relationship between \(X\) and \(Y\) is linear. We are just looking for the best linear approximation to the true relationship, whatever that might be. Taylor’s theorem gives reasons to believe that a linear model is a sensible approximation even for more complex functions, at least locally. Indeed, if the true regression function \(f(x)\) is a smooth function, given a specific value \(x_0\), we can expand the function as
\label{equation:notebooks/review_linear_models:017d5635-bcc6-4545-a156-8b2a81f57e98}\begin{equation}
    f(x) = f(x_0) + \frac{f'(x_0)}{1!}(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \cdots
\end{equation}
\sphinxAtStartPar
This expansion breaks down the function into an infinite sum of terms based on the function’s derivatives at \(x_0\). The approximation starts with the function’s value at \(x_0\), then adds adjustments based on how the function changes (its derivatives) as you move away from \(x_0\). For \(x\) close enough to \(x_0\), we can get away with truncating the series at first order, as in
\label{equation:notebooks/review_linear_models:c6b384a2-1656-42ec-8f39-40926fbc2e80}\begin{equation}
    f(x) \approx f(x_0) + f'(x_0)(x-x_0)
\end{equation}
\sphinxAtStartPar
When you truncate the Taylor series after the first derivative term, we are essentially creating a linear model. This model approximates the function \(f(x)\) using a straight line that tangentially matches the function’s slope at \(x_0\). This approach is valid as long as the higher\sphinxhyphen{}order terms (like the quadratic term and beyond) are negligible, which usually means \(x\) is close enough to \(x_0\). Thus, while a  linear approximation may work well locally (near \(x_0\)), extending this approximation globally (over a wide range of \(x\) values) may not always be accurate unless the function is nearly linear over that range. The key to a successful linear approximation lies in determining how “close” \(x\) must be to \(x_0\) for the higher\sphinxhyphen{}order terms to be negligible. For a linear approximation to be valid, we want the influence of this term (and all higher\sphinxhyphen{}order terms) to be small compared to the first derivative term. We formalize this by imposing that the linear term dominates over the quadratic
\label{equation:notebooks/review_linear_models:03ddbcb1-073d-4dca-8219-7462e42dc78e}\begin{equation}
    |x-x_0|f'(x_0) \gg \frac{f''(x_0)}{2}
\end{equation}
\sphinxAtStartPar
which is true if
\label{equation:notebooks/review_linear_models:11b74c00-2531-4ac4-a9b3-000d0ea71f16}\begin{equation}
    2\frac{f'(x_0)}{f''(x_0)} \gg |x-x_0|
\end{equation}
\sphinxAtStartPar
This tells us that the distance between \(x\) and \(x_0\) must be smaller than twice the ratio of the magnitude of the first derivative to the magnitude of the second derivative for the linear approximation to hold effectively. The exact bounds of “close enough” depend on the relative sizes of the first and second derivatives of the function at \(x_0\), providing a rule of thumb for when a linear model is likely to be a good approximation.

\sphinxAtStartPar
To show this concept, let’s try to create a plot where we use several linear approximations at different points along a non\sphinxhyphen{}linear function. This will demonstrate how linear functions can locally approximate the non\sphinxhyphen{}linear function at different regions.
\begin{itemize}
\item {} 
\sphinxAtStartPar
First, we will define a \sphinxstylestrong{non\sphinxhyphen{}linear function} \(f(x) = \sin(x) + x\), and compute its first derivative \(f'(x) = \cos(x) + x\).

\item {} 
\sphinxAtStartPar
Then, we select three points (\(x_0 = 1, 3, 5\)) at which to compute the \sphinxstylestrong{linear approximations}. For each point \(x_0\):
\begin{itemize}
\item {} 
\sphinxAtStartPar
We compute \(y_0 = f(x_0)\) and the slope \(f'(x_0)\).

\item {} 
\sphinxAtStartPar
We use the linear approximation formula \(f(x) \approx y_0 + f'(x_0)(x - x_0)\).

\item {} 
\sphinxAtStartPar
We plot the linear approximation along with the original non\sphinxhyphen{}linear function.

\end{itemize}

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Define the non\PYGZhy{}linear function}
\PYG{k}{def} \PYG{n+nf}{true\PYGZus{}function}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{n}{x}

\PYG{c+c1}{\PYGZsh{} Compute the first derivative of the true function}
\PYG{k}{def} \PYG{n+nf}{true\PYGZus{}function\PYGZus{}derivative}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{x}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{1}

\PYG{c+c1}{\PYGZsh{} Points at which we will create linear approximations}
\PYG{n}{x0\PYGZus{}points} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Generate data for the non\PYGZhy{}linear function}
\PYG{n}{X\PYGZus{}nonlinear} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y\PYGZus{}nonlinear} \PYG{o}{=} \PYG{n}{true\PYGZus{}function}\PYG{p}{(}\PYG{n}{X\PYGZus{}nonlinear}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the true non\PYGZhy{}linear function}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}nonlinear}\PYG{p}{,} \PYG{n}{y\PYGZus{}nonlinear}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True function}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot linear approximations at different points}
\PYG{k}{for} \PYG{n}{x0} \PYG{o+ow}{in} \PYG{n}{x0\PYGZus{}points}\PYG{p}{:}
    \PYG{n}{y0} \PYG{o}{=} \PYG{n}{true\PYGZus{}function}\PYG{p}{(}\PYG{n}{x0}\PYG{p}{)}
    \PYG{n}{slope} \PYG{o}{=} \PYG{n}{true\PYGZus{}function\PYGZus{}derivative}\PYG{p}{(}\PYG{n}{x0}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}approx} \PYG{o}{=} \PYG{n}{y0} \PYG{o}{+} \PYG{n}{slope} \PYG{o}{*} \PYG{p}{(}\PYG{n}{X\PYGZus{}nonlinear} \PYG{o}{\PYGZhy{}} \PYG{n}{x0}\PYG{p}{)}
    
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}nonlinear}\PYG{p}{,} \PYG{n}{y\PYGZus{}approx}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Linear approx at \PYGZdl{}x\PYGZus{}0\PYGZdl{}=}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{x0}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{p}{[}\PYG{n}{x0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{n}{y0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Non\PYGZhy{}linear function and its linear approximations}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{51ae1b6ba2dc1d928e7ac2d7483aed89f30222190d5fd6d4838fe853b093bf0b}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The resulting plot shows how different linear functions can locally approximate the non\sphinxhyphen{}linear function reasonably well at different regions. However, we can see how the approximation is only valid in the vicinity of \(x_0\).


\section{Linear regression framework}
\label{\detokenize{notebooks/review_linear_models:linear-regression-framework}}
\sphinxAtStartPar
A linear regression model is a model used to analyze the relationship between a dependent variable (response) and one or more independent variables (predictors or covariates). In the case of one predictor, the linear regression model is referred to as a simple linear regression model, and it is given by
\label{equation:notebooks/review_linear_models:d228f2f6-6ffa-437c-b4d1-a9f0b487492f}\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{equation}
\sphinxAtStartPar
where \(y_i\) is the \(i\)th observation of the response variable, \(x\) is the \(i\)th observation of the predictor, \(\beta_0\) is the intercept (the value of \(y\) when \(x\) is zero), \(\beta_1\) is the regression coefficient (the expected change in \(y\) per unit change in \(x\)), and \(\epsilon\) represents the error term, accounting for the variability in \(y\) that cannot be explained by the linear relationship with \(x\). In most of the cases, we will assume \(\epsilon\) to be normally distributed around zero with finite variance, \(\epsilon \sim \mathcal{N}(0, \sigma^2)\). In the case of \(p\) predictors and \(n\) observations, the model is referred to as a multiple linear regression model, and we can express it in matrix notation, as in
\label{equation:notebooks/review_linear_models:8eeb73e4-0282-4687-ac3e-64b74ce35e96}\begin{equation}
    \mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}
\end{equation}
\sphinxAtStartPar
where
\begin{equation*}
    \mathbf{y}=\left[\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{array}\right], \quad \mathbf{X}=\left[\begin{array}{ccccc}
1 & x_{11} & x_{12} & \cdots & x_{1 p} \\
1 & x_{21} & x_{22} & \cdots & x_{2 p} \\
\vdots & \vdots & & \vdots \\
1 & x_{n 1} & x_{n 2} & \cdots & x_{n p}
\end{array}\right], \quad \boldsymbol{\beta}=\left[\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{array}\right], \quad \text { and } \quad \boldsymbol{\epsilon}=\left[\begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{array}\right]
\end{equation*}
\sphinxAtStartPar
\(\mathbf{y}\) is a \(n \times 1\) vector of response variables, \(\mathbf{X}\) is a \(n \times (p + 1)\) model matrix, \(\boldsymbol{\beta}\) is a \((p + 1) \times 1\) vector of regression coefficients, and \(\boldsymbol{\epsilon}\) is a \(n \times 1\) vector representing the noise, with covariance matrix \(\sigma^2 \mathbf{I}\). If the predictors and the response are centered (for example by subtracting the mean), the intercept term can be removed from the model. In that case, the size of the model matrix becomes \(n \times p\), and \(\boldsymbol{\beta}\) a \(p \times 1\) vector. We will assume this is the case in the following sections.

\sphinxAtStartPar
Linear regression models are based on several \sphinxstylestrong{key assumptions}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Linearity}: the relationship between the predictors and the response is linear. This implies that a change in a predictor leads to a proportional change in the response. While real\sphinxhyphen{}life processes are rarely purely linear, we can often assume local linearity. This means that the linearity assumption holds within a reasonably limited range of the design space, recognizing that the relationship may not be linear over a wider range.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Independence}: observations are independent of each other. In other words, the observations do not influence each other. This assumption can be violated in many situations, particularly in time series data or spatial data where there might be autocorrelation (i.e., the value of a variable at one point in time or space is correlated with its values at other points).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Homoscedasticity}: the variance of the error terms (residuals) is constant across all levels of the independent variables. This condition, known as homoscedasticity, implies that the spread of the residuals should be roughly the same for all values of the predictors. If the variance of the residuals changes with the level of the predictors, the condition is known as heteroscedasticity, which can lead to inefficiencies and bias in the estimation of parameters.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Normality of the error terms}: this assumption is particularly important for hypothesis testing and creating confidence intervals. It’s crucial to note that this assumption pertains to the errors, not necessarily to the distributions of the predictors or the response variable. While linear regression can be robust to mild violations of this assumption, severe departures can affect the reliability of inference procedures

\end{itemize}


\section{Ordinary least squares}
\label{\detokenize{notebooks/review_linear_models:ordinary-least-squares}}
\sphinxAtStartPar
In regression analysis, the most common method to estimate the unknown model parameters \(\mathbf{\beta}\) is ordinary least squares (OLS). The OLS method seeks to find the coefficients that minimize the sum of squares of the errors, \(\epsilon_i\). Recall that a key assumption in linear regression models is that \(\{\epsilon_i\}\) are uncorrelated random variables. We aim to find the vector of least squares estimators \(\boldsymbol{{beta}}\) that minimizes
\label{equation:notebooks/review_linear_models:2e9eb108-bd0a-4cd3-8587-de8ab2c6d2d3}\begin{equation}
    \mathcal{L} = \sum_{i=1}^{n} \epsilon_i^2 = \boldsymbol{\epsilon}^\top\boldsymbol{\epsilon} 
\end{equation}
\sphinxAtStartPar
Because \(\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}\), we can express \(\boldsymbol{\epsilon} = \mathbf{y} - \mathbf{X} \boldsymbol{\beta}\). So, we have
\label{equation:notebooks/review_linear_models:269594c2-e0bc-49a3-b887-e1a1b991e20d}\begin{align}
    \mathcal{L} 
    &= (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
    &= (\mathbf{y}^\top - \boldsymbol{\beta}^\top\mathbf{X}^\top) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
    &= \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta} \\
    &= \mathbf{y}^\top \mathbf{y} - 2 \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}
\end{align}
\sphinxAtStartPar
Note that \(\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta}\) is a scalar, because we have \((1 \times n) \times (n \times p) \times (p \times 1)\), resulting in a \(1 \times 1\) matrix, which is a scalar. Similarly, \(\boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y}\), having dimensions \((1 \times p) \times (p \times n) \times (n \times 1)\), also results in a \(1 \times 1\) matrix, or scalar. Moreover, due to the properties of transposition and the commutative property of scalar multiplication, these two expressions are not only scalars but also represent the same scalar value. Transposing a scalar does not affect its value, thus we have \((\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta})^\top=\boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y}\). Remind that, in matrix multiplication, if we transpose the product of two matrices, we reverse the order of multiplication and transpose each matrix: \((\mathbf{A}\mathbf{B})^\top=\mathbf{B}^\top \mathbf{A}^\top\).

\sphinxAtStartPar
Now, we need to get the derivative of \(\mathcal{L}\) with respect to the parameter vector \(\boldsymbol{\beta}\) and set it to zero. This way we will find the estimated coefficients \(\mathbf{b}\).
\label{equation:notebooks/review_linear_models:8398278a-f3b6-4a26-aa48-24a58bea45a7}\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \boldsymbol{\beta}} \Bigr|_{\widehat{\boldsymbol{\beta}}} = - 2 \mathbf{X}^\top \mathbf{y} + 2 \mathbf{X}^\top \mathbf{X}\widehat{\boldsymbol{\beta}} = \mathbf{0}
\end{equation}
\sphinxAtStartPar
which simplifies to
\label{equation:notebooks/review_linear_models:a53a778f-989b-4617-90a7-6422c71b6b33}\begin{equation}
    \mathbf{X}^\top \mathbf{X}\widehat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{y}
\end{equation}
\sphinxAtStartPar
Multiplying both sides by the inverse of \(\mathbf{X}^\top \mathbf{X}\) we get the OLS estimate
\label{equation:notebooks/review_linear_models:acab5d34-7107-4f0a-998f-a86741466ff5}\begin{equation}
    \widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}
\end{equation}
\sphinxAtStartPar
The matrix \(\mathbf{X}^\top \mathbf{X}\) is sometimes referred to as the Gram matrix or the moment matrix, because it contains the ``second moments’’ (i.e., variances and covariances) of the independent variables. The diagonal elements are the sums of squares of each predictor, and the off\sphinxhyphen{}diagonal elements represent the sums of cross\sphinxhyphen{}products (or covariances) between different predictors. The fitted regression model is then given by
\label{equation:notebooks/review_linear_models:87e5c540-052d-44d6-8091-fcbbabb72ee5}\begin{equation}
    \widehat{\mathbf{y}} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} = \mathbf{H}\mathbf{y}
\end{equation}
\sphinxAtStartPar
where the matrix \(\mathbf{H}\) is referred to as the hat matrix or influence matrix. We can see how the fitted values at the data points used to estimate the model are linear combinations of the observed responses, with weights given by the hat matrix. Geometrically, this means that we find the fitted values by taking the vector of observed responses \(\mathbf{y}\) and projecting it onto a certain plane, which is entirely defined by the values in \(\mathbf{X}\). If we repeat our experiment (e.g., survey, observation) many times at the same locations \(\mathbf{X}\), we get different responses \(\mathbf{y}\) every time. But \(\mathbf{H}\) does not change. The properties of the fitted values are thus largely determined by the properties of \(\mathbf{H}\).


\section{Expected value and variance of the least square estimators}
\label{\detokenize{notebooks/review_linear_models:expected-value-and-variance-of-the-least-square-estimators}}
\sphinxAtStartPar
Because we know that \(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I}_n)\), we can say that \(\mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2\mathbf{I}_n)\). Since \(\boldsymbol{\epsilon}\) follows a multivariate normal distribution, any linear combination of \(\boldsymbol{\epsilon}\) is also multivariate normally distributed, including \(\mathbf{y}\). The expectation of \(\mathbf{y}\) is \(\mathbf{X} \boldsymbol{\beta}\) because \(\boldsymbol{\epsilon}\) has expectation zero. Similarly, since \(\widehat{\boldsymbol{\beta}}\) is a linear transformation of \(\mathbf{y}\), it is also normally distributed.

\sphinxAtStartPar
Assuming the model is correct, we can first evaluate the bias of the OLS estimator by looking at the expected value of \(\widehat{\boldsymbol{\beta}}\), which is given by
\label{equation:notebooks/review_linear_models:cca28c15-a732-4074-b98a-bc84c08e4d90}\begin{align}
    \mathbb{E}[\widehat{\boldsymbol{\beta}}] &= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}] \\
    &= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top (\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon})] \\
    &= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta}+(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\epsilon})] \\
    &= \mathbb{E}[\boldsymbol{\beta} + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\epsilon}] \\
    &= \boldsymbol{\beta}
\end{align}
\sphinxAtStartPar
Thus, \(\widehat{\boldsymbol{\beta}}\) is an unbiased estimator of \(\boldsymbol{\beta}\) if the model is correct. In the derivation of the expected value we used that \(\mathbb{E}[\boldsymbol{\epsilon}]=\mathbf{0}\) and that \((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}=\mathbf{I}\), and of course the expected value of the constant \(\boldsymbol{\beta}\) is \(\boldsymbol{\beta}\) itself.

\sphinxAtStartPar
The variance property of \(\widehat{\boldsymbol{\beta}}\) is expressed by the covariance matrix
\label{equation:notebooks/review_linear_models:96d49594-c3b1-466e-94d4-8818dc3da9dd}\begin{equation}
    \operatorname{Cov}[\widehat{\boldsymbol{\beta}}] = \mathbb{E}[(\widehat{\boldsymbol{\beta}}-\mathbb{E}[\widehat{\boldsymbol{\beta}}])(\widehat{\boldsymbol{\beta}}-\mathbb{E}[\widehat{\boldsymbol{\beta}}])^\top]
\end{equation}
\sphinxAtStartPar
which is a \(p \times p\) symmetric matrix whose \(j\)th diagonal element is the variance of \(\widehat{\boldsymbol{\beta}}_j\) and whose (\(ij\))th off\sphinxhyphen{}diagonal element is the covariance between \(\widehat{\beta}_i\) and \(\widehat{\beta}_j\). The
covariance matrix of \(\widehat{\boldsymbol{\beta}}\) is found by applying a variance operator to \(\widehat{\boldsymbol{\beta}}\)
\label{equation:notebooks/review_linear_models:a26692f8-6bea-412d-9ead-4351b685b0d4}\begin{equation}
    \operatorname{Cov}[\widehat{\boldsymbol{\beta}}] = \operatorname{Var}[\widehat{\boldsymbol{\beta}}] = \operatorname{Var}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}]
\end{equation}
\sphinxAtStartPar
Now let us remind that, in general, if a vector \(\mathbf{v}\) has covariance matrix \(\mathbf{C}\) we have that
\label{equation:notebooks/review_linear_models:b195f9fe-95ae-4c1e-aa4d-5a90e9bd3a36}\begin{equation}
    \operatorname{Cov}[\mathbf{A}\mathbf{v}] = \mathbf{A} \mathbf{C} \mathbf{A}^\top
\end{equation}
\sphinxAtStartPar
where \(\mathbf{A}\) is a linear transformation. Thus, knowing that \(\mathbf{y}\) has a covariance equal to \(\sigma^2\mathbf{I}_n\), we get that
\label{equation:notebooks/review_linear_models:81e83adc-8d8a-4e27-b3f0-399016095ca9}\begin{align}
    \operatorname{Var}[\widehat{\boldsymbol{\beta}}] &= \operatorname{Var}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}]\\ &= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top(\sigma^2\mathbf{I}_n)((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top)^\top \\
    &= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{I}_n((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top)^\top \\
    &= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} \\
    &= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\end{align}
\sphinxAtStartPar
Thus, we have that our estimator is normally distributed around the true parameter vector as
\label{equation:notebooks/review_linear_models:ec085e71-650f-44dd-9c2f-4331b5887ed3}\begin{equation}
    \widehat{\boldsymbol{\beta}}|\mathbf{X} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})
\end{equation}

\subsection{First\sphinxhyphen{}order model example}
\label{\detokenize{notebooks/review_linear_models:first-order-model-example}}
\sphinxAtStartPar
Let’s create an example to show how we can manually implement OLS to fit a multiple linear regression model. First, we generate 1000 data points with two predictors \(\mathbf{x}_1\) and \(\mathbf{x}_2\), and \(y\) is calculated as
\begin{equation*}
\begin{split}\mathbf{y} = 3 + 4 \mathbf{x}_1 + 5 \mathbf{x}_2 + \text{noise}\end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate synthetic data with two predictors}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{X1} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X2} \PYG{o}{=} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{3} \PYG{o}{+} \PYG{l+m+mi}{4} \PYG{o}{*} \PYG{n}{X1} \PYG{o}{+} \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{X2} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
To estimate the regression coefficients, we first create the data matrix \(\mathbf{X}\), including a column of ones to account for the intercept term, and the two features \(\mathbf{x}_1\) and \(\mathbf{x}_2\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Combine the predictors into one matrix and adding a column of ones for the intercept}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, we can manually implement OLS and find the best\sphinxhyphen{}fitting parameters \(\widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Manually implement the OLS method}
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{X}\PYG{p}{)} \PYG{o}{@} \PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{y}

\PYG{c+c1}{\PYGZsh{} Print the estimated coefficients}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Intercept: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Coefficient for X1: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Coefficient for X2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Intercept: 3.079
Coefficient for X1: 3.963
Coefficient for X2: 4.956
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how the estimated coefficients are very close to the real ones. Now, we can visualise the fitted surface by creating a grid of values for \(\mathbf{x}_1\) and \(\mathbf{x}_2\) and computing the predicted \(\mathbf{y}\) values over this grid to generate the regression surface.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{mplot3d} \PYG{k+kn}{import} \PYG{n}{Axes3D}

\PYG{c+c1}{\PYGZsh{} Predictions for the surface plot}
\PYG{n}{x1\PYGZus{}surf}\PYG{p}{,} \PYG{n}{x2\PYGZus{}surf} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{y\PYGZus{}surf} \PYG{o}{=} \PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{+} \PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{n}{x1\PYGZus{}surf} \PYG{o}{+} \PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]} \PYG{o}{*} \PYG{n}{x2\PYGZus{}surf}

\PYG{c+c1}{\PYGZsh{} Plotting the data and the regression surface}
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{111}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{x1\PYGZus{}surf}\PYG{p}{,} \PYG{n}{x2\PYGZus{}surf}\PYG{p}{,} \PYG{n}{y\PYGZus{}surf}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{rstride}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{cstride}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fitted first\PYGZhy{}order surface}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{50031edeefcfedebc646e51e25192625474b75c2da4cd907df9626ddf98e4920}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Second\sphinxhyphen{}order model example}
\label{\detokenize{notebooks/review_linear_models:second-order-model-example}}
\sphinxAtStartPar
A second\sphinxhyphen{}order model is a model that extends the linear regression model to capture non\sphinxhyphen{}linear relationships by including quadratic terms. The model can be written as:
\begin{equation*}
\begin{split}
y_i = \beta_0 + \beta_1 x_i + \beta_2 x_i^2 + \epsilon_i
\end{split}
\end{equation*}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(y_i\) is the \(i\)\sphinxhyphen{}th observation of the response variable,

\item {} 
\sphinxAtStartPar
\(x_i\) is the \(i\)\sphinxhyphen{}th observation of the predictor,

\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept,

\item {} 
\sphinxAtStartPar
\(\beta_1\) is the linear coefficient,

\item {} 
\sphinxAtStartPar
\(\beta_2\) is the quadratic coefficient,

\item {} 
\sphinxAtStartPar
\(\epsilon_i\) represents the error term.

\end{itemize}

\sphinxAtStartPar
In matrix notation, for \(n\) observations and one predictor, the model can be expressed as:
\begin{equation*}
\begin{split}
\mathbf{y} = \mathbf{X} \boldsymbol{\beta} + \boldsymbol{\epsilon}
\end{split}
\end{equation*}
\sphinxAtStartPar
where:
\begin{equation*}
\begin{split}
\mathbf{y} = \begin{bmatrix} y_1 \\ y_2 \\ \vdots \\ y_n \end{bmatrix}, \quad \mathbf{X} = \begin{bmatrix} 1 & x_{11} & x_{11}^2 \\ 1 & x_{21} & x_{21}^2 \\ \vdots & \vdots & \vdots \\ 1 & x_{n1} & x_{n1}^2 \end{bmatrix}, \quad \boldsymbol{\beta} = \begin{bmatrix} \beta_0 \\ \beta_1 \\ \beta_2 \end{bmatrix}, \quad \boldsymbol{\epsilon} = \begin{bmatrix} \epsilon_1 \\ \epsilon_2 \\ \vdots \\ \epsilon_n \end{bmatrix}
\end{split}
\end{equation*}
\sphinxAtStartPar
Then, once we have augmented the matrix \(\mathbf{X}\) to include quadratic terms, we can find the regression coefficients using the OLS estimate, as we did in the first\sphinxhyphen{}order case. It is easy to see how this is also a valid approach when we have more than one predictor and multiple quadratic terms.

\sphinxAtStartPar
Let’s now generate some data from the following second\sphinxhyphen{}order model:
\begin{equation*}
\begin{split}\mathbf{y} = 2 + 3\mathbf{x} + 5 \mathbf{x}^2 + \text{noise}\end{split}
\end{equation*}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{X} \PYG{o}{+} \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Quadratic function with noise}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we did before, we create the data matrix \(\mathbf{X}\) by stacking a column of ones, and two more columns: one for the original feature \(\mathbf{x}\), and one for its squared version \(\mathbf{x}^2\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Prepare the design matrix for second\PYGZhy{}order polynomial regression}
\PYG{n}{X\PYGZus{}poly} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{X}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, we can find the estimated coefficients using the OLS formula.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Manually implement the OLS method}
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{X\PYGZus{}poly}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{X\PYGZus{}poly}\PYG{p}{)} \PYG{o}{@} \PYG{n}{X\PYGZus{}poly}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{y}

\PYG{c+c1}{\PYGZsh{} Print the estimated coefficients}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Intercept: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Coefficient for x: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Coefficient for x\PYGZca{}2: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{beta}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Intercept: 2.341
Coefficient for x: 2.937
Coefficient for x\PYGZca{}2: 4.548
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Finally, we cna plot the fitted curve.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Predictions for plotting}
\PYG{n}{X\PYGZus{}new} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X\PYGZus{}new\PYGZus{}poly} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X\PYGZus{}new}\PYG{p}{,} \PYG{n}{X\PYGZus{}new}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{]}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{X\PYGZus{}new\PYGZus{}poly}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{beta}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting the data and the polynomial regression model}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}new}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Fitted second\PYGZhy{}order model}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{a07e9ab1c7d62dda18bd0325fb8cbcbaf5f099d6506b8d55f4d90147531a155c}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Hypothesis testing for regression coefficients}
\label{\detokenize{notebooks/review_linear_models:hypothesis-testing-for-regression-coefficients}}

\subsection{Starting with an example}
\label{\detokenize{notebooks/review_linear_models:starting-with-an-example}}
\sphinxAtStartPar
In linear regression, hypothesis testing is used to determine whether the relationship between the independent variables (predictors) and the dependent variable (response) is statistically significant. The null hypothesis (\(H_0\)) typically states that there is no relationship between the predictor and the response, i.e., the coefficient for the predictor is zero. The alternative hypothesis (\(H_1\)) states that there is a significant relationship, i.e., the coefficient is not zero.

\sphinxAtStartPar
Let’s generate a synthetic dataset with multiple predictors, fit a multiple linear regression model, and perform hypothesis testing on the coefficients to determine their significance. We will generate a dataset with multiple predictors and some extra predictors that are not significant to test the hypothesis testing process.

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step procedure}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Generate synthetic data with multiple predictors. The true coefficients are \sphinxcode{\sphinxupquote{{[}5, 3, \sphinxhyphen{}2, 1, 0, 0{]}}}, where some predictors have zero coefficients to simulate non\sphinxhyphen{}significant predictors. The response variable is generated as \(y = 5 + 3X_1 - 2X_2 + X_3 + \text{noise}\), where the noise is normally distributed with mean 0 and standard deviation 1.

\item {} 
\sphinxAtStartPar
Fit a multiple linear regression model using \sphinxcode{\sphinxupquote{statsmodels}}.

\item {} 
\sphinxAtStartPar
Interpret the regression summary table, including the p\sphinxhyphen{}values.

\item {} 
\sphinxAtStartPar
Explain the formulas for p\sphinxhyphen{}value computation.

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} True coefficients}
\PYG{n}{beta} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Including some zero coefficients for non\PYGZhy{}significant predictors}

\PYG{c+c1}{\PYGZsh{} Generate random predictors}
\PYG{n}{X1} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X2} \PYG{o}{=} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X3} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
\PYG{n}{X4} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}
\PYG{n}{X5} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{X6} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Combine predictors into a matrix}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{c\PYGZus{}}\PYG{p}{[}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{,} \PYG{n}{X3}\PYG{p}{,} \PYG{n}{X4}\PYG{p}{,} \PYG{n}{X5}\PYG{p}{,} \PYG{n}{X6}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Generate response variable with some noise}
\PYG{n}{y} \PYG{o}{=} \PYG{l+m+mi}{5} \PYG{o}{+} \PYG{l+m+mi}{3}\PYG{o}{*}\PYG{n}{X1} \PYG{o}{+} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{o}{*}\PYG{n}{X2} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{o}{*}\PYG{n}{X3} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Convert to pandas DataFrame for ease of use with statsmodels}
\PYG{n}{df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X4}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X5}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X6}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{y}

\PYG{c+c1}{\PYGZsh{} Add a constant term (intercept) to the predictors}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{df}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit the model using statsmodels}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print the summary of the regression model}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{results}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R\PYGZhy{}squared:                       0.860
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.859
Method:                 Least Squares   F\PYGZhy{}statistic:                     1013.
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):               0.00
Time:                        19:11:20   Log\PYGZhy{}Likelihood:                \PYGZhy{}1415.6
No. Observations:                1000   AIC:                             2845.
Df Residuals:                     993   BIC:                             2879.
Df Model:                           6                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const          5.0404      0.124     40.701      0.000       4.797       5.283
X1             2.9303      0.054     53.805      0.000       2.823       3.037
X2            \PYGZhy{}1.9891      0.035    \PYGZhy{}56.372      0.000      \PYGZhy{}2.058      \PYGZhy{}1.920
X3             0.9598      0.111      8.681      0.000       0.743       1.177
X4            \PYGZhy{}0.0396      0.055     \PYGZhy{}0.716      0.474      \PYGZhy{}0.148       0.069
X5            \PYGZhy{}0.1008      0.112     \PYGZhy{}0.897      0.370      \PYGZhy{}0.321       0.120
X6             0.1066      0.109      0.978      0.328      \PYGZhy{}0.107       0.321
==============================================================================
Omnibus:                        2.438   Durbin\PYGZhy{}Watson:                   2.039
Prob(Omnibus):                  0.296   Jarque\PYGZhy{}Bera (JB):                2.178
Skew:                           0.019   Prob(JB):                        0.337
Kurtosis:                       2.775   Cond. No.                         11.5
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{summary()}} function from \sphinxcode{\sphinxupquote{statsmodels}} provides detailed information about the regression model, including coefficients, standard errors, t\sphinxhyphen{}values, and p\sphinxhyphen{}values for each predictor.

\sphinxAtStartPar
\sphinxstylestrong{How to read this table}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Dep. Variable: y}} This is the response variable that the model is trying to predict.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{R\sphinxhyphen{}squared: 0.860}} indicates the proportion of the variance in the dependent variable that is predictable from the independent variables.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Adj. R\sphinxhyphen{}squared: 0.859}} adjusts for the number of predictors in the model.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{F\sphinxhyphen{}statistic: 1013.}} tests the overall significance of the model, a very small p\sphinxhyphen{}value (\sphinxcode{\sphinxupquote{Prob (F\sphinxhyphen{}statistic): 0.00}}) indicates that at least one predictor is significantly related to the response variable.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Log\sphinxhyphen{}Likelihood: \sphinxhyphen{}1415.6}} can be used in model comparison. Higher values (closer to zero) indicate a better fit.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{No. Observations: 1000}} is total number of data points used in the regression.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{AIC: 2845.}} and \sphinxcode{\sphinxupquote{BIC: 2879.}} are information criteria used for model comparison. Lower values indicate a better fit.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{Df Residuals: 993}} and \sphinxcode{\sphinxupquote{Df Model: 6}} are the degrees of freedom for the residuals and the model, respectively.

\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{coefficients table} is the main part of the output and it includes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{coef}} the coefficient estimates for the intercept term and for each predictor.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{std err}} measure the variability of the coefficient estimates. Smaller values indicate more precise estimates.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{t}} the t\sphinxhyphen{}values computed as the ratio of the coefficient to its standard error: \(t = \frac{\hat{\beta}}{SE(\hat{\beta})}\).

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{P>|t|}} the p\sphinxhyphen{}value which indicates the probability of observing a t\sphinxhyphen{}value at least as extreme as the computed one if the null hypothesis (\(\beta = 0\)) is true. A small p\sphinxhyphen{}value (typically < 0.05) suggests that the coefficient is significantly different from zero.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{{[}0.025 0.975{]}}} the confidence intervals provide a range of values within which the true coefficient is likely to fall with 95\% confidence.

\end{itemize}

\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{diagnostic tests} part include tests that check for various assumptions and properties of the residuals.
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Omnibus}: Tests for skewness and kurtosis. A small value indicates normality.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Durbin\sphinxhyphen{}Watson}: Tests for autocorrelation in residuals. Values around 2 indicate no autocorrelation.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Jarque\sphinxhyphen{}Bera}: Another test for normality. A small value indicates normality

\end{itemize}

\end{enumerate}


\subsection{Some background about hypothesis testing and confidence intervals}
\label{\detokenize{notebooks/review_linear_models:some-background-about-hypothesis-testing-and-confidence-intervals}}

\subsubsection{Test for overall significance of the model}
\label{\detokenize{notebooks/review_linear_models:test-for-overall-significance-of-the-model}}
\sphinxAtStartPar
The test for significance of regression is a test to determine if there is a linear relationship between the response variable \(y\) and a subset of the regressor variables \(x_1, x_2,\ldots, x_p\). The appropriate hypotheses are
\label{equation:notebooks/review_linear_models:9f491bea-24f0-446e-a048-bde41d70fd77}\begin{align}
    H_0 &: \beta_1 = \beta_2 = \ldots = \beta_p = 0 \\
    H_1 &: \beta_j \neq 0 \quad \text{for at least one } j
\end{align}
\sphinxAtStartPar
This test procedure is called an analysis of variance (ANOVA) because it is based on a decomposition of the total variability in the response variable \(y\). The total variability in the response variable is measured by the total sum of squares (\(\operatorname{SS}_T\)), calculated as
\label{equation:notebooks/review_linear_models:85b5b1a7-a9c1-4de4-8f1c-01c59c49e319}\begin{equation}
    \operatorname{SS}_T = \sum_{i=1}^{n} (y_i - \bar{y})^2
\end{equation}
\sphinxAtStartPar
which is used as a measure of overall variability in the data. Intuitively, this is reasonable because if we were to divide \(\operatorname{SS}_T\) by the appropriate number of degrees of freedom, we would have the sample variance of the \(y\)s. This \(\operatorname{SS}_T\)  is partitioned into
\label{equation:notebooks/review_linear_models:e5959e6a-56ca-48ea-8d7e-e9f1ad80afcd}\begin{equation}
    \operatorname{SS}_T = \operatorname{SS}_M + \operatorname{SS}_E
\end{equation}
\sphinxAtStartPar
where \(\operatorname{SS}_M\) is the sum of squares due to the regression model(\(\operatorname{SS}_M\)), measuring the explained variation in \( y \), calculated as \( \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \); and \(\operatorname{SS}_E\) is the sum of squares due to error (\(\operatorname{SS}_E\)), measuring the unexplained variation in \( y \), calculated as \( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \). \(\operatorname{SS}_M\) measures the explained variability in the dependent variable due to the independent variables in the regression model. Under the null hypothesis (which typically states that all regression coefficients except the intercept are zero), \(\operatorname{SS}_M\) captures variability that is purely due to chance. To reject the null hypotheses and say that a significant part of the variability is explained by the model (thus justifying the existence of at least one coefficient), we would like \(\operatorname{SS}_M\) to be significantly larger than \(\operatorname{SS}_R\). More formally, we use the test statistic is given by
\label{equation:notebooks/review_linear_models:2584ea19-d2c0-4c4c-8f90-ba1f2cb6c0e1}\begin{equation}
    F_0 = \frac{\operatorname{SS}_M/p}{\operatorname{SS}_E/(n-p-1)} = \frac{\operatorname{MS}_M}{\operatorname{MS}_E}
\end{equation}
\sphinxAtStartPar
Since the residuals are assumed to be normally distributed, \(\operatorname{SS}_M\) and \(\operatorname{SS}_E\) follow a chi\sphinxhyphen{}squared distribution, and their ratio follows an F\sphinxhyphen{}distribution. This is why the critical value for the test is given \(F_{\alpha, p, n-p-1}\), where \(\alpha\) is the confidence level. We reject the null hypothesis \(H_0\) if \(F_0 > F_{\alpha, p, n-p-1}\). \textbackslash{}textbf\{ANOVA one factor at the time:\} If we have five factors (A, B, C, D, E) and we want to find which factors are significant for a given significance level \(\alpha\), we compare \(MS_i/MS_E\) for \(i=A, \ldots, E\) where the \(MS_E\) is usually the pure error that comes from replications (or the whole error?).


\subsubsection{Tests on individual coefficients}
\label{\detokenize{notebooks/review_linear_models:tests-on-individual-coefficients}}
\sphinxAtStartPar
Adding a variable to the regression model always causes the sum of squares for the regression model (\(\operatorname{SS}_M\)) to increase and the error sum of squares to decrease. The hypotheses for testing the significance of any individual regression coefficient \(\beta_j\) are
\label{equation:notebooks/review_linear_models:c8eec338-9134-407f-9495-b51510c24640}\begin{align}
    H_0 &: \beta_j = 0 \\
    H_1 &: \beta_j \neq 0
\end{align}
\sphinxAtStartPar
If \(H_0\) is not rejected, then this indicates that \(x_j\) can be deleted from the model. However, note that this is truly a partial or marginal test, because the regression coefficients depend on all the other regressor variables \(x_i\), with \(i \neq j\), that are in the model. Because we know that
\label{equation:notebooks/review_linear_models:1b7760bb-d554-4681-a73d-3ecc656764cc}\begin{equation}
    \widehat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})
\end{equation}
\sphinxAtStartPar
For the \(i\)th coefficient, we do have that
\label{equation:notebooks/review_linear_models:66eb9c09-db97-4480-8067-b8654aebe4e6}\begin{equation}
    \widehat{\beta}_i \sim \mathcal{N}(\beta_i, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii})
\end{equation}
\sphinxAtStartPar
where \(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}\) is the diagonal element of the covariance matrix \(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\) corresponding to \(b_j\). The test statistic is given by
\label{equation:notebooks/review_linear_models:21ee71ba-42fc-49f1-8798-3112ec4345d4}\begin{equation}
    t_0 = \frac{\widehat{\beta}_i}{\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}}}
\end{equation}
\sphinxAtStartPar
The denominator \(\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}}\) is also called standard error of the regression coefficients \(\widehat{\beta}_i\), so we can also write it as
\label{equation:notebooks/review_linear_models:abd2131a-ec68-4a3d-8fa3-1818a543fa2f}\begin{equation}
    t_0 = \frac{\widehat{\beta}_i}{\text{se}(\widehat{\beta}_i)}
\end{equation}
\sphinxAtStartPar
The distribution is a Student’s t distribution because the coefficients itself is normally distributed, while the estimated variance has a Chi\sphinxhyphen{}square distribution. So the ratio between a normal and the squared root of a Chi\sphinxhyphen{}square follows a t distribution.


\subsubsection{Confidence intervals on individual regression coefficients}
\label{\detokenize{notebooks/review_linear_models:confidence-intervals-on-individual-regression-coefficients}}
\sphinxAtStartPar
A confidence interval, from the frequentist standpoint, is an interval estimate of a parameter \(\beta_j\) that, if the same data collection and analysis procedure were repeated many times, would contain the true parameter value a certain percentage (e.g., 95\%) of the time \(\beta_j\). This is because in frequentist statistics, parameters are considered fixed but unknown quantities. The data are random, which means the calculated confidence interval varies from sample to sample. Saying a 95\% confidence interval for a parameter is \([a, b]\) means that if we were to repeat the study many times, 95\% of such calculated intervals would contain the true parameter value. It does not mean there is a 95\% probability that the true parameter lies within that specific interval for the observed data. Conversely, Bayesian methods treat parameters as random variables and provide a probability distribution (the posterior distribution) that quantifies uncertainty about parameter values given the observed data. The uncertainty about a parameter is directly quantified by its posterior distribution. A 95\% credible interval from Bayesian analysis means there is a 95\% probability that the parameter lies within this interval, given the data and the prior information. Frequentist confidence intervals do not allow for probabilistic statements about the parameter being within the interval for a given dataset. In contrast, Bayesian credible intervals provide a probability that the parameter lies within the interval, given the data and prior.

\sphinxAtStartPar
So far, we have demonstrated that \(\widehat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})\). This also implied that the marginal distribution for any regression coefficient is \(\widehat{\beta}_j \sim \mathcal{N}(\beta_j, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj})\). Then, we can say that
\label{equation:notebooks/review_linear_models:4be623b2-cf62-42fa-9459-6b11989c66c5}\begin{equation}
    \frac{\widehat{\beta}_j - \beta_j}{\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}} \sim \mathcal{N}(0, 1) 
\end{equation}
\sphinxAtStartPar
Since we do not know \(\sigma^2\), we used its estimate \(\widehat{\sigma}^2\) obtained through the \(\operatorname{MS}_E\), which has a \(\chi^2\) distribution with \(n-p\) degrees of freedom. Thus, we have
\label{equation:notebooks/review_linear_models:a7cdf482-9cf9-42be-9fbe-911f764ccc93}\begin{equation}
    \frac{\widehat{\beta}_j - \beta_j}{\sqrt{\widehat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}} \sim t_{n-p}
\end{equation}
\sphinxAtStartPar
We can then define a 100\((1-\alpha)\%\) confidence interval for the regression coefficient \(\beta_j\), \(j = 0,1,\ldots, p\), as
\label{equation:notebooks/review_linear_models:39530e26-f6e2-4dff-b2a4-ee3e69cc7e28}\begin{equation}
    \widehat{\beta}_j - t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}} \leq \beta_j \leq \widehat{\beta}_j + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}
\end{equation}
\sphinxAtStartPar
where \(t_{\alpha/2, n-p}\) is the critical value from the t\sphinxhyphen{}distribution for \(\alpha/2\) and \(n-p\) degrees of freedom. This can also be written in terms of the standard error of the estimated coefficient \(b_j\) as
\label{equation:notebooks/review_linear_models:59d4cbea-239b-40af-bc08-648eb6980d11}\begin{equation}
    \widehat{\beta}_j - t_{\alpha/2, n-p} \text{se}(\widehat{\beta}_j) \leq \beta_j \leq \widehat{\beta}_j + t_{\alpha/2, n-p} \text{se}(\widehat{\beta}_j)
\end{equation}

\subsubsection{Confidence interval on the mean response and prediction variance}
\label{\detokenize{notebooks/review_linear_models:confidence-interval-on-the-mean-response-and-prediction-variance}}
\sphinxAtStartPar
We can define a confidence interval on the mean response at a particular point \(\mathbf{x}_0\). The fitted value at this particular point is
\label{equation:notebooks/review_linear_models:636e07a9-6892-433a-ba10-1772dc8846fd}\begin{equation}
    \widehat{y}_0 = \mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}
\end{equation}
\sphinxAtStartPar
Because we have \(\mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2\mathbf{I}_n)\), suggesting how \(\mathbb{E}[y|\mathbf{X}] = \mathbf{X} \boldsymbol{\beta}\), we also have that \(\mathbb{E}[y|\mathbf{x}_0] = \mathbf{x}_0^\top \boldsymbol{\beta}\). Then, we have
\label{equation:notebooks/review_linear_models:2e8cf960-fb19-4746-bb0b-1566c3a876ec}\begin{equation}
     \mathbb{E}[\widehat{y}_0] = \mathbb{E}[\mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}] = \mathbf{x}_0^\top \boldsymbol{\beta}
\end{equation}
\sphinxAtStartPar
because \(\mathbb{E}[\widehat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\). Thus, \(\widehat{y}_0\) is an unbiased estimator of \(\mathbb{E}[y|\mathbf{x}_0]\). Since \(\widehat{y}_0\) is a linear combination of \(\widehat{\boldsymbol{\beta}}\), which has a variance of \(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\), the variance of \(\widehat{y}_0\) is given by
\label{equation:notebooks/review_linear_models:657e1ca5-2df5-4d01-8a0f-7b61df49b4c9}\begin{equation}
    \operatorname{Var}[\widehat{y}_0] = \operatorname{Var}\left[\mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}\right] = \mathbf{x}_0^\top(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})\mathbf{x}_0 = \sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}
\sphinxAtStartPar
Then, we can define the 100(1\sphinxhyphen{}\(\alpha\))\% confidence interval on the mean response at point \(\mathbf{x}_0\) as
\label{equation:notebooks/review_linear_models:033c56ee-348a-478b-abce-7a268daac609}\begin{equation}
    \widehat{y}_0 - t_{\alpha/2, n-p} \sqrt{\sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0} \leq \mathbb{E}[y|\mathbf{x}_0] \leq \widehat{y}_0 + t_{\alpha/2, n-p} \sqrt{\sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0}
\end{equation}
\sphinxAtStartPar
In general, we can define the \sphinxstylestrong{prediction variance (PV)} of the fitted value at location \(\mathbf{x}_0\) as
\label{equation:notebooks/review_linear_models:d058c34a-e80f-415f-9ac4-38b5413bf5b8}\begin{equation}
    \operatorname{PV} = \operatorname{Var}[\widehat{y}_0] = \sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}
\sphinxAtStartPar
Then, the \sphinxstylestrong{unscaled prediction variance (UPV)} is given by
\label{equation:notebooks/review_linear_models:15761646-905a-4c51-846b-da527f21ceda}\begin{equation}
    \operatorname{UPV} = \frac{\operatorname{Var}[\widehat{y}_0]}{\sigma^2} = \mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}
\sphinxAtStartPar
The main benefit of the UPV is that it can be used during the design selection phase. The division
by \(\sigma^2\) means that \textbackslash{}underline\{UPV is a function of only the design matrix\} and does not require data to have been collected. UPV provides a way to quantify the precision of the predictions made by a regression model at specific points. It tells us how much the predicted value at a certain point \(\mathbf{x}_0\) is expected to vary, due to the variability in the estimated regression coefficients, but without considering the inherent variability of the response itself. The key intuition is that since \(\widehat{\boldsymbol{\beta}}\) carries variance, any prediction made using these estimates also inherits variance (i.e., uncertainty propagation?). UPV connects parameter estimation to prediction variance by translating the uncertainty in the model’s coefficients into the uncertainty of the model’s predictions. If we have already collected data, we can try to estimate \(\sigma^2\) using the \(\operatorname{MS}_E\). Then, we can define the \sphinxstylestrong{estimated prediction variance (EPV)} as
\label{equation:notebooks/review_linear_models:065195b0-0ee1-4057-91a2-f0c50905ab49}\begin{equation}
    \operatorname{EPV} = \widehat{\sigma}^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}

\section{Regularization methods}
\label{\detokenize{notebooks/review_linear_models:regularization-methods}}
\sphinxAtStartPar
Regularization techniques are fundamental to the field of machine learning and statistics, primarily used to prevent overfitting, improve model generalization, and handle multicollinearity among predictors. We will now briefly introduce three primary methods for regularization.


\subsection{Lasso Regression (L1 Penalty)}
\label{\detokenize{notebooks/review_linear_models:lasso-regression-l1-penalty}}
\sphinxAtStartPar
The least absolute shrinkage and selection operator (lasso) regression introduces a penalty term equal to the absolute value of the magnitude of coefficients to the loss function. The objective function of Lasso regression is given by:
\label{equation:notebooks/review_linear_models:34a7514f-ca18-43ab-8ad8-2a47678c6fd0}\begin{equation}
    \text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\},
\end{equation}
\sphinxAtStartPar
where \(\lambda\) is a regularization parameter that controls the strength of the penalty.

\sphinxAtStartPar
\sphinxstylestrong{Key aspects}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Promotes sparsity in the model coefficients, effectively performing feature selection.

\item {} 
\sphinxAtStartPar
Particularly useful when dealing with high\sphinxhyphen{}dimensional data or when feature selection is desired.

\item {} 
\sphinxAtStartPar
Can result in models that are easier to interpret due to fewer predictors.

\end{itemize}


\subsection{Ridge Regression (L2 Penalty)}
\label{\detokenize{notebooks/review_linear_models:ridge-regression-l2-penalty}}
\sphinxAtStartPar
Ridge regression adds a penalty equal to the square of the magnitude of coefficients to the loss function. The formulation of Ridge regression is as follows:
\label{equation:notebooks/review_linear_models:46c95f90-50b4-46d8-b3e9-d33ace00e2b1}\begin{equation}
    \text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\},
\end{equation}
\sphinxAtStartPar
where \(\lambda\) is the regularization parameter.

\sphinxAtStartPar
\sphinxstylestrong{Key aspects}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Reduces model complexity by shrinking coefficients, but does not necessarily reduce them to zero.

\item {} 
\sphinxAtStartPar
Particularly beneficial in situations with multicollinearity among predictors.

\item {} 
\sphinxAtStartPar
Helps to stabilize the coefficient estimates and improve model generalization.

\end{itemize}


\subsection{Elastic Net (combination of L1 and L2 penalties)}
\label{\detokenize{notebooks/review_linear_models:elastic-net-combination-of-l1-and-l2-penalties}}
\sphinxAtStartPar
Elastic Net combines the penalties of Lasso and Ridge, incorporating both the L1 and L2 penalty terms. The objective function for Elastic Net is:
\label{equation:notebooks/review_linear_models:a7fd1cec-4394-4023-b497-8c84a52bebd4}\begin{equation}
    \text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \frac{\lambda_2}{2} \sum_{j=1}^{p} \beta_j^2 \right\},
\end{equation}
\sphinxAtStartPar
where \(\lambda_1\) and \(\lambda_2\) are parameters that control the strength of the L1 and L2 penalties, respectively.

\sphinxAtStartPar
\sphinxstylestrong{Key aspects}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Combines the feature selection properties of Lasso with the ridge regression’s ability to handle multicollinearity.

\item {} 
\sphinxAtStartPar
Encourages a grouping effect, where correlated predictors either enter or leave the model together.

\item {} 
\sphinxAtStartPar
Offers a balance between Lasso and Ridge regression by allowing for control over both penalties, making it versatile for various scenarios.

\item {} 
\sphinxAtStartPar
Useful in high\sphinxhyphen{}dimensional data scenarios where Lasso might suffer due to high correlations among predictors.

\end{itemize}

\sphinxAtStartPar
Each of these regularization methods has its unique strengths and applications, and the choice among them should be guided by the specific characteristics of the data and the modeling objectives at hand.

\sphinxstepscope


\chapter{Machine Learning}
\label{\detokenize{notebooks/review_ML:machine-learning}}\label{\detokenize{notebooks/review_ML::doc}}
\sphinxAtStartPar
This chapter provides a very quick overview of the main concepts in machine learning, mostly to refresh some of the concepts or models that will be used throughout this book.


\section{Supervised learning vs. unsupervised learning}
\label{\detokenize{notebooks/review_ML:supervised-learning-vs-unsupervised-learning}}
\sphinxAtStartPar
In supervised learning, the model is trained on a labeled dataset, meaning that each training example is composed by input features (\(\mathbf{X}\)) and labels (\(\mathbf{y}\)). The goal is to learn the function relating the labels to the input features. Common tasks in supervised learning include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regression}: predicting a continuous value. Example: Predicting house prices based on features like size, location, and age.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Classification}: predicting a discrete label. Example: Identifying whether an email is spam or not based on its content.

\end{itemize}

\sphinxAtStartPar
In unsupervised learning, the model is usually trained on unlabeled examples (only the features \(\mathbf{X}\)). The goal can be to infer the latent structure present within a set of data points. Common tasks in unsupervised learning include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Clustering}: grouping similar data points together. Example: Customer segmentation based on purchasing behavior.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dimensionality Reduction}: reducing the number of features while retaining the most important information. Example: Principal Component Analysis (PCA) for visualizing high\sphinxhyphen{}dimensional data.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{cluster} \PYG{k+kn}{import} \PYG{n}{KMeans}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data for regression (supervised learning)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{X\PYGZus{}reg} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y\PYGZus{}reg} \PYG{o}{=} \PYG{l+m+mi}{4} \PYG{o}{+} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{X\PYGZus{}reg} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train a linear regression model}
\PYG{n}{lin\PYGZus{}reg} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{lin\PYGZus{}reg}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}reg}\PYG{p}{,} \PYG{n}{y\PYGZus{}reg}\PYG{p}{)}
\PYG{n}{y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{lin\PYGZus{}reg}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}reg}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data for clustering (unsupervised learning)}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{X\PYGZus{}cluster} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{300}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Define true cluster centers}
\PYG{n}{true\PYGZus{}centers} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.8}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} Assign data points to clusters}
\PYG{n}{y\PYGZus{}cluster} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{X\PYGZus{}cluster}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{newaxis}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{true\PYGZus{}centers}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Train a KMeans clustering model}
\PYG{n}{kmeans} \PYG{o}{=} \PYG{n}{KMeans}\PYG{p}{(}\PYG{n}{n\PYGZus{}clusters}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}cluster}\PYG{p}{)}
\PYG{n}{y\PYGZus{}kmeans} \PYG{o}{=} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}cluster}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot regression results}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{14}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X\PYGZus{}reg}\PYG{p}{,} \PYG{n}{y\PYGZus{}reg}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Data points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}reg}\PYG{p}{,} \PYG{n}{y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regression line}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Supervised Learning: Linear Regression}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Feature (X)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Target (y)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot clustering results}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{colors} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X\PYGZus{}cluster}\PYG{p}{[}\PYG{n}{y\PYGZus{}kmeans} \PYG{o}{==} \PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}cluster}\PYG{p}{[}\PYG{n}{y\PYGZus{}kmeans} \PYG{o}{==} \PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{n}{colors}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cluster }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{cluster\PYGZus{}centers\PYGZus{}}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{cluster\PYGZus{}centers\PYGZus{}}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Draw circle around each cluster}
    \PYG{n}{cluster\PYGZus{}center} \PYG{o}{=} \PYG{n}{kmeans}\PYG{o}{.}\PYG{n}{cluster\PYGZus{}centers\PYGZus{}}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
    \PYG{n}{cluster\PYGZus{}points} \PYG{o}{=} \PYG{n}{X\PYGZus{}cluster}\PYG{p}{[}\PYG{n}{y\PYGZus{}kmeans} \PYG{o}{==} \PYG{n}{i}\PYG{p}{]}
    \PYG{n}{radius} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{norm}\PYG{p}{(}\PYG{n}{cluster\PYGZus{}points} \PYG{o}{\PYGZhy{}} \PYG{n}{cluster\PYGZus{}center}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{circle} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{Circle}\PYG{p}{(}\PYG{n}{cluster\PYGZus{}center}\PYG{p}{,} \PYG{n}{radius}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{colors}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{gca}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{add\PYGZus{}patch}\PYG{p}{(}\PYG{n}{circle}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unsupervised Learning: KMeans Clustering}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Feature 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Feature 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{ce126958fdc8a912067ec41aaa211b08f9512636ca98000b6afa8beb2c25d5cf}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Overfitting and underfitting}
\label{\detokenize{notebooks/review_ML:overfitting-and-underfitting}}
\sphinxAtStartPar
Overfitting occurs when a model learns not only the underlying patterns in the training data but also the noise and random fluctuations. This excessive learning leads the model to perform exceptionally well on the training data, but poorly on unseen or new data, such as validation or test datasets. Overfitting is more likely to happen when the model is overly complex relative to the amount of training data it has been given. Complexity can come from having too many features, using high\sphinxhyphen{}degree polynomial models, or incorporating too many parameters. The characteristics of overfitting are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{High training accuracy, low test accuracy}: the model fits the training data very well but generalizes poorly to new data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Complex models}: models with a large number of parameters or high\sphinxhyphen{}degree polynomials are prone to overfitting.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Noise fitting}: the model captures noise and random variations in the training data as if they were true patterns.

\end{itemize}

\sphinxAtStartPar
Mitigation strategies might include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regularization}: techniques like Lasso (L1 penalty), Ridge (L2 penalty), and Elastic Net add a penalty for larger coefficients, thus discouraging overly complex models.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pruning}: in decision trees, pruning helps by cutting off branches that have little importance.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cross\sphinxhyphen{}validation}: using techniques like k\sphinxhyphen{}fold cross\sphinxhyphen{}validation ensures that the model performs well on different subsets of the data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simpler models}: choosing simpler models with fewer parameters can reduce the risk of overfitting.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{More data}: increasing the size of the training data can help the model generalize better.

\end{itemize}


\subsection{Underfitting}
\label{\detokenize{notebooks/review_ML:underfitting}}
\sphinxAtStartPar
Underfitting occurs when a model is too simple to capture the underlying patterns and structure of the data. This simplicity can arise from using too few features, having a low\sphinxhyphen{}degree polynomial, or having an overly simplistic model structure. An underfitted model will perform poorly on both the training data and new data because it fails to capture the essential trends and relationships within the data. The characteristics of underfitting are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Low training accuracy, low test accuracy}: the model does not perform well even on the training data, indicating it has not captured the underlying patterns.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simple models}: models with too few parameters, low\sphinxhyphen{}degree polynomials, or insufficient features.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bias}: the model has high bias and cannot adequately represent the complexity of the data.

\end{itemize}

\sphinxAtStartPar
Mitigation strategies might include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Complexity}: increase the complexity of the model by adding more features, using higher\sphinxhyphen{}degree polynomials, or more complex algorithms.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feature engineering}: create new features that better capture the underlying patterns in the data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Model tuning}: adjust hyperparameters to better fit the data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Increase training}: ensure the model is adequately trained by allowing more iterations or using more sophisticated training techniques.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{PolynomialFeatures}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{metrics} \PYG{k+kn}{import} \PYG{n}{mean\PYGZus{}squared\PYGZus{}error}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{num}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{0.2}

\PYG{c+c1}{\PYGZsh{} Split data into training and test sets}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Function to plot model}
\PYG{k}{def} \PYG{n+nf}{plot\PYGZus{}models}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{,} \PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{degree} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{,} \PYG{n}{start}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{polynomial\PYGZus{}features} \PYG{o}{=} \PYG{n}{PolynomialFeatures}\PYG{p}{(}\PYG{n}{degree}\PYG{o}{=}\PYG{n}{degree}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}train\PYGZus{}poly} \PYG{o}{=} \PYG{n}{polynomial\PYGZus{}features}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}test\PYGZus{}poly} \PYG{o}{=} \PYG{n}{polynomial\PYGZus{}features}\PYG{o}{.}\PYG{n}{fit\PYGZus{}transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{)}
        
        \PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}poly}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}train\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}train\PYGZus{}poly}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}test\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}test\PYGZus{}poly}\PYG{p}{)}
        
        \PYG{c+c1}{\PYGZsh{} Plotting the fitted line using the whole range of X for a smoother curve}
        \PYG{n}{X\PYGZus{}plot} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{min}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{max}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}plot\PYGZus{}poly} \PYG{o}{=} \PYG{n}{polynomial\PYGZus{}features}\PYG{o}{.}\PYG{n}{transform}\PYG{p}{(}\PYG{n}{X\PYGZus{}plot}\PYG{p}{)}
        \PYG{n}{y\PYGZus{}plot\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}plot\PYGZus{}poly}\PYG{p}{)}
        
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{)}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Training data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Test data}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X\PYGZus{}plot}\PYG{p}{,} \PYG{n}{y\PYGZus{}plot\PYGZus{}pred}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Model prediction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{4}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Polynomial Degree: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{degree}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{Train MSE: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}squared\PYGZus{}error}\PYG{p}{(}\PYG{n}{y\PYGZus{}train}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{y\PYGZus{}train\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{, Test MSE: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{mean\PYGZus{}squared\PYGZus{}error}\PYG{p}{(}\PYG{n}{y\PYGZus{}test}\PYG{p}{,}\PYG{+w}{ }\PYG{n}{y\PYGZus{}test\PYGZus{}pred}\PYG{p}{)}\PYG{l+s+si}{:}\PYG{l+s+s1}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
    
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting models}
\PYG{n}{degrees} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{15}\PYG{p}{]}
\PYG{n}{plot\PYGZus{}models}\PYG{p}{(}\PYG{n}{degrees}\PYG{p}{,} \PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}test}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{8452279d289841aeeeb404114fb01aab1e8eb87755042a22e749b7b7cd27a5c6}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The three plots above illustrate the concepts of underfitting, good fit, and overfitting using polynomial regression models of varying degrees.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Underfitting}: the first plot shows a linear regression model (polynomial degree 1). The model is too simple to capture the underlying nonlinear relationship in the data, resulting in high bias. This is evident from the high training and test mean squared error (MSE). The model fails to capture the curve in the data, leading to underfitting.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Good fit}: the second plot represents a polynomial regression model with degree 4. This model captures the underlying pattern of the data well, striking a balance between bias and variance. The training and test MSEs are relatively low, indicating that the model generalizes well to new data. This is an example of an optimal fit where the model complexity is appropriate for the data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Overfitting}: the third plot shows a polynomial regression model with degree 15. The model is overly complex, capturing not only the underlying pattern but also the noise in the training data. This leads to low training MSE but significantly higher test MSE, as the model does not generalize well to new data. This high variance results in poor performance on the test set, exemplifying overfitting.

\end{enumerate}

\sphinxAtStartPar
These plots clearly demonstrate how model complexity affects the ability to generalize, emphasizing the importance of finding a balance to avoid both underfitting and overfitting.


\section{Tree\sphinxhyphen{}based methods}
\label{\detokenize{notebooks/review_ML:tree-based-methods}}
\sphinxAtStartPar
So far, we mostly covered the use of linear models. Let’s now briefly introduce another powerful class of models, that will be used in the book. Tree\sphinxhyphen{}based methods are powerful tools for both regression and classification tasks. They involve segmenting the predictor space into a number of simple regions, and making predictions based on these regions. Decision trees are a type of tree\sphinxhyphen{}based method that can handle complex data structures and relationships.


\subsection{Decision trees}
\label{\detokenize{notebooks/review_ML:decision-trees}}
\sphinxAtStartPar
The decision tree is the most simple tree\sphinxhyphen{}based model. It is a non\sphinxhyphen{}parametric model that splits the data into subsets based on the value of the features, creating a tree\sphinxhyphen{}like structure. A decision tree is composed of nodes and branches. Each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a predicted value. The tree grows by splitting nodes into child nodes, based on the values of input features. The goal is to reduce the impurity at each step. The most common algorithm for regression trees is CART (Classification and Regression Trees). The quality of a split is measured using metrics like mean squared error (MSE) or mean absolute error (MAE). The key advantages of decision trees include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Interpretability}: decision trees are easy to interpret and visualize.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}linearity}: they can capture non\sphinxhyphen{}linear relationships between features and the target variable.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Little data preprocessing}: they can handle both numerical and categorical data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feature importance}: trees provide insights into feature importance based on how often and effectively features are used to split the data.

\end{itemize}

\sphinxAtStartPar
Let’s build and visualise a simple decision tree for a regression task using synthetic data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{tree} \PYG{k+kn}{import} \PYG{n}{DecisionTreeRegressor}\PYG{p}{,} \PYG{n}{plot\PYGZus{}tree}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{ravel}\PYG{p}{(}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{0.1}

\PYG{c+c1}{\PYGZsh{} Split data into training and test sets}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit a decision tree regressor}
\PYG{n}{tree\PYGZus{}reg} \PYG{o}{=} \PYG{n}{DecisionTreeRegressor}\PYG{p}{(}\PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{tree\PYGZus{}reg}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Visualize the decision tree}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plot\PYGZus{}tree}\PYG{p}{(}\PYG{n}{tree\PYGZus{}reg}\PYG{p}{,} \PYG{n}{filled}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{feature\PYGZus{}names}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{X}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{rounded}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Decision Tree for Regression}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{c0fceff539d6a89dfb201a33e791710bbb3c0c7ef77c13ff3ab998d268b15a5b}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The decision tree plot shows how the data is split at each node based on the input feature \(X\). Each internal node represents a decision based on the value of \(X\), and each leaf node shows the predicted value of \(y\). The tree provides a clear and interpretable way to understand how the predictions are made based on the input features.


\subsection{Random Forests}
\label{\detokenize{notebooks/review_ML:random-forests}}
\sphinxAtStartPar
Random Forests are an ensemble learning method that combines multiple decision trees to improve the predictive performance and control overfitting. The key idea is to build a “forest” of decision trees, where each tree is trained on a random subset of the data and a random subset of features. The key concepts behind random forests are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bootstrap aggregating (bagging)}: random forests use bagging, where multiple subsets of the data are sampled with replacement to train each tree independently. Mathematically, if the original dataset has \(n\) samples, each tree is trained on a bootstrap sample of \(n\) samples, drawn with replacement.
\begin{equation*}
\begin{split}
   \text{Bootstrap Sample} = \{ x_i \}_{i=1}^{n} \quad \text{where} \quad x_i \sim \mathcal{D}
   \end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Feature randomness}: at each split in the decision trees, a random subset of features is considered, introducing further randomness and reducing correlation between the trees. If there are \(p\) features, a typical number of features considered at each split is \(m = \sqrt{p}\) for classification or \(m = \frac{p}{3}\) for regression.
\begin{equation*}
\begin{split}
   \text{Random Subset of Features} = \{ X_j \}_{j=1}^{m} \quad \text{where} \quad m \leq p
   \end{split}
\end{equation*}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Voting mechanism}: for regression tasks, the final prediction is the average of the predictions from all individual trees, while for classification tasks, the majority vote is taken.
\begin{equation*}
\begin{split}
   \text{Regression Prediction} = \frac{1}{T} \sum_{t=1}^{T} \hat{y}_t(x)
   \end{split}
\end{equation*}\begin{equation*}
\begin{split}
   \text{Classification Prediction} = \text{mode}\left(\{ \hat{y}_t(x) \}_{t=1}^{T} \right)
   \end{split}
\end{equation*}
\end{enumerate}

\sphinxAtStartPar
The ensemble approach employed by random forests reduces the variance and the risk of overfitting compared to individual decision trees. Indeed, aggregating multiple trees often leads to better predictive performance. Moreover, random forests provide insights into feature importance by evaluating the average decrease in impurity over all trees.

\sphinxAtStartPar
For classification, a common impurity measures are Gini impurity and entropy, which is defined for a node \(t\) as:
\begin{equation*}
\begin{split}
  G(t) = 1 - \sum_{i=1}^{C} p_i^2
  \end{split}
\end{equation*}
\sphinxAtStartPar
where \(p_i\) is the probability of class \(i\) at node \(t\), and \(C\) is the number of classes.

\sphinxAtStartPar
For regression, the impurity measure is typically the \sphinxstylestrong{variance}, which is defined for a node \(t\) as:
\begin{equation*}
\begin{split}
  \sigma^2(t) = \frac{1}{N_t} \sum_{i=1}^{N_t} (y_i - \bar{y})^2
  \end{split}
\end{equation*}
\sphinxAtStartPar
where \(N_t\) is the number of samples in node \(t\), \(y_i\) is the target value for the \(i\)\sphinxhyphen{}th sample, and \(\bar{y}\) is the mean target value in node \(t\).

\sphinxAtStartPar
\sphinxstylestrong{Feature importance} in random forests is calculated based on the mean decrease in impurity. For a feature \(X_j\):
\begin{equation*}
\begin{split}
\text{Importance}(X_j) = \frac{1}{T} \sum_{t=1}^{T} \sum_{k \in \text{nodes}} \Delta I_k(X_j)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(\Delta I_k(X_j)\) is the decrease in impurity at node \(k\) due to split on feature \(X_j\), and \(T\) is the total number of trees in the forest.

\sphinxAtStartPar
The bar plot below shows the importance scores of each feature used in a random forest model fitted to synthetic data. Each bar represents a feature, and its height indicates the relative importance of that feature in predicting the target variable. Higher bars correspond to features that are more important for the model. These features contribute more significantly to reducing the impurity of the splits in the decision trees. Understanding feature importance can provide insights into the underlying data structure and guide feature selection or engineering efforts.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{3.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{0.1}

\PYG{c+c1}{\PYGZsh{} Split data into training and test sets}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit a random forest regressor}
\PYG{n}{forest\PYGZus{}reg} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{forest\PYGZus{}reg}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Extract and plot feature importance}
\PYG{n}{importances} \PYG{o}{=} \PYG{n}{forest\PYGZus{}reg}\PYG{o}{.}\PYG{n}{feature\PYGZus{}importances\PYGZus{}}
\PYG{n}{features} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{features}\PYG{p}{,} \PYG{n}{importances}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{align}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Feature Index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Importance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Forest Feature Importance}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{d055733738b726a3ebc6878ce721d058d40b2378967b40614bfdea36ddc5aba5}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{XGBoost}
\label{\detokenize{notebooks/review_ML:xgboost}}
\sphinxAtStartPar
XGBoost (Extreme Gradient Boosting) is an advanced implementation of gradient boosting, designed for speed and performance. It builds decision trees sequentially, where each tree attempts to correct the errors of the previous trees. The key concepts of XGBoost are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Gradient boosting}: the method sequentially builds trees by fitting the residual errors of the previous trees. Each tree tries to minimize a loss function by using gradient descent. The objective is to minimize the following loss function:
\begin{equation*}
\begin{split}
   \mathcal{L} = \sum_{i=1}^{n} l(y_i, \hat{y}_i^{(t)}) + \sum_{k=1}^{t} \Omega(f_k)
   \end{split}
\end{equation*}
\sphinxAtStartPar
where \(l\) is a differentiable loss function (e.g., mean squared error for regression, log loss for classification), \(\hat{y}_i^{(t)}\) is the prediction at the \(t\)\sphinxhyphen{}th iteration, and \(\Omega\) is a regularization term for the complexity of the trees \(f_k\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regularization}: XGBoost incorporates regularization to prevent overfitting, making it more robust than traditional gradient boosting methods. The regularization term \(\Omega\) is given by:
\begin{equation*}
\begin{split}
   \Omega(f) = \gamma T + \frac{1}{2} \lambda \sum_{j=1}^{T} w_j^2
   \end{split}
\end{equation*}
\sphinxAtStartPar
where \(T\) is the number of leaves in the tree, \(w_j\) are the leaf weights, \(\gamma\) is a regularization parameter for the number of leaves, and \(\lambda\) is a regularization parameter for the leaf weights.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parallel processing}: XGBoost supports parallel processing and efficient handling of missing values, which enhances its performance. This is achieved by using a block structure for data storage and computation, allowing multiple operations to be carried out simultaneously.

\end{enumerate}

\sphinxAtStartPar
XGBoost is known for its high predictive accuracy and efficiency. It can handle a variety of data types and is widely used in many industrial contexts. Moreover, it has built\sphinxhyphen{}in regularization parameters to control overfitting and improve generalization. Indeed, the objective function in XGBoost combines the loss function and the regularization term. For the \(t\)\sphinxhyphen{}th iteration, the objective function can be approximated using a second\sphinxhyphen{}order Taylor expansion:
\begin{equation*}
\begin{split}
\mathcal{L}^{(t)} \approx \sum_{i=1}^{n} [l(y_i, \hat{y}_i^{(t-1)}) + g_i f_t(x_i) + \frac{1}{2} h_i f_t^2(x_i)] + \Omega(f_t)
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(g_i = \frac{\partial l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)}}\) and \(h_i = \frac{\partial^2 l(y_i, \hat{y}_i^{(t-1)})}{\partial \hat{y}_i^{(t-1)2}}\) are the first and second derivatives of the loss function, respectively.

\sphinxAtStartPar
The structure score of a tree is given by:
\begin{equation*}
\begin{split}
\mathcal{L}_{split} = -\frac{1}{2} \left( \frac{\sum_{i \in I_L} g_i}{\sum_{i \in I_L} h_i + \lambda} + \frac{\sum_{i \in I_R} g_i}{\sum_{i \in I_R} h_i + \lambda} - \frac{\sum_{i \in I} g_i}{\sum_{i \in I} h_i + \lambda} \right) + \gamma
\end{split}
\end{equation*}
\sphinxAtStartPar
where \(I_L\) and \(I_R\) are the instances in the left and right child nodes, respectively, and \(I\) is the set of all instances in the node before the split.

\sphinxAtStartPar
In the plot below, we show the training process, where the lines display the root mean squared error (RMSE) on both the training and test sets across the boosting rounds. The x\sphinxhyphen{}axis represents the number of boosting rounds (trees added), and the y\sphinxhyphen{}axis represents the RMSE.

\sphinxAtStartPar
The plot helps in understanding how the model’s performance evolves during training. Ideally, both training and test RMSE should decrease initially, indicating improved model performance. If the training RMSE continues to decrease while the test RMSE starts to increase, it suggests overfitting. The evaluation plot is a crucial diagnostic tool for tuning the parameters (e.g., learning rate, max depth) to achieve a balance between underfitting and overfitting.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{train\PYGZus{}test\PYGZus{}split}
\PYG{k+kn}{import} \PYG{n+nn}{xgboost} \PYG{k}{as} \PYG{n+nn}{xgb}

\PYG{c+c1}{\PYGZsh{} Generate synthetic data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{3.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{100}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{0.1}

\PYG{c+c1}{\PYGZsh{} Split data into training and test sets}
\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{y\PYGZus{}train}\PYG{p}{,} \PYG{n}{y\PYGZus{}test} \PYG{o}{=} \PYG{n}{train\PYGZus{}test\PYGZus{}split}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{,} \PYG{n}{test\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Convert the data into DMatrix format for XGBoost}
\PYG{n}{train\PYGZus{}dmatrix} \PYG{o}{=} \PYG{n}{xgb}\PYG{o}{.}\PYG{n}{DMatrix}\PYG{p}{(}\PYG{n}{X\PYGZus{}train}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{y\PYGZus{}train}\PYG{p}{)}
\PYG{n}{test\PYGZus{}dmatrix} \PYG{o}{=} \PYG{n}{xgb}\PYG{o}{.}\PYG{n}{DMatrix}\PYG{p}{(}\PYG{n}{X\PYGZus{}test}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{y\PYGZus{}test}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the model parameters}
\PYG{n}{params} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{objective}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{reg:squarederror}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{max\PYGZus{}depth}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mi}{3}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eta}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+m+mf}{0.1}\PYG{p}{,}
    \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{eval\PYGZus{}metric}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{:} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{rmse}\PYG{l+s+s2}{\PYGZdq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Train the XGBoost model with evaluation sets}
\PYG{n}{evals} \PYG{o}{=} \PYG{p}{[}\PYG{p}{(}\PYG{n}{train\PYGZus{}dmatrix}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{n}{test\PYGZus{}dmatrix}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]}
\PYG{n}{num\PYGZus{}rounds} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{evals\PYGZus{}result} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}
\PYG{n}{xg\PYGZus{}reg} \PYG{o}{=} \PYG{n}{xgb}\PYG{o}{.}\PYG{n}{train}\PYG{p}{(}\PYG{n}{params}\PYG{p}{,} \PYG{n}{train\PYGZus{}dmatrix}\PYG{p}{,} \PYG{n}{num\PYGZus{}boost\PYGZus{}round}\PYG{o}{=}\PYG{n}{num\PYGZus{}rounds}\PYG{p}{,} \PYG{n}{evals}\PYG{o}{=}\PYG{n}{evals}\PYG{p}{,} \PYG{n}{evals\PYGZus{}result}\PYG{o}{=}\PYG{n}{evals\PYGZus{}result}\PYG{p}{,} \PYG{n}{verbose\PYGZus{}eval}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the evaluation results}
\PYG{n}{epochs} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{evals\PYGZus{}result}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x\PYGZus{}axis} \PYG{o}{=} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{epochs}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}axis}\PYG{p}{,} \PYG{n}{evals\PYGZus{}result}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Train}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{x\PYGZus{}axis}\PYG{p}{,} \PYG{n}{evals\PYGZus{}result}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{rmse}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Test}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Boosting Round}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{RMSE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{XGBoost Training and Test RMSE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{33938cfabae38c6df99c0dda23a9389612374fa7db03dba73534daa1017515ae}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\part{Introduction}

\sphinxstepscope


\chapter{Motivation}
\label{\detokenize{notebooks/motivation:motivation}}\label{\detokenize{notebooks/motivation::doc}}
\sphinxAtStartPar
Understanding causality is crucial in many fields, including economics, medicine, social sciences, and engineering. In the context of electricity markets, grasping causal relationships allows us to make informed decisions, optimise operations, and predict future outcomes with greater accuracy. By studying causality, we move beyond mere correlations, gaining the ability to make strategic decisions, design effective policies, and foster a deeper understanding of the intricacies of electricity markets. The potential advantages of grasping and utilising causal relationships in electricity markets include:

\sphinxAtStartPar
➔ \sphinxstylestrong{Enhancing understanding of complex systems}: electricity markets involve interactions between multiple entities, including generators, consumers, and regulators. Causal inference provides a structured approach to unravel these complexities, helping stakeholders understand how different factors interact and influence market outcomes.

\sphinxAtStartPar
➔ \sphinxstylestrong{Making better decisions}: causal inference helps differentiate between correlation and causation, which is essential for making informed and effective policy decisions. Without a proper understanding of causality, decisions can be influenced by biases such as omitted variable bias, where important factors that affect the outcome are not accounted for. By employing causal inference methods, policymakers can identify and correct for these biases, ensuring that their decisions are based on accurate and reliable evidence. This approach leads to more effective interventions and policies, avoiding the pitfalls of misleading correlations and enhancing the overall decision\sphinxhyphen{}making process.

\sphinxAtStartPar
➔ \sphinxstylestrong{Learning from interventions}: in experimental settings, such as randomised controlled trials, causality helps determine the effect of specific interventions. In electricity markets, experiments might involve testing new pricing schemes or demand response programmes, where understanding the causal impact is vital for scaling successful initiatives.

\sphinxAtStartPar
➔ \sphinxstylestrong{Predicting future trends}: causal models can be beneifcial for long\sphinxhyphen{}term forecasting because they help identify and understand the fundamental drivers of market behavior. While traditional forecasting models rely on historical data and correlations, these approaches can fall short when market conditions change or when predicting the impact of new interventions. For example, causal inference might support:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Understanding root causes}: by identifying the true causal relationships between variables, stakeholders can understand the root causes of market trends rather than just observing surface\sphinxhyphen{}level correlations. This allows for more accurate predictions, especially in changing environments.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Policy impact analysis}: causal models enable the evaluation of how policy changes will affect the market over time. For example, understanding how subsidies for renewable energy causally impact electricity prices can help predict future market conditions as these policies evolve.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Adaptation to new conditions}: markets are dynamic, and conditions often change due to technological advancements, regulatory shifts, or economic developments. Causal models help predict how these changes will influence long\sphinxhyphen{}term trends by focusing on the underlying mechanisms rather than past patterns alone.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Scenario planning}: causal inference allows for robust scenario analysis by manipulating key variables to see potential outcomes. This is crucial for strategic planning and risk management, providing insights into how different factors will interact over the long term.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Avoiding spurious predictions}: correlational models may lead to spurious predictions when unseen confounding variables are at play. Causal models help control for these confounders, ensuring that predictions are based on genuine causal effects rather than coincidental correlations.

\end{itemize}

\sphinxstepscope


\chapter{What to expect from each chapter}
\label{\detokenize{notebooks/guide:what-to-expect-from-each-chapter}}\label{\detokenize{notebooks/guide::doc}}
\sphinxAtStartPar
The content is currently divided into five main parts. Each part aims to provide a set of alternatives to tackle \sphinxstylestrong{one key objective}. This is your map if you are not sure what you need.


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Part
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Name
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Key Objective
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
I
&
\sphinxAtStartPar
Basic Concepts
&
\sphinxAtStartPar
\sphinxstylestrong{Getting an overview of causal inference}
&
\sphinxAtStartPar
Introduction and basic concepts to understand the foundation of causal inference.
\\
\sphinxhline
\sphinxAtStartPar
II
&
\sphinxAtStartPar
Causal Discovery
&
\sphinxAtStartPar
\sphinxstylestrong{Discovering causal relationships from data}
&
\sphinxAtStartPar
Techniques and methods for identifying causal structures and relationships.
\\
\sphinxhline
\sphinxAtStartPar
III
&
\sphinxAtStartPar
Causal Inference
&
\sphinxAtStartPar
\sphinxstylestrong{Estimating causal effects accurately}
&
\sphinxAtStartPar
Methods for quantifying the strength and nature of causal relationships.
\\
\sphinxhline
\sphinxAtStartPar
IV
&
\sphinxAtStartPar
Interpretability
&
\sphinxAtStartPar
\sphinxstylestrong{Interpreting machine learning models}
&
\sphinxAtStartPar
Techniques to understand and explain model predictions and decisions.
\\
\sphinxhline
\sphinxAtStartPar
V
&
\sphinxAtStartPar
Experiments and Data Collection
&
\sphinxAtStartPar
\sphinxstylestrong{Designing effective experiments}
&
\sphinxAtStartPar
Approaches for planning, conducting, and analysing controlled experiments.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}


\section{Detailed overviews}
\label{\detokenize{notebooks/guide:detailed-overviews}}
\sphinxAtStartPar
For parts II to V, we have included an overview chapter at the beginning of each section. These overview chapters are designed to provide a thorough introduction and a quick summary of the primary content, making it easier for you to understand the overall scope and focus of each part.

\sphinxstepscope


\part{I. Basic Concepts}

\sphinxstepscope


\chapter{Correlation vs. Causation}
\label{\detokenize{notebooks/correlation_vs_causation:correlation-vs-causation}}\label{\detokenize{notebooks/correlation_vs_causation::doc}}
\sphinxAtStartPar
When addressing real\sphinxhyphen{}world engineering or business challenges, a fundamental goal is to comprehend the underlying mechanisms governing a system and the \sphinxstylestrong{key drivers} affecting its performance. This understanding often stems from conducting experiments, which are foundational to scientific research. For example, if we aim to determine the effect of a new solar panel design on energy conversion efficiency, systematic experimentation might be the initial approach. Through controlled manipulation and observation, \sphinxstylestrong{experimental data} helps us establish clear cause\sphinxhyphen{}and\sphinxhyphen{}effect relationships.
\begin{quote}

\sphinxAtStartPar
In the context of data analysis and research, a \sphinxstylestrong{cause} refers to an event or a variable that directly influences another event or variable. If a change in \(X\) (the cause) results in a change in \(Y\) (the effect), then \(X\) is said to cause \(Y\). This relationship is often depicted as \(X \rightarrow Y\), signifying that \(X\) is a causal factor for \(Y\).
\end{quote}

\sphinxAtStartPar
In many real\sphinxhyphen{}world settings, such as economics or public policy, performing experiments can be impractical, expensive, or unethical. Consequently, researchers and analysts often rely on \sphinxstylestrong{observational data}, which is collected without any direct manipulation of variables. Working with observational data introduces complexities, particularly in distinguishing between mere correlations and actual causal relationships. Observational data can reveal patterns and associations, but without the ability to control for all influencing factors, these correlations might lead to misleading conclusions about causality.

\sphinxAtStartPar
\sphinxstylestrong{Causal inference} bridges this gap. It is a branch of statistics focused on determining cause\sphinxhyphen{}and\sphinxhyphen{}effect relationships from data where no explicit experimentation has been conducted. Unlike mere correlation analysis, which only measures how variables move together, causal inference aims to uncover whether and how one variable influences another. This distinction is vital for informed decision\sphinxhyphen{}making across various domains, including economics, healthcare, and energy markets. The phrase “correlation does not imply causation” is frequently cited in statistical analysis to caution against prematurely concluding that one variable causes another simply because they are associated. Despite this, it is surprising how often business decisions are made based on perceived associations highlighted by correlation metrics alone.

\sphinxAtStartPar
In the electricity market, understanding causal relationships is crucial. It enables stakeholders to predict the effects of policy changes, modify pricing strategies, and assess the impact of technological advancements on demand and supply dynamics. Causal inference provides the tools to make predictions about the consequences of potential actions, thereby supporting more strategic and effective decision\sphinxhyphen{}making.

\sphinxAtStartPar
\sphinxstylestrong{A Motivating Example}

\sphinxAtStartPar
To illustrate the dichotomy between correlation and causation, consider a simple example involving three variables:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Temperature (°C)

\item {} 
\sphinxAtStartPar
Electricity load (MW)

\item {} 
\sphinxAtStartPar
Ice cream sales (GBP)

\end{enumerate}

\sphinxAtStartPar
To better understand the relationships among these variables, we utilise a \sphinxstylestrong{causal graph}. In this scenario, we assume knowledge of the true causal structure of the data\sphinxhyphen{}generating process: temperature affects both electricity load and ice cream sales.

\sphinxAtStartPar
A causal graph is typically assumed to be a \sphinxstylestrong{directed acyclic graph (DAG)}, which is a graphical representation used to model and reason about the causal relationships between variables. In a DAG:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Nodes} represent variables or events.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Edges} (arrows) indicate causal influences from one variable to another.

\end{itemize}

\sphinxAtStartPar
When we say that the graph is \sphinxstylestrong{acyclic}, it means that it does not contain any loops, ensuring that causality flows in one direction and does not circle back on itself.

\sphinxAtStartPar
In the causal graph below, \sphinxstylestrong{temperature} is depicted as a parent node influencing two child nodes: \sphinxstylestrong{electricity load} and \sphinxstylestrong{ice cream sales}. This structure helps us visualise and understand that fluctuations in temperature are the underlying drivers for changes in electricity consumption and ice cream sales. Typically, we are also interested in understanding the \sphinxstylestrong{strength} of these causal connections. However, for now, let’s not consider the specific impacts of these weights.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{graphviz}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}

\PYG{c+c1}{\PYGZsh{} Create a new graph}
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add nodes}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{I}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice cream}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add edges}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{I}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the graph in the notebook}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x1049d8d90\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Assume we have collected a dataset that follows the causal structure outlined in the previously discussed causal graph. However, let’s also assume that we are unaware of this true causal structure and need to perform exploratory analysis to uncover potential correlations among the observations.

\sphinxAtStartPar
In this scenario, our goal is to explore the data, look for correlations, and attempt to infer possible causal relationships without prior knowledge of the underlying causal mechanisms. This approach mimics real\sphinxhyphen{}world situations where data scientists and researchers often work with complex datasets without a clear understanding of the dynamics that govern the relationships between variables.

\sphinxAtStartPar
Through statistical methods and visual analysis, we will examine the interactions between temperature, electricity load, and ice cream sales, exploring how these variables may be related and considering the implications of our findings.

\sphinxAtStartPar
First, let’s generate some data according to the DAG shown above.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Setting a random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generating synthetic data}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100000}
\PYG{n}{temperatures} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} average temperature in Celsius}
\PYG{n}{electricity\PYGZus{}load} \PYG{o}{=} \PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperatures} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{70} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Quadratic relationship for U\PYGZhy{}shape}
\PYG{n}{ice\PYGZus{}cream\PYGZus{}sales} \PYG{o}{=} \PYG{l+m+mi}{5}\PYG{o}{*} \PYG{n}{temperatures} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{20}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} also influenced by temperature}

\PYG{c+c1}{\PYGZsh{} Creating a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperatures}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{electricity\PYGZus{}load}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice Cream Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{ice\PYGZus{}cream\PYGZus{}sales}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Temperature  Electricity Load  Ice Cream Sales
0    22.483571         76.386598       143.654668
1    19.308678         64.318811        94.658826
2    23.238443         74.974688        89.601501
3    27.615149         78.501907       110.302984
4    18.829233         68.637125        87.293150
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, let’s plot the pairwise correlation plots to explore the dependencies.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature vs Electricity Load with hexbin plot}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature vs Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature vs Ice Cream Sales with hexbin plot}
\PYG{n}{hb2} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice Cream Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature vs Ice Cream Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice Cream Sales (GBP)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Electricity Load vs Ice Cream Sales with hexbin plot}
\PYG{n}{hb3} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice Cream Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load vs Ice Cream Sales}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Ice Cream Sales (GBP)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{8e1a86021a1604d04066d84f4b3d5fefd2264bbe00bafcdc25bee828a7d975d7}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
By examining these plots, we might find ourselves tempted to conclude a correlation between electricity load and ice cream sales, especially in a more complex scenario where the causal structure is not known to us. This observation highlights a common challenge in data analysis: distinguishing between mere correlation and actual causation without prior knowledge of the underlying causal mechanisms.

\sphinxstepscope


\chapter{Causal Representations}
\label{\detokenize{notebooks/DAG:causal-representations}}\label{\detokenize{notebooks/DAG::doc}}
\sphinxAtStartPar
Directed acyclic graphs (DAGs) are essential tools for understanding causal relationships between variables. They are widely used in statistics, machine learning, and causal inference.

\sphinxAtStartPar
The \sphinxstylestrong{key elements} of a DAG are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Node}: represents a variable or an event in the graph.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Edge}: a directed arrow indicating a causal influence from one node (variable) to another.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Parent node}: a node that has outgoing edges to one or more child nodes.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Child node}: a node that has incoming edges from one or more parent nodes.

\end{enumerate}

\sphinxAtStartPar
The \sphinxstylestrong{key properties} of a DAG are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Directed}: each edge in the graph has a direction, from one vertex to another.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Acyclic}: there are no cycles; you cannot start at one vertex, follow a sequence of directed edges, and return to the starting vertex.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Topological ordering}: the nodes of a DAG can be ordered in such a way that for every directed edge u → v, node u comes before node v in the ordering.

\end{enumerate}

\sphinxAtStartPar
A DAG is a graph that is directed and has no cycles, meaning there is no way to start at one node and follow a consistent direction that leads back to the starting node.
\begin{quote}

\sphinxAtStartPar
In a DAG, every parent is a direct cause of all its children.
\end{quote}

\sphinxAtStartPar
To better understand DAGs, let’s consider a simple example with the following variables:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Temperature}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Electricity load}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Renewable energy production}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Electricity price}

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{graphviz}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}

\PYG{c+c1}{\PYGZsh{} Create a new graph}
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add nodes}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{R}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Renewable}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{energy production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add edges}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{T}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{R}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{R}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the graph in the notebook}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x105b03190\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Understanding the DAG}

\sphinxAtStartPar
A DAG is used to visually represent the causal relationships between the key variables in the analysis. The key variables are represented as nodes, and the arrows represent the direction of influence. By examining the graph, we can understand how changes in one variable may impact others, forming a network of dependencies that govern market dynamics. In the example above, we have:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Temperature → electricity load}: as temperature increases or decreases, it directly affects the electricity load. With warmer temperatures, the demand for electricity increases due to air conditioning usage. Similarly, colder temperatures can increase electricity demand for heating purposes.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Temperature → renewable energy production}: temperature can affect the efficiency and output of renewable energy sources. For example, solar panels may produce more energy on sunny, warm days but less on extremely hot days when efficiency drops. Wind patterns, affected by temperature changes, can also impact wind energy production.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Renewable energy production → electricity price}: the amount of renewable energy produced affects electricity prices. When renewable energy production is high, it can lead to lower electricity prices due to the abundance of cheaper energy sources. Conversely, low renewable energy production can increase reliance on more expensive, non\sphinxhyphen{}renewable energy sources, driving up prices.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Electricity load → electricity price}: the demand for electricity (load) influences electricity prices. High electricity demand usually leads to higher prices because the supply must meet the increased load, often requiring more expensive or less efficient energy sources. Lower demand can result in lower prices due to decreased strain on the electricity grid and reliance on cheaper energy sources.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Additional Concepts in DAGs}

\sphinxAtStartPar
\sphinxstylestrong{d\sphinxhyphen{}separation}

\sphinxAtStartPar
In addition to understanding direct causal relationships, it is also important to understand how to determine if variables are conditionally independent given other variables. This concept is known as \sphinxstylestrong{d\sphinxhyphen{}separation}, and it represents a criterion for deciding whether a set of nodes is independent of another set of nodes given a third set. It is a crucial concept in understanding the flow of information and causation in DAGs.

\sphinxAtStartPar
For example, in the context of our electricity market DAG:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Temperature} is d\sphinxhyphen{}separated from \sphinxstylestrong{electricity price} given \sphinxstylestrong{electricity load}. This means that if we know the electricity load, knowing the temperature provides no additional information about the electricity price.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Renewable energy production} and \sphinxstylestrong{electricity load} are not d\sphinxhyphen{}separated, indicating a direct causal relationship without any conditional independence given other variables.

\end{itemize}

\sphinxAtStartPar
By understanding d\sphinxhyphen{}separation, we can better interpret the dependencies and independencies in our DAG, leading to more accurate causal inferences.

\sphinxAtStartPar
\sphinxstylestrong{Local Markov assumption}

\sphinxAtStartPar
The Local Markov ssumption states that a node in a DAG is conditionally independent of its non\sphinxhyphen{}descendants given its parents. This assumption allows for the decomposition of the joint probability distribution of all variables in the DAG into simpler conditional distributions. For example, in our DAG, \sphinxstylestrong{electricity price} is conditionally independent of \sphinxstylestrong{temperature} given \sphinxstylestrong{renewable energy production} and \sphinxstylestrong{electricity load}. This implies that once we know the values of \sphinxstylestrong{renewable energy production} and \sphinxstylestrong{electricity load}, additional information about \sphinxstylestrong{temperature} does not change our understanding of \sphinxstylestrong{electricity price}.

\sphinxAtStartPar
\sphinxstylestrong{Factorization}

\sphinxAtStartPar
Factorization refers to the decomposition of a joint probability distribution into a product of conditional distributions. This is possible under the Local Markov Assumption. The joint distribution \( P(T, L, R, P) \) can be factorized as:
\label{equation:notebooks/DAG:afd2b644-8efb-420d-a7f0-0712fb60babb}\begin{equation}
    P(T, L, R, P) = P(T) \cdot P(L|T) \cdot P(R|T) \cdot P(P|R, L)
\end{equation}
\sphinxAtStartPar
This factorization simplifies the computation of the joint probability distribution by breaking it down into manageable parts. It allows us to understand the contribution of each variable to the overall distribution, aiding in both analysis and inference.

\sphinxAtStartPar
\sphinxstylestrong{Minimality assumption}

\sphinxAtStartPar
The Minimality Assumption asserts that the DAG representing the causal structure is minimal, meaning that removing any edge would violate the Markov condition for the observed data. It ensures that the model encodes only necessary dependency relationships. It means that the DAG includes only those edges that represent necessary causal relationships. Removing any edge would lead to a loss of information about the dependencies among variables, ensuring that the DAG is the simplest model that adequately represents the data.

\sphinxAtStartPar
\sphinxstylestrong{Markov equivalence class}

\sphinxAtStartPar
A Markov equivalence class contains all DAGS that encode the same conditional independencies. This means that within a Markov equivalence class, multiple DAGs can represent the same set of conditional independence relationships among variables. These DAGs are considered equivalent because they imply the same probabilistic dependencies, even if their structures differ.

\sphinxAtStartPar
Consider the following three DAGs involving three variables \(X\), \(Y\), and \(Z\):

\sphinxAtStartPar
\sphinxstylestrong{Graph 1:}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x105b59c90\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Graph 2:}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x105b5ab90\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Graph 3:}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x105b5b210\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Each of these DAGs represents the same conditional independence relationships: \(X \perp Z | Y\), indicating that once we know \(Y\), \(X\) and \(Z\) are independent.

\sphinxAtStartPar
A completed partially directed acyclic graph (CPDAG), also known as an Essential Graph (EG), is a graphical representation that captures all DAGs within a Markov equivalence class. The CPDAG contains both directed and undirected edges:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Directed edges} represent causal relationships that are common to all DAGs in the equivalence class.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Undirected edges} represent relationships where the directionality is ambiguous among the DAGs in the equivalence class.

\end{itemize}

\sphinxAtStartPar
The CPDAG can be constructed by:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Identifying all DAGs that belong to the same Markov equivalence class.

\item {} 
\sphinxAtStartPar
Retaining directed edges that are common to all these DAGs.

\item {} 
\sphinxAtStartPar
Converting edges with uncertain directionality into undirected edges.

\end{enumerate}

\sphinxAtStartPar
Let’s consider a CPDAG for the equivalence class of the three DAGs shown above:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n+nb}{dir}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} undirected edge}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x105b5a210\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
In this CPDAG:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The edges \(X \to Y\) and \(Z \to Y\) are directed because all DAGs in the equivalence class have these directions.

\item {} 
\sphinxAtStartPar
The edge \(X - Z\) is undirected, indicating that the direction between \(X\) and \(Z\) is ambiguous within the equivalence class.

\end{itemize}

\sphinxAtStartPar
The CPDAG or EG provides a compact and comprehensive representation of the causal structure, capturing all possible DAGs that explain the observed data:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Identification of causal relationships}: directed edges in the CPDAG indicate robust causal relationships that hold across all DAGs in the equivalence class.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ambiguity in directionality}: undirected edges highlight areas where additional data or assumptions are needed to resolve the direction of causality.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simplified analysis}: analysing a CPDAG instead of multiple individual DAGs simplifies the process of understanding the underlying causal structure.

\end{itemize}

\sphinxstepscope


\chapter{Basic Causal Structures}
\label{\detokenize{notebooks/basic_dag_structures:basic-causal-structures}}\label{\detokenize{notebooks/basic_dag_structures::doc}}
\sphinxAtStartPar
Understanding basic causal structures is crucial in causal inference because they possess \sphinxstylestrong{unique properties} that aid in the discovery of causal graphs from observational data. Additionally, recognizing these structures is essential for correctly specifying \sphinxstylestrong{regression models} in statistical analysis. Misidentifying or incorrectly including variables based on these structures can lead to erroneous conclusions and model estimates.


\section{Chains}
\label{\detokenize{notebooks/basic_dag_structures:chains}}
\sphinxAtStartPar
A chain occurs when one variable causally affects a second, which in turn affects a third. This linear sequence shows the \sphinxstylestrong{transmission of causality} through an intermediary.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}dot}

\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weather }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{fluctuations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wind forecast }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{errors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x169615390\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This causal graph represents the following \sphinxstylestrong{structural causal model (SCM)}:
\label{equation:notebooks/basic_dag_structures:acb49524-1909-4f3c-9842-cf64291aa07e}\begin{align}
\text{weather fluctuations} &= e_w \\
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\text{balancing costs} &= 1.2 \times \text{wind forecast errors} + e_b \\
\end{align}
\sphinxAtStartPar
where the \sphinxstylestrong{coefficients} 1.5 and 1.2 represent the strenghts of the causal connections.

\sphinxAtStartPar
In this case, the \sphinxstylestrong{causal association flows} from the weather forecast to the balancing costs, and is (fully) \sphinxstylestrong{mediated} by the wind forecast errors.


\section{Forks}
\label{\detokenize{notebooks/basic_dag_structures:forks}}
\sphinxAtStartPar
A fork occurs when a single variable causally influences two other variables, making it a common cause to both. This structure typically indicates that the two downstream variables are \sphinxstylestrong{correlated due to a shared source} but do not causally influence each other. This coincides with the exmaple we saw in the previous chapter, where the common cause (temperature) gave the impression that electricity load and ice cream sales were correlated.

\sphinxAtStartPar
Another example, related to the balancing costs in the electricity markets, might be the effect of weather fluctuations on wind forecast errors and on electricity load due to increase heating (or cooling) demand.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weather }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{fluctuations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wind forecast }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{errors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{heating }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{demand}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x1683f81d0\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This causal graph represents the following \sphinxstylestrong{SCM}:
\label{equation:notebooks/basic_dag_structures:e5df486c-39cd-4224-8f9f-ae244fe62967}\begin{align}
\text{weather fluctuations} &= e_w \\
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\text{heating demand costs} &= 0.7 \times \text{heating demand} + e_h \\
\end{align}
\sphinxAtStartPar
where \(e_w, e_f, e_h \sim \mathcal{N}(0,1)\).

\sphinxAtStartPar
First, let’s generate some data from this DAG.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Setting a random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generating synthetic data}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100000}
\PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} average temperature in Celsius}
\PYG{n}{forecast\PYGZus{}errors} \PYG{o}{=} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Quadratic relationship for U\PYGZhy{}shape}
\PYG{n}{heating} \PYG{o}{=} \PYG{l+m+mf}{0.7}\PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} also influenced by temperature}

\PYG{c+c1}{\PYGZsh{} Creating a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weather}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{heating}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
    weather  forecast errors  heating demand
0  0.496714         1.775666        1.909541
1 \PYGZhy{}0.138264        \PYGZhy{}1.362751       \PYGZhy{}0.191013
2  0.647689         1.546970       \PYGZhy{}0.876154
3  1.523030         1.665306       \PYGZhy{}0.322517
4 \PYGZhy{}0.234153        \PYGZhy{}0.678633       \PYGZhy{}0.506558
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, let’s plot the pairwise correlation plots.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature vs Electricity Load with hexbin plot}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weather}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weather fluctuations vs wind forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast error (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature vs Ice Cream Sales with hexbin plot}
\PYG{n}{hb2} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weather}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weather fluctuations vs heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heating demand (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Electricity Load vs Ice Cream Sales with hexbin plot}
\PYG{n}{hb3} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wind forecast errors vs heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast error (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Heating demand (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{0cfa9fdbd9881ebbb6918bd8f3a9063dec3b3dceb6bb63f77a6749edae311ea9}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that, despites \sphinxstylestrong{not being causally related}, wind forecast errors and heating demand appears to be \sphinxstylestrong{positively correlated}. This is because they have a \sphinxstylestrong{common cause}, represented by the weather fluctuations.


\section{Immoralities}
\label{\detokenize{notebooks/basic_dag_structures:immoralities}}
\sphinxAtStartPar
An immorality happens when two variables independently cause a third variable, but there is no causal connection between the two independent variables. Conditioning on the common effect (\sphinxstylestrong{collider}) can introduce a \sphinxstylestrong{spurious association} between these independent variables.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{forecast }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{errors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{outages}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x106b9a110\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This causal graph represents the following \sphinxstylestrong{SCM}:
\label{equation:notebooks/basic_dag_structures:613d3df7-8730-4fd8-ae78-37ef04626241}\begin{align}
\text{forecast errors} &= e_f \\
\text{outages} &= e_o \\
\text{balancing costs} &= 2 \times \text{forecast errors} + 1.5 \times \text{outages} + e_b \\
\end{align}
\sphinxAtStartPar
where \(e_f, e_o, e_b \sim \mathcal{N}(0,1)\).

\sphinxAtStartPar
We will now see the effect of the \sphinxstylestrong{Berkson’s paradox}, which is the spurious correlation observed in the two parent variables, after conditioning on the collider.

\sphinxAtStartPar
First, let’s generate some data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Set the random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate data for the Immorality example}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{100000}
\PYG{n}{forecast\PYGZus{}errors} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\PYG{n}{outages} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\PYG{n}{balancing\PYGZus{}costs} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{forecast\PYGZus{}errors} \PYG{o}{+} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{outages} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} the collider}

\PYG{n}{data\PYGZus{}immorality} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{outages}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{balancing costs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{balancing\PYGZus{}costs}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data\PYGZus{}immorality}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   forecast errors   outages  balancing costs
0         0.496714  1.030595         2.695504
1        \PYGZhy{}0.138264 \PYGZhy{}1.155355        \PYGZhy{}2.018984
2         0.647689  0.575437         2.025579
3         1.523030 \PYGZhy{}0.619238         1.978338
4        \PYGZhy{}0.234153 \PYGZhy{}0.327403        \PYGZhy{}0.993676
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, let’s take a look at the correlations, after conditioning on the child variable.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} All the data}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data\PYGZus{}immorality}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data\PYGZus{}immorality}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} cb1 = fig.colorbar(hb1, ax=axes[0])}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors vs outages }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(not conditioned)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} After conditioning}
\PYG{n}{data\PYGZus{}immorality\PYGZus{}conditioned} \PYG{o}{=} \PYG{n}{data\PYGZus{}immorality}\PYG{p}{[}\PYG{n}{data\PYGZus{}immorality}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{balancing costs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{5}\PYG{p}{]}
\PYG{n}{hb2} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data\PYGZus{}immorality\PYGZus{}conditioned}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data\PYGZus{}immorality\PYGZus{}conditioned}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} cb2 = fig.colorbar(hb2, ax=axes[1])}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors vs outages }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(conditioning on balancing costs \PYGZgt{} 5)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{74d9cd4ed793ddefb29f8404c74bc3f4ee94af7de6fd98c22a6b5e9088699b87}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can observe how conditioning on a collider introduces spurious correlations in the data. Outages and forecast errors are \sphinxstylestrong{not causally related} but, when conditioning on the balancing costs, they \sphinxstylestrong{appear to be correlated}.

\sphinxAtStartPar
This underscores the importance of being cautious when attempting to draw causal conclusions from observational data.

\sphinxAtStartPar
\sphinxstylestrong{In practice}, this means that merely including the collider as a predictor in a \sphinxstylestrong{regression model} can lead to misleading results, where the estimated coefficients deviate significantly from the true causal effects. This potential misstep highlights the necessity of carefully selecting variables for inclusion in regression models, especially when the goal is to uncover causal relationships rather than mere associations.

\sphinxstepscope


\chapter{Definitions and Terminology}
\label{\detokenize{notebooks/glossary:definitions-and-terminology}}\label{\detokenize{notebooks/glossary::doc}}
\sphinxAtStartPar
Understanding the terminology and concepts in causal inference is crucial for grasping the methodologies and their applications. This chapter is designed to be a vademecum that you can consult whenever you have a doubt. Whether you are just starting your causal inference course or revisiting the concepts after a few months, this glossary will provide you with a refresher on the terms you need to remember.


\section{Causal structures and graphical representations}
\label{\detokenize{notebooks/glossary:causal-structures-and-graphical-representations}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Causal graph}: a visual representation of causal relationships among a set of variables, often depicted as a directed acyclic graph (DAG).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Collider}: a variable that is influenced by two or more other variables in a causal graph, potentially introducing bias when conditioned upon.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Confounder}: a variable that influences both the treatment and the outcome, potentially biasing the estimated effect of the treatment.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Directed acyclic graph (DAG)}: a graphical representation of causal relationships between variables. Each node represents a variable, and each directed edge represents a causal effect from one variable to another. The graph is acyclic, meaning it does not contain any cycles.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{D\sphinxhyphen{}separation}: a criterion used to determine conditional independence between two sets of nodes in a directed acyclic graph (DAG), given a third set of nodes. It helps identify whether paths between nodes are blocked by a conditioning set.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Instrumental variable (IV)}: a variable that is used to estimate causal relationships when controlled experiments are not feasible. The IV affects the treatment but has no direct effect on the outcome except through the treatment.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Local Markov assumption}: states that a node in a DAG is conditionally independent of its non\sphinxhyphen{}descendants given its parents. This assumption allows for the decomposition of the joint probability distribution of all variables in the DAG into simpler conditional distributions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Mediator}: a variable that lies on the causal path between the treatment and the outcome, helping to explain the mechanism through which the treatment affects the outcome.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Minimality assumption}: Asserts that the DAG representing the causal structure is minimal, meaning that removing any edge would violate the Markov condition for the observed data. It ensures that the model encodes only necessary dependency relationships.

\end{itemize}


\section{Experimental design and analysis}
\label{\detokenize{notebooks/glossary:experimental-design-and-analysis}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{A/B testing}: a randomised controlled experiment used to compare two versions of a variable to determine which performs better, commonly used in marketing and product development.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Control group and treatment group}: in experimental design, the control group is the group that does not receive the treatment or intervention, while the treatment group is the group that receives the treatment or intervention. Comparing the outcomes of these two groups helps to estimate the causal effect of the treatment.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Counterfactuals}: this framework allows for the analysis of counterfactual questions—what would happen to one variable if another were changed, holding everything else constant.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Design of experiments (DoE)}: a systematic approach to planning, conducting, analysing, and interpreting controlled experiments to ensure valid, reliable, and replicable results.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Ignorability/exchangeability assumption}: sssumes that the way individuals are assigned to treatments can be ignored, allowing for the assumption of random assignment in observational studies. This is crucial for identifying causal effects.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Placebo effect}: a phenomenon where subjects experience a perceived or actual improvement in their condition despite receiving a non\sphinxhyphen{}active treatment, due to their expectations of the treatment’s efficacy.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Randomisation}: the process of assigning subjects to treatment and control groups by chance, reducing bias and ensuring that the groups are comparable.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regression discontinuity design (RDD)}: a quasi\sphinxhyphen{}experimental design that exploits a cutoff or threshold to assign treatments in order to estimate causal effects.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Selection bias}: a bias that occurs when the subjects included in a study are not representative of the population intended to be analysed, leading to incorrect conclusions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Synthetic control method}: a method for estimating causal effects by comparing treated units to a weighted combination of untreated units that best approximates the characteristics of the treated units.

\end{itemize}


\section{Effects and estimates}
\label{\detokenize{notebooks/glossary:effects-and-estimates}}\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Average treatment effect (ATE)}: the difference in the average outcomes between the treatment group and the control group. It measures the overall impact of the treatment on the population.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Causal effect}: the change in an outcome directly attributable to a change in a treatment or exposure.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conditional average treatment effect (CATE)}: the average treatment effect for a specific subset of the population, defined by certain characteristics or conditions. It provides a more granular understanding of how the treatment effect varies across different groups within the population.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Confounding bias}: a bias that occurs when the treatment effect is mixed with the effect of a confounder, leading to an incorrect estimation of the causal effect.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Endogeneity}: a situation in which an explanatory variable is correlated with the error term in a regression model, leading to biased and inconsistent parameter estimates.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exogeneity}: a condition where an explanatory variable is not correlated with the error term, ensuring unbiased and consistent parameter estimates.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{External validity}: the extent to which the results of a study can be generalised to other settings, populations, or time periods.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Heterogeneity}: the variation in treatment effects across different subgroups or individuals within a population.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hidden common causes}: variables that are not observed but influence multiple observed variables, leading to dependent error terms. Addressing hidden common causes is crucial for accurate causal inference.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Internal validity}: the extent to which the observed effects in a study are due to the treatment and not to other factors.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Propensity score}: the probability of a unit being assigned to the treatment group given a set of observed covariates. Used to reduce bias in the estimation of treatment effects in observational studies.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Robustness check}: methods used to test the stability of the estimated effects under different model specifications or assumptions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sensitivity analysis}: a technique used to determine how the results of a study or model change when the assumptions or parameters are varied.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Spillover effect}: an effect where the treatment of one group or individual influences the outcomes of another group or individual, potentially biasing the estimated treatment effect.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{SUTVA}: the stable unit treatment value assumption (SUTVA) is a fundamental assumption in causal inference and experimental design. It asserts that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The treatment assigned to one unit does not affect the outcomes of other units.

\item {} 
\sphinxAtStartPar
Each unit’s outcome depends only on its own treatment, not on the treatments assigned to other units.

\item {} 
\sphinxAtStartPar
There are no different versions of the treatment.

\item {} 
\sphinxAtStartPar
The treatment is applied uniformly across all treated units.

\end{itemize}

\end{itemize}

\sphinxstepscope


\part{II. Causal Discovery}

\sphinxstepscope


\chapter{Overview}
\label{\detokenize{notebooks/preface_causal_discovery:overview}}\label{\detokenize{notebooks/preface_causal_discovery::doc}}
\sphinxAtStartPar
Causal discovery involves identifying causal relationships from data. This process uncovers the underlying causal structure without assuming prior knowledge of the direction or nature of causality. It typically employs statistical and computational methods to determine which variables influence others, providing a foundation for further causal analysis.

\sphinxAtStartPar
The key objective of causal discovery is often to retrieve and visualize a \sphinxstylestrong{causal graph from observational data}. In general, the causal graph for a specific problem can be retrieved relying on:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Domain knowledge}: in some cases, there are well\sphinxhyphen{}known principles and mechanisms governing the processes under investigation. We might be able to draw causal graphs relying on the expertise of practitioners or on the findings reported in previous studies.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Data\sphinxhyphen{}driven methods}: when we cannot assume prior knowledge about the process, we can obtain a causal graph using causal discovery algorithms such as the ones discussed in this chapter.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hybrid approaches}: in many cases, the two aforementioned approaches are complementary. For example, we could:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Start with domain knowledge}: if experts can only outline an incomplete causal graph, we can include this knowledge in the causal discovery algorithm to discover the missing relationships.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Start with the data}: we might derive a tentative causal graph from a causal discovery algorithm and then iteratively validate and refine it through consultation with engineers or practitioners.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
With regard to data\sphinxhyphen{}driven methods, there are two main approaches:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Independence\sphinxhyphen{}based causal discovery}: this class of methods involves analyzing basic DAG structures like \sphinxstylestrong{chains} and \sphinxstylestrong{forks} to interpret how variables influence each other. This approach relies on statistical tests to identify conditional independencies among variables, which can then be used to infer the structure of the causal graph. Methods such as the PC algorithm {[}\hyperlink{cite.bibliography:id13}{SGS01}{]}, the Fast Causal Inference (FCI) algorithm {[}\hyperlink{cite.bibliography:id14}{Spi01}{]}, or the PCMCI algorithm {[}\hyperlink{cite.bibliography:id15}{RNK+19}{]} fall into this category. One of the main drawbacks of these methods is that, in many cases, they can only identify the \sphinxstylestrong{Markov equivalence class} of the graph. This means that while we can identify sets of relationships that are consistent with the observed independencies, we cannot definitively determine the direction of causality between all the variables. For instance, if we have only two variables, which are independent, this method cannot specify whether one causes the other or if they are causally unrelated.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Semi\sphinxhyphen{}parametric causal discovery}: this approach involves making some assumptions about the functional form of the relationships between variables but remains flexible by not fully specifying a parametric model. Methods like the Linear Non\sphinxhyphen{}Gaussian Acyclic Model (LiNGAM) {[}\hyperlink{cite.bibliography:id4}{SHHyvarinen+06}{]} are examples of this approach. Going beyond the Markov equivalence class comes at the expenses of making more specific \sphinxstylestrong{assumptions about the functional forms} of relationships between variables. These assumptions allow for a more detailed discovery of the causal graph, often identifying specific causal directions rather than just equivalence classes. By assuming certain \sphinxstylestrong{non\sphinxhyphen{}linearities} or \sphinxstylestrong{specific distributions of errors}, semi\sphinxhyphen{}parametric methods can exploit asymmetries in the data that reveal the direction of causal effects. This approach is more powerful in that it can often discern the actual causal structure rather than just a set of possible structures. However, the trade\sphinxhyphen{}off is that these methods require stronger assumptions about the nature of the data and the relationships involved, which may not always hold true.

\end{itemize}

\sphinxAtStartPar
In the upcoming Chapters, we focus primarily on semi\sphinxhyphen{}parametric approaches, inspired by the methodologies presented in \sphinxhref{https://link.springer.com/book/10.1007/978-4-431-55784-5}{Statistical Causal Discovery: LiNGAM Approach}. Most of the examples hereby presented have been implemented using the LiNGAM Python package {[}\hyperlink{cite.bibliography:id3}{IIZ+23}{]}.


\section{Content of Causal Discovery chapters}
\label{\detokenize{notebooks/preface_causal_discovery:content-of-causal-discovery-chapters}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Chapter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Linear Models
&
\sphinxAtStartPar
How to retrieve the causal graph from the data, if we can assume a linear model with non\sphinxhyphen{}Gaussian noise for the data\sphinxhyphen{}generating process.
\\
\sphinxhline
\sphinxAtStartPar
Nonlinear Models
&
\sphinxAtStartPar
Extends the LiNGAM approach by considering nonlinear functional forms, and accomodating for Gaussian noise.
\\
\sphinxhline
\sphinxAtStartPar
Time Series Models
&
\sphinxAtStartPar
Extends the LiNGAM approach by providing a method for detecting contemporaneous and lagged effects in time series data.
\\
\sphinxhline
\sphinxAtStartPar
Structural Breaks
&
\sphinxAtStartPar
Highilight the challenges of trying to identifying causal graphs in dynamic and evolving environments.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{Linear Models}
\label{\detokenize{notebooks/semiparametric_direct_lingam:linear-models}}\label{\detokenize{notebooks/semiparametric_direct_lingam::doc}}
\sphinxAtStartPar
If we are willing to make assumptions about the \sphinxstylestrong{functional form} of the structural causal model or the \sphinxstylestrong{distribution of the errors}, we can go beyond the Markov equivalence class and identify the complete causal structure of the data\sphinxhyphen{}generating process. In this first chapter of semi\sphinxhyphen{}parametric models, we introduce the \sphinxstylestrong{linear non\sphinxhyphen{}Gaussian acyclic model (LiNGAM)} {[}\hyperlink{cite.bibliography:id4}{SHHyvarinen+06}{]}.


\section{The LiNGAM model}
\label{\detokenize{notebooks/semiparametric_direct_lingam:the-lingam-model}}
\sphinxAtStartPar
The LiNGAM model for \(p\) observed variables \(x_1, x_2, \ldots, x_p\) is given by
\label{equation:notebooks/semiparametric_direct_lingam:e036b46b-4032-450a-bbae-cc54a1b2203e}\begin{equation}
    x_i = \sum_{j \in \operatorname{pa}(x_i)}b_{ij}x_j + e_i \quad (i=1, \ldots, p)
\end{equation}
\sphinxAtStartPar
where each observed variable \(x_i\) is a linear sum of their parent variables \(\operatorname{pa}(x_i)\) plus some noise \(e_i\). If the coefficient \(b_{ij}\) is zero, then there is no direct causal effect from \(x_j\) to \(x_i\). The error variables \(e_i\) are \sphinxstylestrong{independent} and follow continuous \sphinxstylestrong{non\sphinxhyphen{}Gaussian} distributions. The independence means there are \sphinxstylestrong{no unobserved or hidden common causes}.

\sphinxAtStartPar
In matrix notation, the model is given by
\label{equation:notebooks/semiparametric_direct_lingam:746bc263-43ad-4207-a077-5ac9068c60ed}\begin{equation}
    \mathbf{x} = \mathbf{B}\mathbf{x} + \mathbf{e}
\end{equation}
\sphinxAtStartPar
where \(\mathbf{x}\) and \(\mathbf{e}\) are \(p\)\sphinxhyphen{}dimensional vectors, and \(\mathbf{B}\) is a \(p\times p\) matrix that contains the \(b_{ij}\) coefficients, with \(i, j=1, \ldots, p\).

\sphinxAtStartPar
Let us consider a simple \sphinxstylestrong{example} with three variables. To ease the introduction of the LiNGAM model, we will use the variables from the chain example of the previous chapters, where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Wind forecast errors} are directly influences by sudden changes in the weather.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Balancing costs} are influenced by wind forecast errors. For example, if the system is short because it was expecting a larger production from wind farms, it might incur balancing costs to procure electricity from alternative sources.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Weather fluctuarions} are influenced by external factors and serve as an exogenous variable in this model.

\end{itemize}


\subsection{Strucural equations}
\label{\detokenize{notebooks/semiparametric_direct_lingam:strucural-equations}}
\sphinxAtStartPar
If we know the coefficients of those causal relations, the \sphinxstylestrong{SCM} can be written as
\label{equation:notebooks/semiparametric_direct_lingam:126fed39-c044-49c2-a274-2bc04916624a}\begin{align}
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\text{balancing costs} &= 1.2 \times \text{wind forecast errors} + e_b \\
\text{weather fluctuations} &= e_w \\
\end{align}

\subsection{Matrix notation}
\label{\detokenize{notebooks/semiparametric_direct_lingam:matrix-notation}}
\sphinxAtStartPar
Then, in matrix notation, this model is given by
\label{equation:notebooks/semiparametric_direct_lingam:cd586ba3-5dad-4e47-8ec3-e62962648996}\begin{equation}
    \left[\begin{array}{c} 
    \text{wind forecast errors} \\
    \text{balancing costs} \\
    \text{weather fluctuations}
    \end{array}\right]
    =
    \left[\begin{array}{ccc}
    0 & 0 & 1.5 \\
    1.2 & 0 & 0 \\
    0 & 0 & 0
    \end{array}\right]
    \left[\begin{array}{c} 
    \text{wind forecast errors} \\
    \text{balancing costs} \\
    \text{weather fluctuations}
    \end{array}\right]
    +
    \left[\begin{array}{c}
    e_f \\
    e_b \\
    e_w
    \end{array}\right]
\end{equation}

\subsection{Causal ordering}
\label{\detokenize{notebooks/semiparametric_direct_lingam:causal-ordering}}
\sphinxAtStartPar
Typically, we \sphinxstylestrong{order} the the structural equations according to the \sphinxstylestrong{true causal order}, so that the matrix \(\mathbf{B}\) is permuted to be a \sphinxstylestrong{lower triangular matrix} (this property is used later on to identify the true structure), with all the diagonal elements equal to zero (strictly lower triangular). This simply means that we rewrite the structural equations as
\label{equation:notebooks/semiparametric_direct_lingam:06a4ef55-3daf-478e-a8af-86a722343790}\begin{align}
\text{weather fluctuations} &= e_w \\
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\text{balancing costs} &= 1.2 \times \text{wind forecast errors} + e_b \\
\end{align}
\sphinxAtStartPar
that, in matrix notation, becomes
\label{equation:notebooks/semiparametric_direct_lingam:1b6162f0-885e-4e42-937e-4806b04e2a2e}\begin{equation}
    \left[\begin{array}{c} 
    \text{weather fluctuations} \\
    \text{wind forecast errors} \\
    \text{balancing costs}
    \end{array}\right]
    =
    \left[\begin{array}{ccc}
    0 & 0 & 0 \\
    1.5 & 0 & 0 \\
    0 & 1.2 & 0
    \end{array}\right]
    \left[\begin{array}{c} 
    \text{weather fluctuations} \\
    \text{wind forecast errors} \\
    \text{balancing costs}
    \end{array}\right]
    +
    \left[\begin{array}{c}
    e_w \\
    e_f \\
    e_b
    \end{array}\right]
\end{equation}
\sphinxAtStartPar
Now, we can rewrite the LiNGAM model as
\label{equation:notebooks/semiparametric_direct_lingam:23341fb4-fd30-436f-bad4-09c099f2362f}\begin{equation}
    x_i = \sum_{j:k(j)<k(i)}b_{ij}x_j + e_i \quad (i=1, \ldots, p)
\end{equation}
\sphinxAtStartPar
where \(k(\cdot)\) represents the causal ordering, meaning that each variable \(x_i\) is a linear sum of the \(x_j\) variables observed earlier in the causal graph (\(k(j)<k(i)\)), plus its own error variable \(e_i\). We can now plot the \sphinxstylestrong{causal graph}, using the LiNGAM library.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}dot}

\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{m} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{m}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weather }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{fluctuations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wind forecast }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{errors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x13f5f7850\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Estimation methods}
\label{\detokenize{notebooks/semiparametric_direct_lingam:estimation-methods}}
\sphinxAtStartPar
Now that we explained the basic formulation, we explain how to \sphinxstylestrong{estimate the model from the data}. There are two main approaches to estimate a LiNGAM model: a direct estimation method (\sphinxstylestrong{DirectLiNGAM}) through a series of regressions and independence tests, and a method based on independent component analysis (ICA) (\sphinxstylestrong{ICA\sphinxhyphen{}based LinGAM}). In the case of linear models, the key assumption is that the \sphinxstylestrong{errors are non\sphinxhyphen{}Gaussians}. This is the most important assumption as it allows us to identify the causal structure, both using DirectLiNGAM and ICA\sphinxhyphen{}based LiNGAM.

\sphinxAtStartPar
Now, we will explain the intuition behing DirectLiNGAM, an iterative procedure that employs a series of regressions and independence tests to directly identify the causal order among the observed variables. It leverages the non\sphinxhyphen{}Gaussianity of the data and the assumption of linear causality to systematically test for independence between variables and their residuals from regressions.

\sphinxAtStartPar
The key intuition is that, with non\sphinxhyphen{}Gaussian errors, the \sphinxstylestrong{residuals in the anti\sphinxhyphen{}causal direction} will not be independent of the predictor. We will see how this works with a practical example.


\subsection{Example with Gaussian errors}
\label{\detokenize{notebooks/semiparametric_direct_lingam:example-with-gaussian-errors}}
\sphinxAtStartPar
Let us consider a simple example, like the one we saw above, but for simplicity we only keep two variables:
\label{equation:notebooks/semiparametric_direct_lingam:38fff02b-b747-4918-ba8a-abd9e5493a3b}\begin{align}
\text{weather fluctuations} &= e_w \\
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\end{align}
\sphinxAtStartPar
We now assume Gaussian errors, so we have that \(e_w, e_f  \sim \mathcal{N}(0, 1)\).

\sphinxAtStartPar
We will show that, in this case, by analysing the residuals we cannot understand the true causal direction, because we can both predict the wind from the prices and the prices from the wind.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Synthetic data generation based on the given equations}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Predictor variable}
\PYG{n}{forecast\PYGZus{}errors} \PYG{o}{=} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Response variable with non\PYGZhy{}Gaussian noise (uniform)}

\PYG{c+c1}{\PYGZsh{} Prepare the data for regression}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Add a constant term for the intercept}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Add a constant term for the intercept}

\PYG{c+c1}{\PYGZsh{} Linear regression in the true causal direction (wind \PYGZhy{}\PYGZgt{} price)}
\PYG{n}{true\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{endog}\PYG{o}{=}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{n}{exog}\PYG{o}{=}\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{true\PYGZus{}model\PYGZus{}results} \PYG{o}{=} \PYG{n}{true\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}pred} \PYG{o}{=} \PYG{n}{true\PYGZus{}model\PYGZus{}results}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{forecast\PYGZus{}errors} \PYG{o}{\PYGZhy{}} \PYG{n}{forecast\PYGZus{}errors\PYGZus{}pred}

\PYG{c+c1}{\PYGZsh{} Linear regression in the anti\PYGZhy{}causal direction (price \PYGZhy{}\PYGZgt{} wind)}
\PYG{n}{anti\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{endog}\PYG{o}{=}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{n}{exog}\PYG{o}{=}\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{anti\PYGZus{}model\PYGZus{}results} \PYG{o}{=} \PYG{n}{anti\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}pred} \PYG{o}{=} \PYG{n}{anti\PYGZus{}model\PYGZus{}results}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{\PYGZhy{}} \PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}pred}

\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} All the data}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{n}{forecast\PYGZus{}errors\PYGZus{}residuals}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals in the true causal direction}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(weather fluctuations \PYGZhy{}\PYGZgt{} forecast errors)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weather}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} After conditioning}
\PYG{n}{hb2} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}residuals}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals in the anti\PYGZhy{}causal direction}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(forecast errors \PYGZhy{}\PYGZgt{} weather fluctuations)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Residuals\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{1b2e2d532fc3a177d88d74f68c28c1af9404464f4bdee44156a5bd0e0ff4b35c}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Where we can see how the residuals are independent of the input variable in both the true and anti\sphinxhyphen{}causal directions.


\subsection{Example with non\sphinxhyphen{}Gaussian errors}
\label{\detokenize{notebooks/semiparametric_direct_lingam:example-with-non-gaussian-errors}}
\sphinxAtStartPar
Now, by simply assuming the errors are uniformly distributed, we can wee how it will be possible to detect the true causal direction by testing the independence of the residuals from the input variable.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Synthetic data generation based on the given equations}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10000}
\PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Predictor variable}
\PYG{n}{forecast\PYGZus{}errors} \PYG{o}{=} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Response variable with non\PYGZhy{}Gaussian noise (uniform)}

\PYG{c+c1}{\PYGZsh{} Prepare the data for regression}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Add a constant term for the intercept}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Add a constant term for the intercept}

\PYG{c+c1}{\PYGZsh{} Linear regression in the true causal direction (wind \PYGZhy{}\PYGZgt{} price)}
\PYG{n}{true\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{endog}\PYG{o}{=}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{n}{exog}\PYG{o}{=}\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{true\PYGZus{}model\PYGZus{}results} \PYG{o}{=} \PYG{n}{true\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}pred} \PYG{o}{=} \PYG{n}{true\PYGZus{}model\PYGZus{}results}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{forecast\PYGZus{}errors\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{forecast\PYGZus{}errors} \PYG{o}{\PYGZhy{}} \PYG{n}{forecast\PYGZus{}errors\PYGZus{}pred}

\PYG{c+c1}{\PYGZsh{} Linear regression in the anti\PYGZhy{}causal direction (price \PYGZhy{}\PYGZgt{} wind)}
\PYG{n}{anti\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{endog}\PYG{o}{=}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{n}{exog}\PYG{o}{=}\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{anti\PYGZus{}model\PYGZus{}results} \PYG{o}{=} \PYG{n}{anti\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}pred} \PYG{o}{=} \PYG{n}{anti\PYGZus{}model\PYGZus{}results}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors\PYGZus{}with\PYGZus{}const}\PYG{p}{)}
\PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{\PYGZhy{}} \PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}pred}

\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{axes} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} All the data}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{n}{forecast\PYGZus{}errors\PYGZus{}residuals}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals in the true causal direction}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(weather fluctuations \PYGZhy{}\PYGZgt{} forecast errors)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Weather}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} After conditioning}
\PYG{n}{hb2} \PYG{o}{=} \PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,} \PYG{n}{weather\PYGZus{}fluctuations\PYGZus{}residuals}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals in the anti\PYGZhy{}causal direction}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(forecast errors \PYGZhy{}\PYGZgt{} weather fluctuations)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{axes}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Residuals\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{b65437f398683396a19ec60b0902d7ba88e7b4ca77ceb4238393806bbf877cd9}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Full LiNGAM estimation example}
\label{\detokenize{notebooks/semiparametric_direct_lingam:full-lingam-estimation-example}}
\sphinxAtStartPar
Now, we try a full estimation example, where we have a slightly more complex causal structure, that we will need to retrieve from observational data. We now add to the model:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Outages}: an exogenous variable affecting the balancing costs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{heating demand}: a variable affected by changes in the weather, which affects the balancing costs.

\end{itemize}

\sphinxAtStartPar
Assume the following \sphinxstylestrong{structural equations}:
\label{equation:notebooks/semiparametric_direct_lingam:25bf803c-e49a-4fc0-8c0d-091a4ac1fb4b}\begin{align}
\text{weather fluctuations} &= e_w \\
\text{outages} &= e_o \\
\text{heating demand} &= 0.3 \times \text{weather flucturations} + e_h \\
\text{wind forecast errors} &= 1.5 \times \text{weather flucturations} + e_f \\
\text{balancing costs} &= 1.5 \times \text{outages} + 0.7 \times \text{heating demand} + 1.2 \times \text{wind forecast errors} + e_b \\
\end{align}
\sphinxAtStartPar
where the errors are now assumed to be \sphinxstylestrong{uniformly distributed}.

\sphinxAtStartPar
Then, the true \sphinxstylestrong{causal graph} is given by

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Weather fluctuations}
    \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Outages}
    \PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Heating demand}
    \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Wind forecast errors}
    \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} Balancing costs}
\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{weather }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{fluctuations}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{outages}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{heating }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{demand}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{wind forecast }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{errors}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x145059b90\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let us now generate some data from this causal graph.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} Set the random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Number of samples}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} Generating synthetic data based on the specified equations with uniform errors}
\PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)} 
\PYG{n}{outages} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{forecast\PYGZus{}errors} \PYG{o}{=} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{heating} \PYG{o}{=} \PYG{l+m+mf}{0.3} \PYG{o}{*} \PYG{n}{weather\PYGZus{}fluctuations} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{balancing\PYGZus{}costs} \PYG{o}{=} \PYG{l+m+mf}{0.7} \PYG{o}{*} \PYG{n}{heating} \PYG{o}{+} \PYG{l+m+mf}{1.2} \PYG{o}{*} \PYG{n}{forecast\PYGZus{}errors} \PYG{o}{+} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{outages} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Creating a DataFrame to hold the generated data}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{weather fluctuations}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{weather\PYGZus{}fluctuations}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{outages}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{outages}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind forecast errors}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{forecast\PYGZus{}errors}\PYG{p}{,}
                     \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{heating demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{heating}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{balancing costs}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{balancing\PYGZus{}costs}\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, we fit the \sphinxstylestrong{DirectLiNGAM} model and plot the estimated causal structure.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{lingam}

\PYG{n}{direct\PYGZus{}model} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{DirectLiNGAM}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{direct\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{direct\PYGZus{}model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x1451afc50\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how we successfully recovered the true casual structure, even though the model did not find the exact coefficients for the wind forecast errors and heating demand.

\sphinxstepscope


\chapter{Nonlinear Models}
\label{\detokenize{notebooks/semiparametric_resit:nonlinear-models}}\label{\detokenize{notebooks/semiparametric_resit::doc}}
\sphinxAtStartPar
In many scenarios, assuming a linear relationship between observed variables may not accurately reflect the true dynamics of the system. For instance, consider the relationship between temperature and electricity load. This relationship is characteristically nonlinear, exhibiting increased electricity consumption at both colder and warmer temperatures, with consumption dipping during mild temperature days.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Setting a random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generating synthetic data}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{10000}  \PYG{c+c1}{\PYGZsh{} number of days for a year}
\PYG{n}{temperatures} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} average temperature in Celsius}
\PYG{n}{electricity\PYGZus{}load} \PYG{o}{=} \PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperatures} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{70} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Quadratic relationship for U\PYGZhy{}shape}

\PYG{c+c1}{\PYGZsh{} Creating a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{o}{+}\PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperatures}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{electricity\PYGZus{}load}\PYG{p}{,}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Setting up the plots}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{6}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature vs Electricity Load with hexbin plot}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature vs Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Text(0, 0.5, \PYGZsq{}Electricity Load (MW)\PYGZsq{})
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{8465beff073ed934c20ad043b5b194311ac49c30f1ac5852877b63cb07d03df1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
It turns out that \sphinxstylestrong{the presence of nonlinearities is beneficial for the causal discovery}. Just like non\sphinxhyphen{}Gaussian error distributions, nonlinear functional relations introduce asymmetries in the data distributions that can be exploited to infer the causal structure. Indeed, non\sphinxhyphen{}invertible functional relationships between the observed variables can provide clues to the generating causal model.


\section{Additive noise models (ANMs)}
\label{\detokenize{notebooks/semiparametric_resit:additive-noise-models-anms}}
\sphinxAtStartPar
One of the most popular approaches to causal discovery with nonlinear models is to use additive noise models (ANMs) {[}\hyperlink{cite.bibliography:id5}{PMJScholkopf14}{]}. In the case of an ANM, each observed variable \(x_i\) is obtained as a function of its parent variables, plus independent additive noise
\label{equation:notebooks/semiparametric_resit:70cc48cc-63dc-4bc3-912a-977971826cb6}\begin{equation}
    x_i = f_i(\mathbf{x}_{\text{pa}(i)})+ e_i
\end{equation}
\sphinxAtStartPar
where \(f_i\) is an arbitrary function (possibly different for each \(i\)), and the noise variables \(e_i\) are jointly independent and with arbitrary probability densities \(p_{e_i}(e_i)\). We can easily see that when the functions \(f_i\) are linear and the \(p_{e_i}(e_i)\) are non\sphinxhyphen{}Gaussian, we are in the standard LiNGAM settings.


\section{Estimation}
\label{\detokenize{notebooks/semiparametric_resit:estimation}}
\sphinxAtStartPar
The model can be estimated using the \sphinxstylestrong{regression with subsequent independence test (RESIT)} approach, which follows the same intuition we saw in the DirectLiNGAM case. Indeed, we will see how the residuals in the anti\sphinxhyphen{}causal direction are not independent of the input variable if the function is not linear is non\sphinxhyphen{}Gaussian. The main steps of RESIT are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Test whether \(x\) and \(y\) are statistically independent.

\item {} 
\sphinxAtStartPar
Test whether a model \(y = f(x) + e\) is consistent with the data, by:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumii}{enumiii}{}{.}%
\item {} 
\sphinxAtStartPar
Doing a \sphinxstylestrong{nonlinear regression} of \(y\) on \(x\) to get an estimate \(\widehat{f}\).

\item {} 
\sphinxAtStartPar
Computing the residuals \(\widehat{e}=y-\widehat{f}(x)\).

\item {} 
\sphinxAtStartPar
Testing whether \(\widehat{e}\) is independent of \(x\).

\end{enumerate}

\item {} 
\sphinxAtStartPar
Test in a similar way if the \sphinxstylestrong{reverse model} \(x = g(y) + e\) fits the data.

\end{enumerate}


\subsection{Example with nonlinear functions}
\label{\detokenize{notebooks/semiparametric_resit:example-with-nonlinear-functions}}
\sphinxAtStartPar
Let us consider the simple example with two variables of temperature and electricity load, where
\label{equation:notebooks/semiparametric_resit:bdf1594d-78e9-48b9-bb5b-6ec020fc0d43}\begin{align}
\text{temperature} &= e_t \\
\text{electricity load} &= 0.2 \times (\text{temperature}-20)^2 + 70 + e_l \\
\end{align}
\sphinxAtStartPar
We will now try to fit a \sphinxstylestrong{nonlinear regression model} in the two directions and \sphinxstylestrong{check the residuals}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{preprocessing} \PYG{k+kn}{import} \PYG{n}{PolynomialFeatures}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{pipeline} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}pipeline}

\PYG{c+c1}{\PYGZsh{} Fit a quadratic model (true causal direction: Temperature \PYGZhy{}\PYGZgt{} Electricity Load)}
\PYG{n}{model\PYGZus{}true} \PYG{o}{=} \PYG{n}{make\PYGZus{}pipeline}\PYG{p}{(}\PYG{n}{PolynomialFeatures}\PYG{p}{(}\PYG{n}{degree}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{model\PYGZus{}true}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{electricity\PYGZus{}pred\PYGZus{}true} \PYG{o}{=} \PYG{n}{model\PYGZus{}true}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{residuals\PYGZus{}true} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{electricity\PYGZus{}pred\PYGZus{}true}

\PYG{c+c1}{\PYGZsh{} Fit a quadratic model (anti\PYGZhy{}causal direction: Electricity Load \PYGZhy{}\PYGZgt{} Temperature)}
\PYG{n}{model\PYGZus{}anti} \PYG{o}{=} \PYG{n}{make\PYGZus{}pipeline}\PYG{p}{(}\PYG{n}{PolynomialFeatures}\PYG{p}{(}\PYG{n}{degree}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{model\PYGZus{}anti}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{temperature\PYGZus{}pred\PYGZus{}anti} \PYG{o}{=} \PYG{n}{model\PYGZus{}anti}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{residuals\PYGZus{}anti} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{temperature\PYGZus{}pred\PYGZus{}anti}

\PYG{c+c1}{\PYGZsh{} Plotting residuals vs input variables}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Residuals for true causal direction}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{residuals\PYGZus{}true}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cb1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{hb1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals vs Temperature}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(True Causal Direction)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Residuals for true causal direction}
\PYG{n}{hb1} \PYG{o}{=} \PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{hexbin}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{residuals\PYGZus{}anti}\PYG{p}{,} \PYG{n}{gridsize}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Blues}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{bins}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{log}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{cb1} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{hb1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals vs Electricity Load}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(Anti\PYGZhy{}Causal Direction)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Load (MW)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals (°C)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{fbb3d5c6ed56783405747df5360052fcf09b97c037cae335f9bbac75d3d53048}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Full RESIT example}
\label{\detokenize{notebooks/semiparametric_resit:full-resit-example}}
\sphinxAtStartPar
We now try to apply the RESIT approach to a slightly more complex example, where some of the relationships are \sphinxstylestrong{nonlinear}.

\sphinxAtStartPar
Let us assume the following \sphinxstylestrong{structural equations}:
\label{equation:notebooks/semiparametric_resit:924d8fdc-fe5d-4ab4-bb56-388d9848b564}\begin{align}
\text{temperature} &= e_t \\
\text{electricity load} &= 0.2 \times (\text{temperature}-20)^2 + 70 + e_l \\
\text{thermal constraints} &= 1.5 \times \text{electricity load} + \sin{(\text{temperature})} + e_c \\
\end{align}
\sphinxAtStartPar
where the errors \(e_i\) are now assumed to \sphinxstylestrong{Gaussian}.

\sphinxAtStartPar
The \sphinxstylestrong{causal graph} is then given by

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}dot}

\PYG{c+c1}{\PYGZsh{} Matrix of coefficients (weights)}
\PYG{n}{B} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}
    \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Temperatures}
    \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}   \PYG{c+c1}{\PYGZsh{} Electricity load}
    \PYG{p}{[}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}    \PYG{c+c1}{\PYGZsh{} Thermal constraints}
\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting causal graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{temperatures}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{electricity load}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{thermal constraints}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x16a91ad50\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
where the coefficient of 1 is only an indication of causal relation (and its direction) between two variables

\sphinxAtStartPar
Let us now generate some observations according to that causal graph.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Setting random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generating synthetic data}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{temperatures} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{20}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} average temperature in Celsius}
\PYG{n}{electricity\PYGZus{}load} \PYG{o}{=} \PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperatures} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mi}{70} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Quadratic relationship for U\PYGZhy{}shape}
\PYG{n}{thermal\PYGZus{}constraints} \PYG{o}{=} \PYG{l+m+mf}{1.5}\PYG{o}{*} \PYG{n}{electricity\PYGZus{}load} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{temperatures}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Creating DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{temperatures}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperatures}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{electricity load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{electricity\PYGZus{}load}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{thermal constraints}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{thermal\PYGZus{}constraints}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now try to estimate the causal structure with the \sphinxstylestrong{RESIT} approach.

\sphinxAtStartPar
The interesting part is that we can \sphinxstylestrong{use any nonlinear regression model}. For example, we can try to use RESIT with a \sphinxstylestrong{random forest} regression model.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{lingam}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}

\PYG{n}{rf} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{max\PYGZus{}depth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{RESIT}\PYG{p}{(}\PYG{n}{regressor}\PYG{o}{=}\PYG{n}{rf}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x171ed6850\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Bootstrapping}
\label{\detokenize{notebooks/semiparametric_resit:bootstrapping}}
\sphinxAtStartPar
While our initial results approached the true causal graph, there is always inherent \sphinxstylestrong{uncertainty} in any estimation process derived from a single dataset. To enhance the robustness and reliability of our causal estimates, we can employ bootstrapping, a powerful statistical \sphinxstylestrong{resampling} method.

\sphinxAtStartPar
Bootstrapping involves repeatedly sampling from the original data set with replacement to create multiple “bootstrap” samples. By applying the causal estimation process to each of these samples, we can observe how our estimates vary across different versions of the data. This approach not only helps in assessing the stability of our causal inference but also in improving the accuracy of our estimates by averaging the results.

\sphinxAtStartPar
So, we can estimate multiple DAGs, for each bootstrap replica of our dataset, and get a \sphinxstylestrong{probability score associated to each causal connection}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{print\PYGZus{}causal\PYGZus{}directions}\PYG{p}{,} \PYG{n}{print\PYGZus{}dagc}

\PYG{n}{replications} \PYG{o}{=} \PYG{l+m+mi}{10}
\PYG{n}{result} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{bootstrap}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{n\PYGZus{}sampling}\PYG{o}{=}\PYG{n}{replications}\PYG{p}{)}
\PYG{n}{cdc} \PYG{o}{=} \PYG{n}{result}\PYG{o}{.}\PYG{n}{get\PYGZus{}causal\PYGZus{}direction\PYGZus{}counts}\PYG{p}{(}\PYG{n}{n\PYGZus{}directions}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{n}{min\PYGZus{}causal\PYGZus{}effect}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{split\PYGZus{}by\PYGZus{}causal\PYGZus{}effect\PYGZus{}sign}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{print\PYGZus{}causal\PYGZus{}directions}\PYG{p}{(}\PYG{n}{cdc}\PYG{p}{,} \PYG{n}{replications}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
electricity load \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0) (100.0\PYGZpc{})
thermal constraints \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0) (100.0\PYGZpc{})
electricity load \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} thermal constraints (b\PYGZgt{}0) (50.0\PYGZpc{})
thermal constraints \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} electricity load (b\PYGZgt{}0) (50.0\PYGZpc{})
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{dagc} \PYG{o}{=} \PYG{n}{result}\PYG{o}{.}\PYG{n}{get\PYGZus{}directed\PYGZus{}acyclic\PYGZus{}graph\PYGZus{}counts}\PYG{p}{(}\PYG{n}{n\PYGZus{}dags}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{min\PYGZus{}causal\PYGZus{}effect}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{split\PYGZus{}by\PYGZus{}causal\PYGZus{}effect\PYGZus{}sign}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{print\PYGZus{}dagc}\PYG{p}{(}\PYG{n}{dagc}\PYG{p}{,} \PYG{n}{replications}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
DAG[0]: 50.0\PYGZpc{}
	electricity load \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0)
	thermal constraints \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0)
	thermal constraints \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} electricity load (b\PYGZgt{}0)
DAG[1]: 50.0\PYGZpc{}
	electricity load \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0)
	electricity load \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} thermal constraints (b\PYGZgt{}0)
	thermal constraints \PYGZlt{}\PYGZhy{}\PYGZhy{}\PYGZhy{} temperatures (b\PYGZgt{}0)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Time Series Models}
\label{\detokenize{notebooks/semiparametric_varlingam:time-series-models}}\label{\detokenize{notebooks/semiparametric_varlingam::doc}}
\sphinxAtStartPar
In many cases, the \sphinxstylestrong{temporal dimension} introduces complexities that traditional causal models may not adequately address. Particularly in fields like economics, environmental science, and engineering, understanding how variables influence each other both \sphinxstylestrong{instantaneously and across time lags} is crucial for predicting future states and designing effective interventions.

\sphinxAtStartPar
To capture both the immediate and delayed interactions among multiple time\sphinxhyphen{}dependent variables, it is important to employ models that can account for these dynamics. We will now introduce an extension of the LiNGAM model, based on \sphinxstylestrong{vector autoregression (VAR)} models, which has been introduced in {[}\hyperlink{cite.bibliography:id6}{HyvarinenSH08}{]} and {[}\hyperlink{cite.bibliography:id7}{HyvarinenZSH10}{]}.


\section{VARLiNGAM model}
\label{\detokenize{notebooks/semiparametric_varlingam:varlingam-model}}
\sphinxAtStartPar
Assume that at each time step \(t\) we observe a vector \(\mathbf{x}(t)\) with \(p\) variables
\label{equation:notebooks/semiparametric_varlingam:c9908b3f-cd3b-4f36-a94c-11cfe9e4344e}\begin{equation}
    \mathbf{x}(t) = [\mathbf{x}_1(t), \mathbf{x}_2(t), \ldots, \mathbf{x}_p(t)]^\top
\end{equation}
\sphinxAtStartPar
Then, we use the following structural causal model, which considers both the instantaneous (\(\tau = 0\)) and lagged (\(\tau \geq 1\)) causal effects
\label{equation:notebooks/semiparametric_varlingam:24719788-aaf4-4956-8c3b-e686bfcbf259}\begin{equation}
    \mathbf{x}(t) = \sum_{\tau=0}^{k}\mathbf{B}_{\tau}\mathbf{x}(t-\tau)+\mathbf{e}(t)
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\mathbf{B}\) is the coefficient matrix representing the magnitudes of the contemporaneous (\(\mathbf{B}_0\)) and lagged (\(\mathbf{B}_{\tau}\), \(\tau=1, \ldots, h\)) causal effects.

\item {} 
\sphinxAtStartPar
\(\mathbf{e}(t)\) are the non\sphinxhyphen{}Gaussian mutually independent (both of each other and over time) errors.

\end{itemize}

\sphinxAtStartPar
A sufficient condition for the coefficient matrices \(\mathbf{B}_0\) and \(\mathbf{B}_{\tau}\) to be identifiable is that the contemporaneous causal relations represented by the matrix \(\mathbf{B}_0\) is acyclic and the error variables \(e_i(t)\), with \(i=1, \ldots, p\) and \(t=1, \ldots, T\), are non\sphinxhyphen{}Gaussian and independent. This independence implies \sphinxstylestrong{no hidden common cause} between the observed variables at the same point in time or between the observed variables at different points in time.

\sphinxAtStartPar
For example, if we consider the following variables:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Solar generation} (\(x_1(t)\)): amount of electricity generated from solar power at time \(t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transmission capacity usage} (\(x_2(t)\)): percentage of transmission capacity utilized at time \(t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Balancing costs} (\(x_3(t)\)): costs incurred to ensure grid stability and balance the electricity supply with demand at time \(t\).

\end{itemize}

\sphinxAtStartPar
At time \(t\), we will observe the vector given by
\label{equation:notebooks/semiparametric_varlingam:ed1a2c8b-659e-4f9b-9ccb-809e4c2f8d08}\begin{equation}
    \mathbf{x}(t)=\left[\begin{array}{c} 
    \text{solar generation}(t) \\
    \text{transmission capacity usage}(t) \\
    \text{balancing costs}(t)
    \end{array}\right]
\end{equation}
\sphinxAtStartPar
and we can then model the time series data with the following relationships
\label{equation:notebooks/semiparametric_varlingam:bb1e90ee-ca0b-4a06-9ac9-e8deb27ffded}\begin{equation}
        \mathbf{x}(t) = \mathbf{B}_0 \mathbf{x}(t) + \mathbf{B}_1 \mathbf{x}(t-1) + \mathbf{e}(t)
\end{equation}
\sphinxAtStartPar
where \(\mathbf{B}_0\) is the matrix of the \sphinxstylestrong{contemporaneous effects}, given by
\label{equation:notebooks/semiparametric_varlingam:8a657789-0286-4a16-b0d9-6a5d9e4e7d81}\begin{equation}
    \left[\begin{array}{ccc}
    0 & 0 & 0 \\
    -1.0 & 0 & 0 \\
    1.5 & 0.2 & 0
    \end{array}\right]
\end{equation}
\sphinxAtStartPar
implying that current solar generation negatively affects the transmission capacity usage due to intermittency issues. Increased solar output at the same time increases the need for grid balancing due to the variable nature of solar power, which can affect grid stability.

\sphinxAtStartPar
Instead, \(\mathbf{B}_1\) is the matrix of the \sphinxstylestrong{lagged effects}, given by
\label{equation:notebooks/semiparametric_varlingam:b1e9dc81-2761-4fb3-b15f-794d695508e2}\begin{equation}
    \left[\begin{array}{ccc}
    0 & 0 & 0 \\
    0.0 & 0 & 0 \\
    0.5 & 1.5 & 0
    \end{array}\right]
\end{equation}
\sphinxAtStartPar
capturing how solar generation and transmission capacity usage from the previous time step (\(t−1\)) influence current balancing costs. It suggests that previous fluctuations in solar output and transmission constraints can lead to increased current balancing costs.

\sphinxAtStartPar
Finally, \(\mathbf{e}(t)\) represent the non\sphinxhyphen{}Gaussian error terms representing other unmodeled influences.


\section{Estimation}
\label{\detokenize{notebooks/semiparametric_varlingam:estimation}}
\sphinxAtStartPar
One possible estimation method is to combine a traditional least\sphinxhyphen{}squares estimation of an autoregressive (AR) model with the LiNGAM estimation. The key intuition is that the model shown in the introduction is essentially \sphinxstylestrong{a LiNGAM model for the residuals of the predictions made by a traditional VAR model} that only considers the lagged effects and not the contemporaneous ones (where \(\tau >0\)). The estimation method is based on three steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Obtain a least\sphinxhyphen{}squares estimate \(\widehat{\mathbf{M}}_{\tau}\) of the AR model given by
\label{equation:notebooks/semiparametric_varlingam:1d2f2338-c442-42b3-bb7b-1c7c6a1d0485}\begin{equation}
        \mathbf{x}(t) = \sum_{\tau=1}^{k}\mathbf{M}_{\tau}\mathbf{x}(t-\tau)+\mathbf{n}(t)
    \end{equation}
\sphinxAtStartPar
\(\widehat{\mathbf{M}}_{\tau}\) can be thought of as a matrix of the preliminary guesses for the lagged effects.

\item {} 
\sphinxAtStartPar
Compute the residuals of the AR model, which are an estimate of the true errors \(\mathbf{n}(t)\)
\label{equation:notebooks/semiparametric_varlingam:150262ef-fd1a-4687-9458-dca8cd933e05}\begin{equation}
        \widehat{\mathbf{n}}(t) = \mathbf{x}(t) - \sum_{\tau=1}^{k}\widehat{\mathbf{M}}_{\tau}\mathbf{x}(t-\tau)
    \end{equation}
\sphinxAtStartPar
These residuals are the differences between the actual observed data and the predictions from the AR model. They should contain the influences not captured by the past values (potentially the instantaneous effects we are interested in).

\item {} 
\sphinxAtStartPar
Perform a LiNGAM analysis on the residuals, which returns an \sphinxstylestrong{estimate of the coefficient matrix for the instantaneous causal effects} \(\mathbf{B}_0\)
\label{equation:notebooks/semiparametric_varlingam:aa633dc7-e75b-40e3-991c-5ee0cac7f7a8}\begin{equation}
        \widehat{\mathbf{n}}(t) = \mathbf{B}_0 \widehat{\mathbf{n}}(t) + \mathbf{e}(t)
    \end{equation}
\sphinxAtStartPar
The idea is that if there are instantaneous effects, they would show up in the residuals of the AR model, because the AR model did not account for them.

\item {} 
\sphinxAtStartPar
Once \(\mathbf{B}_0\) has been estimated, the next step is to disentangle the lagged causal effects from the total observed effects captured in the AR model. So, we compute the \sphinxstylestrong{estimates of the coefficient matrices for the lagged causal effects}, \(\mathbf{B}_{\tau}\) for \(\tau >0\)
\label{equation:notebooks/semiparametric_varlingam:1ce0140c-5b53-4204-8e64-2c97891741ab}\begin{equation}
        \widehat{\mathbf{B}_{\tau}} = (\mathbf{I} - \widehat{\mathbf{B}}_0) \widehat{\mathbf{M}}_{\tau}
    \end{equation}
\end{enumerate}


\section{Full VARLiNGAM example}
\label{\detokenize{notebooks/semiparametric_varlingam:full-varlingam-example}}
\sphinxAtStartPar
We will no generate observations of the variables introduced before: \sphinxstylestrong{solar generation}, \sphinxstylestrong{transmission capacity usage}, and \sphinxstylestrong{balancing costs}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{graphviz}
\PYG{k+kn}{import} \PYG{n+nn}{lingam}
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}dot}\PYG{p}{,} \PYG{n}{print\PYGZus{}causal\PYGZus{}directions}\PYG{p}{,} \PYG{n}{print\PYGZus{}dagc}
\PYG{k+kn}{import} \PYG{n+nn}{warnings}
\PYG{n}{warnings}\PYG{o}{.}\PYG{n}{filterwarnings}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ignore}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{category}\PYG{o}{=}\PYG{n+ne}{UserWarning}\PYG{p}{)}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{o}{\PYGZpc{}}\PYG{k}{matplotlib} inline
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The matrices explaining the contemporaneous and lagged effects are given by

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B0} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{B1} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
      \PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,}\PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{causal\PYGZus{}order} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Contemporaneous effects}
\label{\detokenize{notebooks/semiparametric_varlingam:contemporaneous-effects}}
\sphinxAtStartPar
As we can see, at time \(t\) balancing costs are influenced by the current usage of the transmission capacity, and by the current solar generation. We can also see how the transmission capacity is negatively impacted by the solar power generation.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{labels\PYGZus{}0} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{solar power }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{generation (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transmission capacity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{usage (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B0}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x148849610\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Lagged effects}
\label{\detokenize{notebooks/semiparametric_varlingam:lagged-effects}}
\sphinxAtStartPar
Given the time\sphinxhyphen{}dependent nature of the data, balancing costs at time \(t\) are also dependent upon the solar generation and spare capacity observed at the previous time step. However, there are no lagged causal associations between solar generation and transmission capacity.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{labels\PYGZus{}1} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{solar power }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{generation (t\PYGZhy{}1)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transmission capacity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{usage (t\PYGZhy{}1)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x13d1aaf10\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Putting all together}
\label{\detokenize{notebooks/semiparametric_varlingam:putting-all-together}}
\sphinxAtStartPar
We can now see the complete causal structure of the data\sphinxhyphen{}generating process, including both contemporaneous and lagged effects.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{solar power }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{generation (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transmission capacity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{usage (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs (t)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,}
         \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{solar power }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{generation (t\PYGZhy{}1)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transmission capacity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{usage (t\PYGZhy{}1)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{costs (t\PYGZhy{}1)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{B0}\PYG{p}{,} \PYG{n}{B1}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}shape}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{lower\PYGZus{}limit}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x148289610\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We will now \sphinxstylestrong{generate some observations} using the previous causal graph, and then try to retrieve the true causal structure using \sphinxstylestrong{VARLiNGAM}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Generating data according to this causal graph}
\PYG{n}{B0} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B0}\PYG{p}{)}
\PYG{n}{B1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Sample size}
\PYG{n}{n\PYGZus{}features} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{causal\PYGZus{}order}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{1000}

\PYG{c+c1}{\PYGZsh{} Initialize the data matrix with zeros (considering t and t\PYGZhy{}1)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}features}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} +1 to accommodate initial values at t=\PYGZhy{}1}

\PYG{c+c1}{\PYGZsh{} Add non\PYGZhy{}Gaussian noise for each variable and each time point}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}features}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate data according to true causal graph}
\PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{for} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n}{causal\PYGZus{}order}\PYG{p}{:}
        \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{n}{var}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B0}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B1}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Drop the initial row used for t=\PYGZhy{}1 values}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Convert to DataFrame for easier handling}
\PYG{n}{cols} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{solar power generation}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{transmission capacity usage}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{balancing costs}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{n}{cols}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now plot the generated time series

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{cols}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x13d8a7850\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{f981074c552af62dfbc9913ca43b06e177fe6d0bd0bb9383b7efc6b1697fc07d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Estimating contemporaneous effects}
\label{\detokenize{notebooks/semiparametric_varlingam:estimating-contemporaneous-effects}}
\sphinxAtStartPar
This process is relatively simple, and includes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fitting a VAR model} to the time series, to get rid of autocorrelation and lagged effects.

\item {} 
\sphinxAtStartPar
Applying traditional \sphinxstylestrong{LiNGAM model to the residuals}.

\end{enumerate}

\sphinxAtStartPar
This allows us to estimate the matrix of contemporaneous effects, \(B_0\).

\sphinxAtStartPar
Here we fit a VAR model

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{tsa}\PYG{n+nn}{.}\PYG{n+nn}{vector\PYGZus{}ar}\PYG{n+nn}{.}\PYG{n+nn}{var\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{VAR}

\PYG{n}{model} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Fit VAR(1)}
\PYG{n}{residuals} \PYG{o}{=} \PYG{n}{results}\PYG{o}{.}\PYG{n}{resid}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
let’s plot the residuals

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{residuals}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{residuals}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{residuals}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{cols}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x13fa83610\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{acfdbb01874de9bd4aa0be4eb4a7e365f192f2f385c531a23c8203e09ba4fe77}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, we can simply apply LiNGAM (as we did in the initial chapter) to the extracted residuals.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{direct\PYGZus{}model} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{DirectLiNGAM}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{direct\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{residuals}\PYG{p}{)}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{direct\PYGZus{}model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x14b76f550\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how we got extremely close to the true contemporaneous effects.


\subsection{Estimating lagged effects}
\label{\detokenize{notebooks/semiparametric_varlingam:estimating-lagged-effects}}
\sphinxAtStartPar
We can now estimate the matrix of lagged effects, \(B_1\). The process (included in the VARLiNGAM method) involves correcting the matrix estimated by the VAR model to take into account the effects already considered in \(B_0\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{model} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{VARLiNGAM}\PYG{p}{(}\PYG{n}{lags}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrices\PYGZus{}}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels\PYGZus{}1}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} B1}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x13fa5e890\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Unfortunately, this time the causal graph is not precisely equal to the true one for the lagged effects. However, the largest lagged effects (transmission capacity and solar generation on balancing costs) have been properly identified, with the correct magnitude and sign.


\subsection{Complete causal graph}
\label{\detokenize{notebooks/semiparametric_varlingam:complete-causal-graph}}
\sphinxAtStartPar
We can now create the complete causal structure by simply stacking the two coefficient matrices \(B_0\) and \(B_1\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Combined estimated graph}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{hstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{direct\PYGZus{}model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{,} \PYG{n}{model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrices\PYGZus{}}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ignore\PYGZus{}shape}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{lower\PYGZus{}limit}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{labels}\PYG{o}{=}\PYG{n}{labels}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x14b7455d0\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Structural Breaks}
\label{\detokenize{notebooks/structural_breaks_example:structural-breaks}}\label{\detokenize{notebooks/structural_breaks_example::doc}}
\sphinxAtStartPar
Another challenges related to time series analysis and causal inference, besides the presence of lagged effects, is the possibility of the system to be in \sphinxstylestrong{multiple states or regimes}. This is a problem well known to economists, since financial data often exhibits distinct behaviour depending on the specific state of the system (e.g., growth or recession).

\sphinxAtStartPar
In electricity markets, we can also witness something similar. In particular, in the \sphinxstylestrong{balancing market}, the system is constantly switching between two states {[}\hyperlink{cite.bibliography:id8}{BIM20}{]}.:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Short}, when there is a shortage of energy.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Long}, when there is an excess of energy.

\end{itemize}

\sphinxAtStartPar
Here, we will just provide an example of the challenges of trying to infer the causal structure of the system, if there is a \sphinxstylestrong{concept drift} affecting the system. In this case, we refer to concept drift as an alteration of the data\sphinxhyphen{}generating process, where the coefficients relating different variables change over time. For example:
\begin{itemize}
\item {} 
\sphinxAtStartPar
A \sphinxstylestrong{market policy} might significantly affect the behaviour or market partecipants.

\item {} 
\sphinxAtStartPar
Newly introduced \sphinxstylestrong{subsidies} might alter the cost structures of energy generators.

\item {} 
\sphinxAtStartPar
Advances in \sphinxstylestrong{battery technology} might introduce new variables.

\end{itemize}


\section{Scenario with two regimes}
\label{\detokenize{notebooks/structural_breaks_example:scenario-with-two-regimes}}
\sphinxAtStartPar
Let’s consider a scenario in the electricity market where three variables are impacted by a significant \sphinxstylestrong{policy change}, such as the introduction of a new subsidy for renewable energy. This policy change affects the relationships among these variables, illustrating how concept drift can occur in response to external changes.


\subsection{Pre\sphinxhyphen{}subsidy structural model:}
\label{\detokenize{notebooks/structural_breaks_example:pre-subsidy-structural-model}}
\sphinxAtStartPar
Before the introduction of subsidies, the causal relationships might be modeled as follows:
\label{equation:notebooks/structural_breaks_example:fe867875-980c-424d-b9e3-17a609e7be74}\begin{equation}
    \mathbf{x}(t) = \mathbf{B}_{\text{pre}} \mathbf{x}(t-1) + \mathbf{e}(t)
\end{equation}
\sphinxAtStartPar
where \(\mathbf{B}_{\text{pre}}\) is:
\label{equation:notebooks/structural_breaks_example:ce686e98-6181-404f-877d-94ab381d9e0e}\begin{equation}
    \mathbf{B}_{\text{pre}} = 
    \begin{bmatrix}
    0 & 0 & 0 \\
    2 & 0 & 0 \\
    -1 & 0.05 & 0
    \end{bmatrix}
\end{equation}

\subsection{Post\sphinxhyphen{}subsidy structural model:}
\label{\detokenize{notebooks/structural_breaks_example:post-subsidy-structural-model}}
\sphinxAtStartPar
After the introduction of subsidies, the relationships change as follows:
\label{equation:notebooks/structural_breaks_example:97dfa300-52b3-491a-aa16-2f58b66b8ffc}\begin{equation}
\mathbf{x}(t) = \mathbf{B}_{\text{post}} \mathbf{x}(t-1) + \mathbf{e}(t)
\end{equation}
\sphinxAtStartPar
where \(\mathbf{B}_{\text{post}}\) is:
\label{equation:notebooks/structural_breaks_example:d8af92e9-fb96-4a6f-9422-2f4b90e0bc30}\begin{equation}
    \mathbf{B}_{\text{post}} = 
    \begin{bmatrix}
    0 & 0 & 0 \\
    -1 & 0 & 0 \\
    5 & 0 & 0
    \end{bmatrix}
\end{equation}
\sphinxAtStartPar
This change in coefficients represents a concept drift, where the underlying data\sphinxhyphen{}generating process has altered due to external policy intervention. The challenge in causal inference here is to accurately detect and adapt to these changes, ensuring that models remain valid over time despite the evolving relationships. Failure to account for such drifts can lead to inaccurate predictions and misguided policy or business decisions.


\subsection{Contemporaneous effects}
\label{\detokenize{notebooks/structural_breaks_example:contemporaneous-effects}}
\sphinxAtStartPar
In \sphinxstylestrong{regime 1} the causal graph is represnted by

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{lingam}\PYG{n+nn}{.}\PYG{n+nn}{utils} \PYG{k+kn}{import} \PYG{n}{make\PYGZus{}dot}

\PYG{n}{B0\PYGZus{}regime1} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{2.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,}\PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime1}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x171a9f2d0\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Instead, in \sphinxstylestrong{regime 2} we have

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B0\PYGZus{}regime2} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{5.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,}\PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime2}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x169516a50\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
(You might have now recognised this as a simple \sphinxstylestrong{fork})


\subsection{Lagged effects}
\label{\detokenize{notebooks/structural_breaks_example:lagged-effects}}
\sphinxAtStartPar
To slightly render the causal discovery problem more difficult, we also introduced some autocorrelation, just like in the \sphinxstylestrong{VARLiNGAM} example we saw in the previous chapter.

\sphinxAtStartPar
This simply means we now also have two \(\mathbf{B}_1\) matrices, one for each regime.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{B1\PYGZus{}regime1} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.7}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.6}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{B1\PYGZus{}regime2} \PYG{o}{=} \PYG{p}{[}\PYG{p}{[}\PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{]}\PYG{p}{,}
              \PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.0}\PYG{p}{,}\PYG{l+m+mf}{0.1}\PYG{p}{]}\PYG{p}{]}

\PYG{n}{causal\PYGZus{}order} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We now \sphinxstylestrong{generate some observations} using this four matrices

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} Sample size}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}features} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{causal\PYGZus{}order}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mi}{500}
\PYG{n}{switch\PYGZus{}point} \PYG{o}{=} \PYG{l+m+mi}{250}  \PYG{c+c1}{\PYGZsh{} Midpoint switch}

\PYG{c+c1}{\PYGZsh{} Coefficients for two different regimes}
\PYG{n}{B0\PYGZus{}regime1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime1}\PYG{p}{)}
\PYG{n}{B0\PYGZus{}regime2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime2}\PYG{p}{)}
\PYG{n}{B1\PYGZus{}regime1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B1\PYGZus{}regime1}\PYG{p}{)}
\PYG{n}{B1\PYGZus{}regime2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{B1\PYGZus{}regime2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Initialize the data matrix with zeros (considering t and t\PYGZhy{}1)}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}features}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} +1 to accommodate initial values at t=\PYGZhy{}1}

\PYG{c+c1}{\PYGZsh{} Data matrix to accommodate initial values at t=\PYGZhy{}1}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}features}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{data}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate data according to VARLiNGAM model with switching regimes}
\PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{sample\PYGZus{}size} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{t} \PYG{o}{\PYGZlt{}} \PYG{n}{switch\PYGZus{}point}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n}{causal\PYGZus{}order}\PYG{p}{:}
            \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{n}{var}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime1}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B1\PYGZus{}regime1}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}
    \PYG{k}{else}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n}{causal\PYGZus{}order}\PYG{p}{:}
            \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{n}{var}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B0\PYGZus{}regime2}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{B1\PYGZus{}regime2}\PYG{p}{[}\PYG{n}{var}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{n}{t}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Drop the initial row used for t=\PYGZhy{}1 values}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Convert to DataFrame for easier handling}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}features}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now plot the data to see what is happening

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{n}{i}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{n}{switch\PYGZus{}point}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{New policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}matplotlib.legend.Legend at 0x173d1a210\PYGZgt{}
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{0c7e634f1e8a8b2fa8688bc1ff02c9cb07302a44c9996b453b1ef0e32f540ec0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can clearly see how the introduction of the new policy altered the time series.


\section{Estimation}
\label{\detokenize{notebooks/structural_breaks_example:estimation}}
\sphinxAtStartPar
To show the difficulty of doing causal discovery using data collected from drifting data streams, we will show what happens in two cases:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
We fit just one model to the whole dataset.

\item {} 
\sphinxAtStartPar
We fit two separate models, one for each regime.

\end{enumerate}


\subsection{Fitting one model to all the data}
\label{\detokenize{notebooks/structural_breaks_example:fitting-one-model-to-all-the-data}}
\sphinxAtStartPar
As we did before, we fit a VAR model, compute the residuals, and apply LiNGAM to its residuals.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{tsa}\PYG{n+nn}{.}\PYG{n+nn}{vector\PYGZus{}ar}\PYG{n+nn}{.}\PYG{n+nn}{var\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{VAR}
\PYG{k+kn}{import} \PYG{n+nn}{lingam}

\PYG{c+c1}{\PYGZsh{} Fit one VAR model}
\PYG{n}{var\PYGZus{}model} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{n}{var\PYGZus{}results} \PYG{o}{=} \PYG{n}{var\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{residuals} \PYG{o}{=} \PYG{n}{var\PYGZus{}results}\PYG{o}{.}\PYG{n}{resid}

\PYG{c+c1}{\PYGZsh{} Apply LiNGAM to the residuals}
\PYG{n}{lingam\PYGZus{}model} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{DirectLiNGAM}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{lingam\PYGZus{}model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{residuals}\PYG{p}{)}
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{lingam\PYGZus{}model}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} B0}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x1696f9f10\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how we got a \sphinxstylestrong{wrong estimation}. The causal structure is like the first regime, but the coefficients are wrong.


\subsection{Fitting two separate models}
\label{\detokenize{notebooks/structural_breaks_example:fitting-two-separate-models}}
\sphinxAtStartPar
We now show the ideal situation, where we assume we were able to \sphinxstylestrong{discern the two regimes} and fit distinct models.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Split the data into two regimes}
\PYG{n}{x\PYGZus{}regime1} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{n}{switch\PYGZus{}point}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}
\PYG{n}{x\PYGZus{}regime2} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{switch\PYGZus{}point}\PYG{p}{:}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Fit VAR model to each regime}
\PYG{n}{var\PYGZus{}model1} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{(}\PYG{n}{x\PYGZus{}regime1}\PYG{p}{)}
\PYG{n}{var\PYGZus{}results1} \PYG{o}{=} \PYG{n}{var\PYGZus{}model1}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} VAR(1) for simplicity}
\PYG{n}{residuals\PYGZus{}regime1} \PYG{o}{=} \PYG{n}{var\PYGZus{}results1}\PYG{o}{.}\PYG{n}{resid}

\PYG{n}{var\PYGZus{}model2} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{(}\PYG{n}{x\PYGZus{}regime2}\PYG{p}{)}
\PYG{n}{var\PYGZus{}results2} \PYG{o}{=} \PYG{n}{var\PYGZus{}model2}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} VAR(1) for simplicity}
\PYG{n}{residuals\PYGZus{}regime2} \PYG{o}{=} \PYG{n}{var\PYGZus{}results2}\PYG{o}{.}\PYG{n}{resid}

\PYG{c+c1}{\PYGZsh{} Apply LiNGAM for each regime}
\PYG{n}{lingam\PYGZus{}model1} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{DirectLiNGAM}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{lingam\PYGZus{}model1}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{residuals\PYGZus{}regime1}\PYG{p}{)}

\PYG{n}{lingam\PYGZus{}model2} \PYG{o}{=} \PYG{n}{lingam}\PYG{o}{.}\PYG{n}{DirectLiNGAM}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{lingam\PYGZus{}model2}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{residuals\PYGZus{}regime2}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}lingam.direct\PYGZus{}lingam.DirectLiNGAM at 0x1738aebd0\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
For the first regime, we now have

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{lingam\PYGZus{}model1}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} B0 regime 1}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x173da3b50\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
and for the second regime

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{make\PYGZus{}dot}\PYG{p}{(}\PYG{n}{lingam\PYGZus{}model2}\PYG{o}{.}\PYG{n}{adjacency\PYGZus{}matrix\PYGZus{}}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} B0 regime 2}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x173d8d890\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can clearly see how, taking into account the \sphinxstylestrong{policy change} we have been able to identify the \sphinxstylestrong{true causal structures}.

\sphinxstepscope


\part{III. Causal Inference}

\sphinxstepscope


\chapter{Overview}
\label{\detokenize{notebooks/preface_causal_inference:overview}}\label{\detokenize{notebooks/preface_causal_inference::doc}}
\sphinxAtStartPar
Causal inference focuses on estimating the strength and nature of identified causal relationships. This part uses statistical techniques to quantify the effect of one variable on another, given that the causal structure is known or assumed. If you do not know the causal graph, you can take a look at Part II about Causal Discovery to see methods to unveil the causal graph from observational data.

\sphinxAtStartPar
Knowing the causal graph is crucial because it helps identify the relationships and dependencies between variables. This knowledge allows for accurate estimation of causal effects, essential for making informed decisions and predictions. Here are some key reasons why understanding the causal graph is important:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Correct identification of relationships}: the causal graph helps in correctly identifying which variables directly affect the outcome of interest. This is important for estimating the true causal effect of a treatment variable.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Avoiding Bias}: without a proper causal graph, estimates of causal effects can be biased due to confounding variables. For instance, failing to account for confounders can lead to incorrect conclusions about the relationship between variables.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Understanding mediators and colliders}: the causal graph helps in identifying mediators (variables that lie on the causal path between the treatment and outcome) and colliders (variables influenced by two or more other variables). Misunderstanding these roles can lead to incorrect model specifications and biased estimates.

\end{itemize}

\sphinxAtStartPar
The following Chapters will try to present some key techniques to obtain precise and reliable estimates of causal effects by accounting for the presence of confounders and exogenous factors.


\section{Content of Causal Inference chapters}
\label{\detokenize{notebooks/preface_causal_inference:content-of-causal-inference-chapters}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Chapter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Instrumental Variables
&
\sphinxAtStartPar
How to estimate causal effects when we might suffer from omitted variable bias, or when the explanatory variable and response variable influence each other.
\\
\sphinxhline
\sphinxAtStartPar
Propensity Score Matching
&
\sphinxAtStartPar
How to control for confounders and avoid the selection bias.
\\
\sphinxhline
\sphinxAtStartPar
Double Machine Learning
&
\sphinxAtStartPar
How to estimate causal inference using machine learnign models and adjusting for the presence of a high\sphinxhyphen{}dimensional set of confounders.
\\
\sphinxhline
\sphinxAtStartPar
Difference\sphinxhyphen{}in\sphinxhyphen{}Differences
&
\sphinxAtStartPar
How to estimate the effect of a treatment on time series data by comparing changes in outcomes over time between treated and control groups.
\\
\sphinxhline
\sphinxAtStartPar
Interrupted Time Series
&
\sphinxAtStartPar
How to estimate treatment effects in time series without the presence of a control group.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{Instrumental Variables}
\label{\detokenize{notebooks/instrumental_variables:instrumental-variables}}\label{\detokenize{notebooks/instrumental_variables::doc}}
\sphinxAtStartPar
Instrumental variables (IV) refer to a method used in econometrics to estimate causal relationships when controlled experiments are not feasible and an explanatory variable is correlated with the error term, leading to \sphinxstylestrong{endogeneity issues}. Saying that a variable is correlated with the error term means that the variable and the error term share some common influence or that the variable captures some part of the variability in the dependent variable that should be attributed to the error term. When we say a variable is correlated with the error term, it means that both the variable and the error term are influenced by some common external factors. In other words, the variable captures some variability in the outcome that should be attributed to these external influences, which introduces bias in our estimates. This happens because the variable not only shows its direct effect on the outcome but also the effect of those omitted external factors.


\section{Endogeneity}
\label{\detokenize{notebooks/instrumental_variables:endogeneity}}
\sphinxAtStartPar
Endogeneity can occur due to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Omitted variable bias}: unobserved variables that affect both the explanatory variable and the outcome.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simultaneity}: when the explanatory variable and the outcome influence each other

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Measurement error}: Inaccuracies in measuring the explanatory variable.

\end{enumerate}

\sphinxAtStartPar
For instance, consider trying to understand how wind power production impacts wholesale electricity prices. However, if wind power production and prices are also affected by factors like solar power or broader economic trends, these external influences can bias our estimates. Here, we can use an IV, like wind speed forecasts, which affects wind power production but doesn’t directly influence electricity prices except through its effect on wind power. This helps us isolate the true effect of wind power production on prices, giving a clearer picture of the causal relationship.

\sphinxAtStartPar
An IV has two main properties:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Relevance}: the IV needs to be something that is related to the variable you’re interested in studying.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exogeneity}: the IV should not be influenced by other factors that affect the outcome you’re studying.

\end{enumerate}

\sphinxAtStartPar
Here is a representation of a case where a candidate IV (wind speed forecast) can help us isolate the effect of an explanatory variable (wind power production), on the response variable (price).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{graphviz}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}

\PYG{c+c1}{\PYGZsh{} Create a new graph}
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add nodes}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wind speed}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{forecasts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wind power}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{prices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unobserved}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{confounders}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add edges}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the graph in the notebook}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x107c77190\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Two\sphinxhyphen{}stage least squares (2SLS)}
\label{\detokenize{notebooks/instrumental_variables:two-stage-least-squares-2sls}}
\sphinxAtStartPar
Two\sphinxhyphen{}stage least squares (2SLS) is an econometric approach used to address endogeneity using IVs. It is performed in two steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Regress the endogenous explanatory variable on the IV to obtain predicted values.

\item {} 
\sphinxAtStartPar
Regress the dependent variable on the predicted values obtained from the first stage.

\end{enumerate}

\sphinxAtStartPar
The key idea is that if we use the IV (wind speed forecast) to predict the endogenous explanatory variable (wind power production), and then use these predicted values to estimate its effect on the response variable (price), we essentially use only the part of the variation in the explanatori variable that is “clean” of the confounding effects of the unobserved confounders (e.g., solar generation, macroeconomic trends). This allows us to estimate a causal effect that is not biased.

\sphinxAtStartPar
If we want to visualize this effect, this corresponds to removing the edge from the unobserved confounders to the endogenous explaantory variable.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create a new graph}
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add nodes}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Wind speed}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{forecasts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted wind }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{power production }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{using the IV}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{prices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Unobserved}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{confounders}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add edges}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Z}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{X}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{U}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Y}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the graph in the notebook}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x107cce3d0\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now generate some data to show how this works in practice. Since the confounders are, by definition, \sphinxstylestrong{unobserved}, we assume to only have at our disposal the wind speed forecast, the wind power production, and the electricity prices.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{true\PYGZus{}effect} \PYG{o}{=} \PYG{l+m+mi}{2}  \PYG{c+c1}{\PYGZsh{} True causal effect of wind power production on electricity prices}
\PYG{n}{wind\PYGZus{}speed\PYGZus{}forecasts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} IV}
\PYG{n}{unobserved\PYGZus{}confounders} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Unobserved factors with variance 10}
\PYG{n}{wind\PYGZus{}power\PYGZus{}production} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{wind\PYGZus{}speed\PYGZus{}forecasts} \PYG{o}{+} \PYG{n}{unobserved\PYGZus{}confounders} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\PYG{n}{electricity\PYGZus{}prices} \PYG{o}{=} \PYG{n}{true\PYGZus{}effect} \PYG{o}{*} \PYG{n}{wind\PYGZus{}power\PYGZus{}production} \PYG{o}{+} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{unobserved\PYGZus{}confounders} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WindSpeedForecasts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{wind\PYGZus{}speed\PYGZus{}forecasts}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WindPowerProduction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{wind\PYGZus{}power\PYGZus{}production}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ElectricityPrices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{electricity\PYGZus{}prices}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   WindSpeedForecasts  WindPowerProduction  ElectricityPrices
0            3.745401             6.646425          15.010277
1            9.507143            14.708451          22.433042
2            7.319939            14.337450          29.275135
3            5.986585            14.664067          31.182353
4            1.560186             4.973025          12.114167
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Ignoring confounders}
\label{\detokenize{notebooks/instrumental_variables:ignoring-confounders}}
\sphinxAtStartPar
If we simply ignore the problem of having unobserved confounders, we might be tempted to simply fit a model on the available data. Let’s try that.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Perform OLS regression}
\PYG{n}{X\PYGZus{}ols} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WindPowerProduction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ols\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ElectricityPrices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}ols}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{OLS Regression Results:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{ols\PYGZus{}model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
OLS Regression Results:
                            OLS Regression Results                            
==============================================================================
Dep. Variable:      ElectricityPrices   R\PYGZhy{}squared:                       0.925
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.925
Method:                 Least Squares   F\PYGZhy{}statistic:                 1.225e+04
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):               0.00
Time:                        19:10:43   Log\PYGZhy{}Likelihood:                \PYGZhy{}2886.7
No. Observations:                1000   AIC:                             5777.
Df Residuals:                     998   BIC:                             5787.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=======================================================================================
                          coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const                  \PYGZhy{}2.5490      0.251    \PYGZhy{}10.149      0.000      \PYGZhy{}3.042      \PYGZhy{}2.056
WindPowerProduction     2.2967      0.021    110.677      0.000       2.256       2.337
==============================================================================
Omnibus:                        0.663   Durbin\PYGZhy{}Watson:                   1.919
Prob(Omnibus):                  0.718   Jarque\PYGZhy{}Bera (JB):                0.609
Skew:                          \PYGZhy{}0.059   Prob(JB):                        0.737
Kurtosis:                       3.022   Cond. No.                         22.2
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{2SLS}
\label{\detokenize{notebooks/instrumental_variables:sls}}
\sphinxAtStartPar
Now, we use the 2SLS approach and fit two models, the first one to predit the explanatory variable and the second one using the predicted variable to predict the response.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} First stage of 2SLS: Regress WindPowerProduction on WindSpeedForecasts}
\PYG{n}{X\PYGZus{}first\PYGZus{}stage} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WindSpeedForecasts}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{first\PYGZus{}stage\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WindPowerProduction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}first\PYGZus{}stage}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FittedWindPowerProduction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{first\PYGZus{}stage\PYGZus{}model}\PYG{o}{.}\PYG{n}{fittedvalues}

\PYG{c+c1}{\PYGZsh{} Second stage of 2SLS: Regress ElectricityPrices on the fitted values from the first stage}
\PYG{n}{X\PYGZus{}second\PYGZus{}stage} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{FittedWindPowerProduction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{second\PYGZus{}stage\PYGZus{}model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ElectricityPrices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X\PYGZus{}second\PYGZus{}stage}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s2}{2SLS Regression Results:}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{second\PYGZus{}stage\PYGZus{}model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
2SLS Regression Results:
                            OLS Regression Results                            
==============================================================================
Dep. Variable:      ElectricityPrices   R\PYGZhy{}squared:                       0.509
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.508
Method:                 Least Squares   F\PYGZhy{}statistic:                     1034.
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):          3.21e\PYGZhy{}156
Time:                        19:10:44   Log\PYGZhy{}Likelihood:                \PYGZhy{}3824.2
No. Observations:                1000   AIC:                             7652.
Df Residuals:                     998   BIC:                             7662.
Df Model:                           1                                         
Covariance Type:            nonrobust                                         
=============================================================================================
                                coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const                         0.9002      0.709      1.269      0.205      \PYGZhy{}0.492       2.292
FittedWindPowerProduction     1.9563      0.061     32.149      0.000       1.837       2.076
==============================================================================
Omnibus:                        0.082   Durbin\PYGZhy{}Watson:                   2.015
Prob(Omnibus):                  0.960   Jarque\PYGZhy{}Bera (JB):                0.119
Skew:                          \PYGZhy{}0.020   Prob(JB):                        0.942
Kurtosis:                       2.964   Cond. No.                         23.7
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now \sphinxstylestrong{compare the results} obtained with the naive approach, ignoring the presence of confounders, and the 2SLS modelling.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Extract coefficients and standard errors}
\PYG{n}{ols\PYGZus{}coef} \PYG{o}{=} \PYG{n}{ols\PYGZus{}model}\PYG{o}{.}\PYG{n}{params}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{ols\PYGZus{}se} \PYG{o}{=} \PYG{n}{ols\PYGZus{}model}\PYG{o}{.}\PYG{n}{bse}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{second\PYGZus{}stage\PYGZus{}coef} \PYG{o}{=} \PYG{n}{second\PYGZus{}stage\PYGZus{}model}\PYG{o}{.}\PYG{n}{params}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{second\PYGZus{}stage\PYGZus{}se} \PYG{o}{=} \PYG{n}{second\PYGZus{}stage\PYGZus{}model}\PYG{o}{.}\PYG{n}{bse}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Plot the comparison}
\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{OLS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2SLS}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{coefficients} \PYG{o}{=} \PYG{p}{[}\PYG{n}{ols\PYGZus{}coef}\PYG{p}{,} \PYG{n}{second\PYGZus{}stage\PYGZus{}coef}\PYG{p}{]}
\PYG{n}{errors} \PYG{o}{=} \PYG{p}{[}\PYG{n}{ols\PYGZus{}se}\PYG{p}{,} \PYG{n}{second\PYGZus{}stage\PYGZus{}se}\PYG{p}{]}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{,} \PYG{n}{coefficients}\PYG{p}{,} \PYG{n}{yerr}\PYG{o}{=}\PYG{n}{errors}\PYG{p}{,} \PYG{n}{capsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{n}{true\PYGZus{}effect}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.8}\PYG{p}{,}  \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True effect}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coefficient of Wind Power Production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Comparison of OLS and 2SLS Estimates}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{570bd8b176dcd75b03484874379faa241fe04de035b99aa3b1e3908f1f60b3e7}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now see tha simply using a model on all the observed variables leads us to a \sphinxstylestrong{biased estimate}. Instead, using 2SLS we can get much closer to the real effect.

\sphinxstepscope


\chapter{Propensity Score Matching}
\label{\detokenize{notebooks/propensity_scores:propensity-score-matching}}\label{\detokenize{notebooks/propensity_scores::doc}}
\sphinxAtStartPar
In many cases, we are interested in assessing the effectiveness of a campaign or other types of treatments. For example, let’s assume we are interested in evaluating the impact of a \sphinxstylestrong{demand\sphinxhyphen{}response programme} on the electricity consumption of a set of households. Suppose that households can join the demand\sphinxhyphen{}response programme by filling out a specific form. For those who join, the programme offers reduced pricing during off\sphinxhyphen{}peak hours. Our goal is to assess the effectiveness of this programme by comparing the electricity consumption of people who joined the programme (our \sphinxstylestrong{treatment group}) to those who did not (our \sphinxstylestrong{control group}).

\sphinxAtStartPar
In an ideal scenario, if we were able to conduct a completely randomised experiment, we would have a balanced and diverse set of customers in both groups. In such a situation, if the possibility of joining the programme was as random as flipping a coin, any significant differences in consumption patterns between the two groups could be attributed to the special pricing offered by the programme.

\sphinxAtStartPar
However, in real life, things are more complicated. Because people are not randomly assigned to the two groups, there may be characteristics of the people who voluntarily joined the programme that are the true causes of changes in consumption. For example, if a majority of the people who agreed to join the programme are retired, it is likely that they will use electricity during off\sphinxhyphen{}peak hours due to their lifestyle and not solely because of the reduced pricing. This is an example of \sphinxstylestrong{selection bias}.

\sphinxAtStartPar
Whenever the treatment and control assignment is not random, there may be demographic or personal factors that affect the outcome of the experiment. These factors are referred to as \sphinxstylestrong{confounders}. This problem is particularly prevalent in observational data (see {[}\hyperlink{cite.bibliography:id12}{RR83}{]}).

\sphinxAtStartPar
\sphinxstylestrong{Propensity score matching (PSM)} is a statistical technique used to estimate the effect of a treatment by accounting for covariates that predict receiving the treatment. In our case, this means we can try to estimate the effect of the demand\sphinxhyphen{}response programme by considering the covariates that might have influenced who joined the programme. Essentially, if we can estimate the probability of someone joining the programme based on demographic factors, we can \sphinxstylestrong{match} treated and untreated subjects with similar propensity scores. By comparing these matched groups, we can compute an adjusted difference that helps mimic a randomised experimental design, reducing selection bias and allowing for a more accurate estimation of the treatment effect.

\sphinxAtStartPar
In mathematical terms, we can define the propensity score \(e(x)\) as the probability of a unit (e.g., customer) joining the demand\sphinxhyphen{}response programme given a set of covariates \(x\):
\label{equation:notebooks/propensity_scores:526a72b1-7586-428a-9372-76ab6efaac9c}\begin{equation}
   e(x) = P(T = 1 \mid X = x)
\end{equation}
\sphinxAtStartPar
where \(T\) is a binary indicator of treatment assignment (1 if joined the programme, 0 if not).

\sphinxAtStartPar
Propensity scores are typically estimated using \sphinxstylestrong{logistic regression}, although other classification methods can be used. For example:
\label{equation:notebooks/propensity_scores:2a8a5535-239a-4f10-8b1d-dd46d4999c92}\begin{equation}
   \hat{e}(x) = \hat{P}(T = 1 \mid X = x) = \frac{1}{1 + \exp(-(\beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_p x_p))}
\end{equation}
\sphinxAtStartPar
After estimating the propensity scores, we can proceed with the matching process. There are several ways to perform matching, such as:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Nearest\sphinxhyphen{}neighbor matching}: each treated unit is matched with the control unit that has the closest propensity score.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Caliper matching}: similar to nearest\sphinxhyphen{}neighbor matching but with a tolerance level (caliper) for how close the propensity scores must be.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stratification (or interval matching)}: units are divided into strata based on their propensity scores, and comparisons are made within each stratum.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Kernel matching}: Weights all control units according to their distance from the treated unit’s propensity score.

\end{enumerate}


\section{Example}
\label{\detokenize{notebooks/propensity_scores:example}}
\sphinxAtStartPar
Let’s walk through an example using Python to illustrate how to implement propensity score matching, estimate the treatment effect, and visualize the results.


\subsection{Data generation}
\label{\detokenize{notebooks/propensity_scores:data-generation}}
\sphinxAtStartPar
We will use a synthetic dataset to simulate our scenario. Suppose our dataset includes the following columns:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{joined\_program}}: A binary indicator (1 if joined the program, 0 if not).

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{peak\_consumption}}: The electricity consumption of the household during peak hours.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{age}}: Age of the household head.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{income}}: Income level of the household.

\item {} 
\sphinxAtStartPar
\sphinxcode{\sphinxupquote{household\_size}}: Number of members in the household.

\end{itemize}

\sphinxAtStartPar
We also introduce some \sphinxstylestrong{bias in the treatment assignment} to simulate a real\sphinxhyphen{}world scenario where older people are more likely to join the programmw and lower\sphinxhyphen{}income households are less likely to join.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LogisticRegression}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{neighbors} \PYG{k+kn}{import} \PYG{n}{NearestNeighbors}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{age} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
\PYG{n}{income} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{50000}\PYG{p}{,} \PYG{l+m+mi}{15000}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}
\PYG{n}{household\PYGZus{}size} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Bias in treatment assignment}
\PYG{n}{noise} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
\PYG{n}{prob\PYGZus{}join} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{p}{(}\PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{age} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.0001} \PYG{o}{*} \PYG{n}{income} \PYG{o}{+} \PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{n}{household\PYGZus{}size} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{5} \PYG{o}{+} \PYG{n}{noise}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{joined\PYGZus{}program} \PYG{o}{=} \PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{prob\PYGZus{}join}\PYG{p}{)}\PYG{o}{.}\PYG{n}{astype}\PYG{p}{(}\PYG{n+nb}{int}\PYG{p}{)}

\PYG{n}{peak\PYGZus{}consumption} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mi}{200} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.001} \PYG{o}{*} \PYG{n}{income} \PYG{o}{+} \PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{n}{household\PYGZus{}size} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{joined\PYGZus{}program} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{joined\PYGZus{}program}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{peak\PYGZus{}consumption}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{age}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{income}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{household\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{household\PYGZus{}size}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}


\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   joined\PYGZus{}program  peak\PYGZus{}consumption  age  income  household\PYGZus{}size
0               1        140.595562   71   58339               3
1               1        134.926546   54   63387               2
2               1        154.885678   61   43665               4
3               1        147.252513   76   51570               1
4               1        146.337150   72   53420               1
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Propensity scores estimation}
\label{\detokenize{notebooks/propensity_scores:propensity-scores-estimation}}
\sphinxAtStartPar
In this step, we use logistic regression to estimate the propensity scores, which represent the probability of joining the programme given the covariates (age, income, and household size).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{covariates} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{income}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{household\PYGZus{}size}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{covariates}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{n}{log\PYGZus{}reg} \PYG{o}{=} \PYG{n}{LogisticRegression}\PYG{p}{(}\PYG{n}{max\PYGZus{}iter}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}
\PYG{n}{log\PYGZus{}reg}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{log\PYGZus{}reg}\PYG{o}{.}\PYG{n}{predict\PYGZus{}proba}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   joined\PYGZus{}program  peak\PYGZus{}consumption  age  income  household\PYGZus{}size  \PYGZbs{}
0               1        140.595562   71   58339               3   
1               1        134.926546   54   63387               2   
2               1        154.885678   61   43665               4   
3               1        147.252513   76   51570               1   
4               1        146.337150   72   53420               1   

   propensity\PYGZus{}score  
0          0.994963  
1          0.959616  
2          0.988158  
3          0.997573  
4          0.995956  
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{kdeplot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Treated}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{kdeplot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Control}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Propensity Scores}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Propensity Score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{70ee292f4689c317c326387335cf89744dbab17339be49f7702fce6f013cdb4e}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
In this plot, the extent to which the treated and control \sphinxstylestrong{propensity score distributions overlap} is crucial:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Significant overlap indicates that for many households, there are comparable counterparts in both groups, which is good for matching.

\item {} 
\sphinxAtStartPar
Minimal overlap suggests that there are many treated households with no comparable controls (or vice versa), which can be problematic for matching.

\end{itemize}

\sphinxAtStartPar
Before matching, it is common to see differences in the distributions, especially if certain covariates strongly influence whether households join the program. The initial (pre\sphinxhyphen{}matching) plot might show that the treated group has higher propensity scores on average, indicating that those who joined the program had characteristics that made them more likely to join. This bias is what matching aims to adjust for by finding control units with similar propensity scores.


\subsection{Nearest\sphinxhyphen{}neighbor matching}
\label{\detokenize{notebooks/propensity_scores:nearest-neighbor-matching}}
\sphinxAtStartPar
We separate the treated (joined the program) and control (did not join the program) groups. Using the nearest\sphinxhyphen{}neighbor matching technique, we match each treated unit with the control unit that has the closest propensity score. The goal here is to match each treated unit (household that joined the programme) with one or more control units (households that did not join the programme) that have similar propensity scores. Here’s a detailed explanation of how we can do it:
\begin{itemize}
\item {} 
\sphinxAtStartPar
We separate the treated and control groups based on whether they joined the program or not.

\item {} 
\sphinxAtStartPar
We find the closest control units for each treated unit based on their propensity scores.

\item {} 
\sphinxAtStartPar
We combine the treated units and their matched control units into a single dataset for further analysis.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Step 3: Perform Matching}
\PYG{n}{treated} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{n}{control} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{]}

\PYG{n}{nbrs} \PYG{o}{=} \PYG{n}{NearestNeighbors}\PYG{p}{(}\PYG{n}{n\PYGZus{}neighbors}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{algorithm}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ball\PYGZus{}tree}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{control}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{distances}\PYG{p}{,} \PYG{n}{indices} \PYG{o}{=} \PYG{n}{nbrs}\PYG{o}{.}\PYG{n}{kneighbors}\PYG{p}{(}\PYG{n}{treated}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get the matched control units}
\PYG{n}{matched\PYGZus{}control\PYGZus{}indices} \PYG{o}{=} \PYG{n}{indices}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{matched\PYGZus{}control} \PYG{o}{=} \PYG{n}{control}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{matched\PYGZus{}control\PYGZus{}indices}\PYG{p}{]}

\PYG{n}{matched\PYGZus{}pairs} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{concat}\PYG{p}{(}\PYG{p}{[}\PYG{n}{treated}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{,} \PYG{n}{matched\PYGZus{}control}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{n}{drop}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{matched\PYGZus{}pairs}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n}{col} \PYG{k}{if} \PYG{n}{i} \PYG{o}{\PYGZlt{}} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{treated}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c\PYGZus{}}\PYG{l+s+s1}{\PYGZsq{}} \PYG{o}{+} \PYG{n}{col} \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{col} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{matched\PYGZus{}pairs}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Distribution of propensity scores after matching}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{kdeplot}\PYG{p}{(}\PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t\PYGZus{}propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Treated}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{kdeplot}\PYG{p}{(}\PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c\PYGZus{}propensity\PYGZus{}score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Control}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fill}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Distribution of Propensity Scores After Matching}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Propensity Score}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Density}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{7b74fc02afddd9fb548968c2b6099171c55d21c5f7c4013c7e8163d2b469dae5}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we see here, after matching, we ideally want the distributions to overlap substantially, indicating that the treated and control groups are similar with respect to the covariates used to estimate the propensity scores.


\subsection{Treatment effect estimation}
\label{\detokenize{notebooks/propensity_scores:treatment-effect-estimation}}
\sphinxAtStartPar
We calculate the Average Treatment Effect on the Treated (ATT) by comparing the average electricity consumption of the treated group to that of the matched control group.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{treatment\PYGZus{}effect} \PYG{o}{=} \PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t\PYGZus{}peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c\PYGZus{}peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Estimated Treatment Effect: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{treatment\PYGZus{}effect}\PYG{l+s+si}{:}\PYG{l+s+s1}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{ KWh}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Estimated Treatment Effect: \PYGZhy{}2.583 KWh
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{Visualising results}
\label{\detokenize{notebooks/propensity_scores:visualising-results}}
\sphinxAtStartPar
We can create some visualisations to compare the distributions of propensity scores and electricity consumption before and after matching.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Electricity consumption before and after matching}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Set3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{showfliers}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Peak Consumption for Program vs Non\PYGZhy{}Program (Original Data)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Joined Program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Consumption (kWh)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{e299b57075027bf1f9f9912428307fe88f501f4a40cb2c0bf59f92fbea0d3eee}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how, looking at the original data, we would get the impression that people who joined the programme consume more lectricity during peak hours. The situation is much different when we look at the consumptions for the matched groups, where we see that joining the programme actually has a positive effect in reducing electricity consumption during peak hours.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Violin plot of consumption for matched data}
\PYG{c+c1}{\PYGZsh{} Boxplot of consumption for matched data}
\PYG{n}{matched\PYGZus{}data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{matched\PYGZus{}pairs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{matched\PYGZus{}pairs}\PYG{p}{)}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{t\PYGZus{}peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{matched\PYGZus{}pairs}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c\PYGZus{}peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{boxplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{peak\PYGZus{}consumption}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{matched\PYGZus{}data}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Set3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{joined\PYGZus{}program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{showfliers}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Peak Consumption for Program vs Non\PYGZhy{}Program (Matched Data)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Non\PYGZhy{}Program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Program}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{b7390f66687a907dc2562ca2700dea622431d21cb14e6a6c218a06990be1a56d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This example demonstrates the basic steps of performing propensity score matching and evaluating the treatment effect. By matching treated and control units with similar propensity scores, we reduce the bias due to confounding variables and can make a more accurate assessment of the treatment’s impact.

\sphinxstepscope


\chapter{Double Machine Learning}
\label{\detokenize{notebooks/double_machine_learning:double-machine-learning}}\label{\detokenize{notebooks/double_machine_learning::doc}}
\sphinxAtStartPar
We have already seen that we can use Propensity Score Matching (PSM) to adjust for confounders when estimating treatment effects. In high\sphinxhyphen{}dimensional settings or when using machine learning model, things can get more complicated. For these circumstances, Chernozhukov et al. {[}\hyperlink{cite.bibliography:id9}{CCD+18}{]} proposed the Double Machine Learning (DML) is a framework, to estimate causal effects when many confounding variables are present. It combines machine learning techniques with econometric methods to control for these confounders and obtain unbiased estimates of treatment effects. For example, let’s assume we are interested in unveiling the effect of wind power (WP) production or solar power (SP) production on electricity prices. In this case, WP and SP production would be our \sphinxstylestrong{treatment variables}, while the electricity price would be our \sphinxstylestrong{response}. We know that these factors have an effect in reducing prices due to their low marginal costs. However, we also know that there are many other factors affecting electricity prices (e.g., demand, gas prices, macroeconomic trends). These are the \sphinxstylestrong{confounders}. We can also assume that some these confounders have an effect both on our treatment and our response variable. For example, the season we are in or the specific hour of the day will certainly affect the generation of SP, but will also have an effect on the demand (hence, the prices) because of the well\sphinxhyphen{}known daily and season consumption profiles. Ignoring the effect of these confounders might lead to biased estimates.

\sphinxAtStartPar
With DML, we are trying to isolate the effect of the treatment variables on the response. This framework assumes that the response \(y\) (e.g., the prices) is a function of the treatment \(w\) and other confounding variables \(x\):
\label{equation:notebooks/double_machine_learning:7c01253e-1579-4ecb-b075-5e6ef60a5dd5}\begin{equation}
    y = g(w, x) + \epsilon
\end{equation}
\sphinxAtStartPar
where \(g\) is an arbitrary function (linear or nonlinear) and \(\epsilon\) is the error term.

\sphinxAtStartPar
Similarly, since we assumed that the treatment is also affected by other confounding variables, we have that \(w\) can be modeled as a function of \(x\):
\label{equation:notebooks/double_machine_learning:5af1f767-39ea-4da4-a91e-f388babc1dcc}\begin{equation}
    w = m(x) + \nu
\end{equation}
\sphinxAtStartPar
where \(m\) is an arbitrary function (linear or nonlinear) and \(\nu\) is the error term.

\sphinxAtStartPar
Now, the DML framework involves two main stages:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Nuisance parameter estimation}: use a machine learning model to estimate the functions \(\hat{g}(w, x)\) and \(\hat{m}(x)\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Orthogonalization and estimation}: use the estimated functions to “remove” the effect of the confounding variables from both \(w\) and \(y\). Then, we estimate the causal effects by regressing the residuals of the response on the residuals of the treatment.

\end{enumerate}

\sphinxAtStartPar
The \sphinxstylestrong{key intuition} is that if we remove the effect of other confounders from the tratment and the response, the variation that remains in the residuals is only due to the treatment itself. It should be noted that this approach assumes we already know the causal graph, and that there are no omitted variables.


\section{The Partially Linear Case}
\label{\detokenize{notebooks/double_machine_learning:the-partially-linear-case}}
\sphinxAtStartPar
For simplicity, we now consider a partially linear case where the relationship between the outcome \(y\) and the treatment \(w\) can be expressed linearly, while allowing for a potentially complex, nonlinear relationship between the confounders \(x\) and both the treatment and outcome.

\sphinxAtStartPar
In this case, the model is specified as follows:
\label{equation:notebooks/double_machine_learning:1d36afac-24df-4f1e-839c-d0d6749c1426}\begin{equation}
    y = \beta w + g(x) + \epsilon
\end{equation}\label{equation:notebooks/double_machine_learning:fa2d4195-8c89-4613-b65c-3fa346b2e4e3}\begin{equation}
    w = m(x) + \nu
\end{equation}
\sphinxAtStartPar
Here:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta\) is the coefficient capturing the causal effect of the treatment \(w\) on the outcome \(y\). This is what we are trying to estimate!

\item {} 
\sphinxAtStartPar
\(g(x)\) is an unknown function capturing the effect of the confounders \(x\) on the outcome.

\item {} 
\sphinxAtStartPar
\(m(x)\) is an unknown function capturing the effect of the confounders \(x\) on the treatment.

\end{itemize}

\sphinxAtStartPar
The \sphinxstylestrong{key steps} to implement the DML  in the partially linear case are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Split the data}: randomly split the data into \(K\) folds.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Train predictive models}: for each fold \(k\) (where \(k \in \{1, 2, ..., K\}\)):
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Treatment model}: train a machine learning model \(\hat{m}_{-k}(x)\) using \(K-1\) folds to predict \(w\) from \(x\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Outcome model}: train a machine learning model \(\hat{g}_{-k}(x)\) using \(K-1\) folds to predict \(y\) from \(x\).

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generate residuals}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Use the models trained on \(K-1\) folds to predict the held\sphinxhyphen{}out fold \(k\).

\item {} 
\sphinxAtStartPar
Compute the residuals for the treatment and outcome models:
\label{equation:notebooks/double_machine_learning:4c0cfa5b-0da1-4f47-b579-2a88d71f0e39}\begin{equation}
            \hat{V}_W = W - \hat{W}, \quad \hat{V}_Y = Y - \hat{Y}
        \end{equation}
\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regress residuals}: regress the residualized outcome \(\hat{V}_Y\) on the residualized treatment \(\hat{V}_W\) to estimate the causal effect \(\beta\):
\textbackslash{}begin\{equation\}
\textbackslash{}hat\{\textbackslash{}beta\}\_k = \textbackslash{}text\{coef\}\textbackslash{}left( \textbackslash{}hat\{V\}\_Y \textbackslash{}sim \textbackslash{}hat\{V\}\_W \textbackslash{}right)
\textbackslash{}end\{equation\}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Average estimates}: repeat steps 2\sphinxhyphen{}4 for each fold and average the resulting \(K\) estimates to obtain the final causal estimate:
\textbackslash{}begin\{equation\}
\textbackslash{}hat\{\textbackslash{}beta\} = \textbackslash{}frac\{1\}\{K\} \textbackslash{}sum\_\{k=1\}\textasciicircum{}\{K\} \textbackslash{}hat\{\textbackslash{}beta\}\_k
\textbackslash{}end\{equation\}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Robustness}: for more robustness with respect to random partitioning in finite samples, repeat the algorithm multiple times (e.g., 100 times) with different random splits and report the median estimate.

\end{enumerate}

\sphinxAtStartPar
This algorithm ensures that the estimation of the treatment effect is orthogonal to the nuisance parameters (the confounders), thereby removing bias due to overfitting and ensuring that the estimated treatment effect is unbiased.


\section{Electricity example}
\label{\detokenize{notebooks/double_machine_learning:electricity-example}}
\sphinxAtStartPar
Let’s now consider a practical case where we want to estimate the effect of day\sphinxhyphen{}ahead wind power and solar power forecasts on spot prices. We know that the forecasts available before gate closure {[}\hyperlink{cite.bibliography:id11}{JonssonPM10}{]} are crucial to determine the equilibrium price. Because of the merit\sphinxhyphen{}order effect, the low marginal costs of renewable energy sources acts like a shifter in reducing the equilibrium price. We now assume to be in a simplified setting where we want to quantify this effect. We consider a simple \sphinxstylestrong{causal graph} where the spot prices are linearly affected by the forecasted WP and SP production levels, and nonlinearly affected by other endogenous factors and market conditions. In particular, we assume to be in the following \sphinxstylestrong{partially linear case}:
\label{equation:notebooks/double_machine_learning:0cc2acbd-09fa-42ea-b723-7009768a1e2e}\begin{equation}
    \text{spot price} = -0.3 * \text{WP} - 0,3 * \text{SP} + f_L(\text{load}) + f_G(\text{gas}) + f_C(\text{coal}) + f_{CO2} (\text{carbon pricing}) + \epsilon
\end{equation}
\sphinxAtStartPar
where \(f_L, \ldots, f_{CO2}\) are arbitrary nonlinear functions, and \(\epsilon\) is the error term.

\sphinxAtStartPar
Our goal is to estimate the coefficients \sphinxhyphen{}0.3 as accurately as possible, having collected data from the following causal graph.


\subsection{The causal graph}
\label{\detokenize{notebooks/double_machine_learning:the-causal-graph}}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{graphviz}
\PYG{k+kn}{from} \PYG{n+nn}{IPython}\PYG{n+nn}{.}\PYG{n+nn}{display} \PYG{k+kn}{import} \PYG{n}{display}

\PYG{c+c1}{\PYGZsh{} Create a new graph}
\PYG{n}{dot} \PYG{o}{=} \PYG{n}{graphviz}\PYG{o}{.}\PYG{n}{Digraph}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add nodes}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Daylight hours }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{(proxy for season)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecasted }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{WP }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{penwidth}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Forecasted }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{SP }\PYG{l+s+se}{\PYGZbs{}n}\PYG{l+s+s1}{production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{penwidth}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Load forecast}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{G}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Gas price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coal price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CO2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Carbon pricing}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{dot}\PYG{o}{.}\PYG{n}{node}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day\PYGZhy{}ahead spot prices}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{penwidth}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define coefficients for the edges}
\PYG{n}{coefficients} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{S}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}0.3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}0.3}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{L}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{G}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{CO2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,}
    \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{C}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZsq{}}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Add edges with coefficients as labels and make only WP\PYGZhy{}\PYGZgt{}P and SP\PYGZhy{}\PYGZgt{}P green}
\PYG{k}{for} \PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{)}\PYG{p}{,} \PYG{n}{coeff} \PYG{o+ow}{in} \PYG{n}{coefficients}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{edge\PYGZus{}color} \PYG{o}{=} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{)} \PYG{o+ow}{in} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gray}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{dot}\PYG{o}{.}\PYG{n}{edge}\PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{coeff}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{n}{edge\PYGZus{}color}\PYG{p}{,} \PYG{n}{penwidth}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{p}{(}\PYG{n}{start}\PYG{p}{,} \PYG{n}{end}\PYG{p}{)} \PYG{o+ow}{in} \PYG{p}{[}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,} \PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{]} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}


\PYG{c+c1}{\PYGZsh{} Display the graph in the notebook}
\PYG{n}{display}\PYG{p}{(}\PYG{n}{dot}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYGZlt{}graphviz.graphs.Digraph at 0x10845a290\PYGZgt{}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsection{The data\sphinxhyphen{}generating process}
\label{\detokenize{notebooks/double_machine_learning:the-data-generating-process}}
\sphinxAtStartPar
We can now create a simple simulator for this kind of data, and plot the results of a simulation run, to show the data:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{k+kn}{from} \PYG{n+nn}{pygam} \PYG{k+kn}{import} \PYG{n}{LinearGAM}\PYG{p}{,} \PYG{n}{s}\PYG{p}{,} \PYG{n}{f}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{model\PYGZus{}selection} \PYG{k+kn}{import} \PYG{n}{KFold}
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{ndimage} \PYG{k+kn}{import} \PYG{n}{gaussian\PYGZus{}filter}
\PYG{k+kn}{from} \PYG{n+nn}{tqdm} \PYG{k+kn}{import} \PYG{n}{tqdm}

\PYG{k}{def} \PYG{n+nf}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}years}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{random\PYGZus{}seed}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{plot\PYGZus{}time\PYGZus{}series}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}\PYG{p}{:}
    \PYG{k}{if} \PYG{n}{random\PYGZus{}seed} \PYG{o+ow}{is} \PYG{o+ow}{not} \PYG{k+kc}{None}\PYG{p}{:}
        \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{n}{random\PYGZus{}seed}\PYG{p}{)}

    \PYG{n}{hours\PYGZus{}per\PYGZus{}day} \PYG{o}{=} \PYG{l+m+mi}{24}
    \PYG{n}{days\PYGZus{}per\PYGZus{}year} \PYG{o}{=} \PYG{l+m+mi}{365}
    \PYG{n}{n} \PYG{o}{=} \PYG{n}{hours\PYGZus{}per\PYGZus{}day} \PYG{o}{*} \PYG{n}{days\PYGZus{}per\PYGZus{}year} \PYG{o}{*} \PYG{n}{n\PYGZus{}years}

    \PYG{n}{date\PYGZus{}range} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{date\PYGZus{}range}\PYG{p}{(}\PYG{n}{start}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2020\PYGZhy{}01\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{periods}\PYG{o}{=}\PYG{n}{n}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{h}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{time} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n}\PYG{p}{)}
    \PYG{n}{day\PYGZus{}of\PYGZus{}year} \PYG{o}{=} \PYG{p}{(}\PYG{n}{time} \PYG{o}{/}\PYG{o}{/} \PYG{n}{hours\PYGZus{}per\PYGZus{}day}\PYG{p}{)} \PYG{o}{\PYGZpc{}} \PYG{n}{days\PYGZus{}per\PYGZus{}year}
    \PYG{n}{D} \PYG{o}{=} \PYG{l+m+mi}{8} \PYG{o}{+} \PYG{l+m+mi}{4} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{*} \PYG{p}{(}\PYG{n}{day\PYGZus{}of\PYGZus{}year} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{80}\PYG{p}{)} \PYG{o}{/} \PYG{n}{days\PYGZus{}per\PYGZus{}year}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{4}

    \PYG{n}{WP} \PYG{o}{=} \PYG{l+m+mi}{60} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1} \PYG{o}{*} \PYG{n}{D} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
    \PYG{n}{SP} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{D} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
    \PYG{n}{L} \PYG{o}{=} \PYG{l+m+mi}{200} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{4} \PYG{o}{*} \PYG{n}{D} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
    \PYG{n}{G} \PYG{o}{=} \PYG{l+m+mi}{150} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{G} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{l+m+mi}{50}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
    \PYG{n}{C} \PYG{o}{=} \PYG{l+m+mi}{80} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{G} \PYG{o}{+} \PYG{l+m+mf}{0.05} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{C} \PYG{o}{=} \PYG{n}{gaussian\PYGZus{}filter}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{maximum}\PYG{p}{(}\PYG{n}{C}\PYG{p}{,} \PYG{l+m+mi}{40}\PYG{p}{)}\PYG{p}{,} \PYG{n}{sigma}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}
    \PYG{n}{O} \PYG{o}{=} \PYG{l+m+mf}{0.7} \PYG{o}{*} \PYG{n}{G} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}\PYG{p}{)}
    
    \PYG{n}{noiseless\PYGZus{}P} \PYG{o}{=} \PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3} \PYG{o}{*} \PYG{n}{WP} \PYG{o}{\PYGZhy{}} \PYG{l+m+mf}{0.3} \PYG{o}{*} \PYG{n}{SP}  \PYG{o}{+} \PYG{p}{(}\PYG{n}{L}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{5}\PYG{p}{)}\PYG{o}{/}\PYG{l+m+mf}{10e9} \PYG{o}{+}
                   \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{50} \PYG{o}{/} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{exp}\PYG{p}{(}\PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{G} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{100}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{20} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{C}\PYG{o}{/}\PYG{l+m+mi}{5}\PYG{p}{)} \PYG{o}{+} \PYG{l+m+mi}{10} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{O} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{P} \PYG{o}{=} \PYG{l+m+mi}{50} \PYG{o}{+} \PYG{n}{noiseless\PYGZus{}P} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{.1}\PYG{p}{,} \PYG{n}{n}\PYG{p}{)}

    \PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{D}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{WP}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{solar\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{SP}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{estimated\PYGZus{}load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{L}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gas\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{G}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{carbon\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{C}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coal\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{O}\PYG{p}{,}
        \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spot\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{P}
    \PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{date\PYGZus{}range}\PYG{p}{)}

    \PYG{k}{if} \PYG{n}{plot\PYGZus{}time\PYGZus{}series}\PYG{p}{:}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{12}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
        \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{var} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{data}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{i}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{n}{var}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{n}{var}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Date}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
            \PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{rotation}\PYG{o}{=}\PYG{l+m+mi}{45}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{data}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}years}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{random\PYGZus{}seed}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{plot\PYGZus{}time\PYGZus{}series}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{7734316a2fe3a58414d9fe38e864ff7a823dc33b788c86e2e54259f05d65680c}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that the simulator simply tries to emulate the seasonality in load and renewables production, with higher demands in colder months and higher SP production during summer months.


\subsection{The DML algorithms}
\label{\detokenize{notebooks/double_machine_learning:the-dml-algorithms}}
\sphinxAtStartPar
We can now implement the DML framework we have seen before, where we try to remove the effect of confounders by fitting a nonlinear model, in this case an generalized additive model (GAM) on the confounders. Here, we are in a peculiar DML setting, where the confounders affecting the treatment variables and the response variable are not the same. Indeed, we have:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Confounder for the treatments: daylight hours.

\item {} 
\sphinxAtStartPar
Confounders for the response: daylight hours, estimated load, gas price, coal price, carbon pricing.

\end{itemize}

\sphinxAtStartPar
Here is the code for implementing DML:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{dml\PYGZus{}algorithm}\PYG{p}{(}\PYG{n}{data}\PYG{p}{,} \PYG{n}{n\PYGZus{}splits}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{n\PYGZus{}repeats}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{results} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{kf} \PYG{o}{=} \PYG{n}{KFold}\PYG{p}{(}\PYG{n}{n\PYGZus{}splits}\PYG{o}{=}\PYG{n}{n\PYGZus{}splits}\PYG{p}{,} \PYG{n}{shuffle}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{n}{seed}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}repeats}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{fold\PYGZus{}coefs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{k}{for} \PYG{n}{train\PYGZus{}index}\PYG{p}{,} \PYG{n}{test\PYGZus{}index} \PYG{o+ow}{in} \PYG{n}{kf}\PYG{o}{.}\PYG{n}{split}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{train}\PYG{p}{,} \PYG{n}{test} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{train\PYGZus{}index}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{test\PYGZus{}index}\PYG{p}{]}

            \PYG{n}{confounders} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{gas\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{coal\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{carbon\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{estimated\PYGZus{}load}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

            \PYG{c+c1}{\PYGZsh{} Residualize using GAM}
            \PYG{n}{gam\PYGZus{}wp} \PYG{o}{=} \PYG{n}{LinearGAM}\PYG{p}{(}\PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{gam\PYGZus{}sp} \PYG{o}{=} \PYG{n}{LinearGAM}\PYG{p}{(}\PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{,} \PYG{n}{train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{solar\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{gam\PYGZus{}spot\PYGZus{}price} \PYG{o}{=} \PYG{n}{LinearGAM}\PYG{p}{(}\PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{+} \PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{+} \PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{)} \PYG{o}{+} \PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{3}\PYG{p}{)} \PYG{o}{+} \PYG{n}{s}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{train}\PYG{p}{[}\PYG{n}{confounders}\PYG{p}{]}\PYG{p}{,} \PYG{n}{train}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spot\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}

            \PYG{n}{test\PYGZus{}wp\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{gam\PYGZus{}wp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{test\PYGZus{}sp\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{solar\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{gam\PYGZus{}sp}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{daylight\PYGZus{}hours}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
            \PYG{n}{test\PYGZus{}spot\PYGZus{}price\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{test}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spot\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{gam\PYGZus{}spot\PYGZus{}price}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{test}\PYG{p}{[}\PYG{n}{confounders}\PYG{p}{]}\PYG{p}{)}

            \PYG{c+c1}{\PYGZsh{} Step 4: Regress residuals}
            \PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{test\PYGZus{}wp\PYGZus{}residuals}\PYG{p}{,} \PYG{n}{test\PYGZus{}sp\PYGZus{}residuals}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{test\PYGZus{}spot\PYGZus{}price\PYGZus{}residuals}\PYG{p}{)}
            \PYG{n}{fold\PYGZus{}coefs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}\PYG{p}{)}

        \PYG{n}{results}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{fold\PYGZus{}coefs}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{median}\PYG{p}{(}\PYG{n}{results}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
To have a \sphinxstylestrong{benchmark}, we also compare the DML method with two simpler approaches:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
Two features: a simple approach where we fit a linear regression model only on the two treatment variables

\item {} 
\sphinxAtStartPar
ALl the features: an all\sphinxhyphen{}inclusive approach where we fit a linear regression model on all the available features.

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{fit\PYGZus{}models}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{results} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{p}{\PYGZcb{}}

    \PYG{c+c1}{\PYGZsh{} Method (i) Using only WP and SP as predictors}
    \PYG{n}{X\PYGZus{}wp\PYGZus{}sp} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{solar\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
    \PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spot\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
    \PYG{n}{model\PYGZus{}wp\PYGZus{}sp} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}wp\PYGZus{}sp}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    \PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wp\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}  \PYG{o}{=} \PYG{n}{model\PYGZus{}wp\PYGZus{}sp}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}
    
    \PYG{c+c1}{\PYGZsh{} Method (ii) Using all variables as predictors}
    \PYG{n}{X\PYGZus{}all} \PYG{o}{=} \PYG{n}{data}\PYG{o}{.}\PYG{n}{drop}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{spot\PYGZus{}price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{model\PYGZus{}all} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}all}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Extracting coefficients for wind\PYGZus{}production and solar\PYGZus{}production specifically}
    \PYG{n}{coef\PYGZus{}all} \PYG{o}{=} \PYG{n}{model\PYGZus{}all}\PYG{o}{.}\PYG{n}{coef\PYGZus{}}
    \PYG{n}{wp\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{X\PYGZus{}all}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wind\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{sp\PYGZus{}idx} \PYG{o}{=} \PYG{n+nb}{list}\PYG{p}{(}\PYG{n}{X\PYGZus{}all}\PYG{o}{.}\PYG{n}{columns}\PYG{p}{)}\PYG{o}{.}\PYG{n}{index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{solar\PYGZus{}production}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    \PYG{n}{results}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n}{coef\PYGZus{}all}\PYG{p}{[}\PYG{n}{wp\PYGZus{}idx}\PYG{p}{]}\PYG{p}{,} \PYG{n}{coef\PYGZus{}all}\PYG{p}{[}\PYG{n}{sp\PYGZus{}idx}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{results}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now run replicate the data generation and parameter estimation experiment using the three approaches (DML and the two benchmarks):

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{n\PYGZus{}iterations} \PYG{o}{=} \PYG{l+m+mi}{5}
\PYG{n}{coefficients} \PYG{o}{=} \PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wp\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}

\PYG{c+c1}{\PYGZsh{} Loop to generate data, residualize covariates, and fit models}
\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{df} \PYG{o}{=} \PYG{n}{generate\PYGZus{}data}\PYG{p}{(}\PYG{n}{n\PYGZus{}years}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{random\PYGZus{}seed}\PYG{o}{=}\PYG{n}{i}\PYG{p}{)}
    \PYG{c+c1}{\PYGZsh{} df\PYGZus{}residualized = residualize\PYGZus{}data(df)}
    \PYG{n}{df\PYGZus{}residualized} \PYG{o}{=} \PYG{n}{df}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    \PYG{n}{res1} \PYG{o}{=} \PYG{n}{fit\PYGZus{}models}\PYG{p}{(}\PYG{n}{df}\PYG{p}{)}
    \PYG{n}{res2} \PYG{o}{=} \PYG{n}{dml\PYGZus{}algorithm}\PYG{p}{(}\PYG{n}{df\PYGZus{}residualized}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{n}{i}\PYG{p}{)}

    \PYG{k}{for} \PYG{n}{key} \PYG{o+ow}{in} \PYG{n}{res1}\PYG{p}{:}
        \PYG{n}{coefficients}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{res1}\PYG{p}{[}\PYG{n}{key}\PYG{p}{]}\PYG{p}{)}
    \PYG{n}{coefficients}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{res2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Convert to DataFrame for analysis}
\PYG{n}{coefficients\PYGZus{}df} \PYG{o}{=} \PYG{p}{\PYGZob{}}\PYG{n}{key}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{value}\PYG{p}{)} \PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{coefficients}\PYG{o}{.}\PYG{n}{items}\PYG{p}{(}\PYG{p}{)}\PYG{p}{\PYGZcb{}}
\PYG{n}{coefficients\PYGZus{}df} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Two features\PYGZus{}wp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wp\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Two features\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{wp\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{All the features\PYGZus{}wp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{All the features\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{all}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML\PYGZus{}wp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{coefficients\PYGZus{}df}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now plot the results:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the labels and colors}
\PYG{n}{labels} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Two features}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{All the features}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{DML}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{colors} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightgreen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{markers} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Calculate new quantiles (P10, P50, P90)}
\PYG{n}{quantiles} \PYG{o}{=} \PYG{n}{coefficients\PYGZus{}df}\PYG{o}{.}\PYG{n}{quantile}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.10}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.90}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{T}
\PYG{n}{quantiles}\PYG{o}{.}\PYG{n}{columns} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P50}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P90}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{quantiles} \PYG{o}{=} \PYG{n}{quantiles}\PYG{o}{.}\PYG{n}{reset\PYGZus{}index}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{rename}\PYG{p}{(}\PYG{n}{columns}\PYG{o}{=}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{index}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coefficient}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the quantiles}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}

\PYG{n}{bar\PYGZus{}width} \PYG{o}{=} \PYG{l+m+mf}{0.3}
\PYG{n}{bins} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{label} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{wp\PYGZus{}label} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}wp}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{sp\PYGZus{}label} \PYG{o}{=} \PYG{l+s+sa}{f}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+si}{\PYGZob{}}\PYG{n}{label}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s1}{\PYGZus{}sp}\PYG{l+s+s1}{\PYGZsq{}}
    \PYG{n}{wp\PYGZus{}data} \PYG{o}{=} \PYG{n}{quantiles}\PYG{p}{[}\PYG{n}{quantiles}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coefficient}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{n}{wp\PYGZus{}label}\PYG{p}{]}
    \PYG{n}{sp\PYGZus{}data} \PYG{o}{=} \PYG{n}{quantiles}\PYG{p}{[}\PYG{n}{quantiles}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coefficient}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{n}{sp\PYGZus{}label}\PYG{p}{]}
    \PYG{n}{offset\PYGZus{}wp} \PYG{o}{=} \PYG{n}{i} \PYG{o}{\PYGZhy{}} \PYG{n}{bar\PYGZus{}width} \PYG{o}{/} \PYG{l+m+mi}{2}
    \PYG{n}{offset\PYGZus{}sp} \PYG{o}{=} \PYG{n}{i} \PYG{o}{+} \PYG{n}{bar\PYGZus{}width} \PYG{o}{/} \PYG{l+m+mi}{2}
    
    \PYG{c+c1}{\PYGZsh{} WP bars and points}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{offset\PYGZus{}wp}\PYG{p}{,} \PYG{n}{wp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P90}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values} \PYG{o}{\PYGZhy{}} \PYG{n}{wp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{bottom}\PYG{o}{=}\PYG{n}{wp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{width}\PYG{o}{=}\PYG{n}{bar\PYGZus{}width}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP coefficient}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{i} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{k}{else} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{offset\PYGZus{}wp}\PYG{p}{,} \PYG{n}{wp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P50}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} SP bars and points}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{bar}\PYG{p}{(}\PYG{n}{offset\PYGZus{}sp}\PYG{p}{,} \PYG{n}{sp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P90}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values} \PYG{o}{\PYGZhy{}} \PYG{n}{sp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{bottom}\PYG{o}{=}\PYG{n}{sp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P10}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{width}\PYG{o}{=}\PYG{n}{bar\PYGZus{}width}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightgreen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP coefficient}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{i} \PYG{o}{==} \PYG{l+m+mi}{0} \PYG{k}{else} \PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
    \PYG{n}{ax}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{offset\PYGZus{}sp}\PYG{p}{,} \PYG{n}{sp\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{P50}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{.}\PYG{n}{values}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{green}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set x\PYGZhy{}axis labels}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticks}\PYG{p}{(}\PYG{n}{bins}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xticklabels}\PYG{p}{(}\PYG{n}{labels}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Simplified legend}
\PYG{n}{handles} \PYG{o}{=} \PYG{p}{[}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightblue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{WP coefficient}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lightgreen}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{SP coefficient}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{,}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{Line2D}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{ls}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{True values}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Add horizontal line for the true value}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{axhline}\PYG{p}{(}\PYG{n}{y}\PYG{o}{=}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} ax.set\PYGZus{}title(\PYGZsq{}Quantiles of Estimated Coefficients for WP and SP\PYGZsq{})}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Method}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Coefficient value}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{x}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{n}{handles}\PYG{o}{=}\PYG{n}{handles}\PYG{p}{,} \PYG{n}{loc}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{lower right}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{5e50a93d8ee4d536e87aa13cb6fdb23cfc63a09c7c630fc102e82b5a0eae4ae3}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how the DML framework proves very effective in reducing the effect of the confounders on the estimation process. We can also see how the two\sphinxhyphen{}feature approach performs poorly, suffering from the \sphinxstylestrong{omitted variable bias}. Also, the all\sphinxhyphen{}inclusive approach, widely used in econometrics, has a much higher estimation variance. This is also due to the fact the specified model is linear while some of the effects were actually nonlinear.

\sphinxstepscope


\chapter{Difference\sphinxhyphen{}in\sphinxhyphen{}Differences}
\label{\detokenize{notebooks/diff_in_diff:difference-in-differences}}\label{\detokenize{notebooks/diff_in_diff::doc}}
\sphinxAtStartPar
Difference\sphinxhyphen{}in\sphinxhyphen{}differences (DiD) is a statistical technique used in econometrics to estimate the causal effect of a treatment on a time series. For example, it might be used to \sphinxstylestrong{evaluate the effect of a new policy} on the electricity prices, in a specific region.

\sphinxAtStartPar
Let us suppose we collected some data from electricity prices over time, and we know that at a certain point in time a new policy was introduced. Let us also suppose that we observed a change in the trend, and we would like to attribute that change to the newly introduced policy. To be sure that the change in price time series is indeed due to the new policy, we would need to know the \sphinxstylestrong{counterfactual}. The counterfactual represents what would have happened to the prices had the new policy not been introduced. Then, by comparing the prices observed under the new policy with what the prices would have been without the policy, we would finally be able to say that the prices changed because of the policy. Without the counterfactual, we might not be able to accurately determine the true effect of the policy because of:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Other influencing factors: in the real world, multiple factors can influence electricity prices simultaneously. These could include changes in fuel prices, demand fluctuations, economic conditions, and other regulatory changes. Without a counterfactual, it is difficult to isolate the impact of the specific policy from these other variables.

\item {} 
\sphinxAtStartPar
Temporal trends: electricity prices may follow certain trends over time regardless of the policy intervention. For instance, prices might be declining due to improvements in technology or increasing due to rising demand. The counterfactual helps to control for these underlying trends, providing a clearer picture of what the prices would have looked like in the absence of the policy.

\end{itemize}

\sphinxAtStartPar
Unfortunately, in observational data, we cannot observe the counterfactual, since we only have access to what has happened under the new policy. DiD tries to tackle this problem by comparing the changes in the prices over time before a treatment group and a control group, where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{treatment group} is, for example, the data collected from a region where the new policy was introduced.

\item {} 
\sphinxAtStartPar
The \sphinxstylestrong{control group} might refer to another region, where the new policy was not introduced.

\end{itemize}

\sphinxAtStartPar
While doing so, DiD relies a crucial assumption known as the \sphinxstylestrong{parallel trends assumption}. This assumption asserts that the treatment and control groups would have followed the same trajectory over time in the absence of the treatment. Using the parallel trends assumption, we can use the change of the prices of the control group as a counterfactual for the treatment group in the absence of the treatment. n simple terms, DiD means that we are looking at:
\label{equation:notebooks/diff_in_diff:b817c93b-a6ca-4431-863f-ed85c5aed464}\begin{align}
    \text{DiD} = &(\text{price in treatment group before policy} - \text{price in treatment group after policy}) \\
    & - (\text{price in control group before policy} - \text{price in control group after policy})
\end{align}
\sphinxAtStartPar
In other words, we are checking if there is a \sphinxstylestrong{difference between the two individual differences}. The first difference measures the change in prices for the treatment group before and after the policy, while the second difference measures the change in prices for the control group before and after the policy. By subtracting these two differences, we can isolate the effect of the policy from other factors that might influence electricity prices over time.

\sphinxAtStartPar
In practice, a common approach to DiD is to specify a linear regression model for the outcome of interest (in this case the price) as in:
\label{equation:notebooks/diff_in_diff:3e46b6fd-8c3f-42a8-9a45-f8f648474c16}\begin{equation}
    y = \beta_0 + \beta_1 \, \text{group} + \beta_2 \, \text{period} + \beta_3 \, (\text{group} \times \text{period}) + \varepsilon
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\), \(\beta_1\), \(\beta_2\), and \(\beta_3\) are the regression coefficients.

\item {} 
\sphinxAtStartPar
“group” is a dummy variable (0 or 1) that indicates whether the observation is from the control group (0) or the treatment group (1).

\item {} 
\sphinxAtStartPar
“period” is a dummy variable (0 or 1) that indicates whether the observation is from the period before the policy implementation (0) or after (1).

\item {} 
\sphinxAtStartPar
“\(\text{group} \times \text{period}\)” is an interaction term to count for the \sphinxstylestrong{DiD causal effect}. This variable captures the combined effect of being in the treatment group and being in the post\sphinxhyphen{}policy period.

\end{itemize}

\sphinxAtStartPar
The model can be estimated with traditional methods such as ordinary least squares (OLS). DiD can also be extended to nonlinear or semi\sphinxhyphen{}parametric settings.

\sphinxAtStartPar
To provide a practical \sphinxstylestrong{example}, we will now generate some data for electricity prices over time for two regions: one where the policy was implemented (treatment group) and one where it was not (control group). We will include a change in the prices of the treatment gorup by adding a \sphinxstylestrong{policy effect} that reduces the prices of 10 units after the policy has been introduced.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate time series data}
\PYG{n}{n\PYGZus{}periods} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{time} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}
\PYG{n}{policy\PYGZus{}start} \PYG{o}{=} \PYG{l+m+mi}{50}  \PYG{c+c1}{\PYGZsh{} Time when the policy starts}

\PYG{c+c1}{\PYGZsh{} Generate prices for control group}
\PYG{n}{control\PYGZus{}prices} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate prices for treatment group with a policy effect}
\PYG{n}{treatment\PYGZus{}prices} \PYG{o}{=} \PYG{l+m+mi}{50} \PYG{o}{+} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}
\PYG{n}{treatment\PYGZus{}prices}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{:}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}  \PYG{c+c1}{\PYGZsh{} Policy effect}

\PYG{c+c1}{\PYGZsh{} Create a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{tile}\PYG{p}{(}\PYG{n}{time}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{control\PYGZus{}prices}\PYG{p}{,} \PYG{n}{treatment\PYGZus{}prices}\PYG{p}{]}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{control}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{n\PYGZus{}periods}\PYG{p}{)}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{period}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{np}\PYG{o}{.}\PYG{n}{concatenate}\PYG{p}{(}\PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{policy\PYGZus{}start}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods} \PYG{o}{\PYGZhy{}} \PYG{n}{policy\PYGZus{}start}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{policy\PYGZus{}start}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{ones}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods} \PYG{o}{\PYGZhy{}} \PYG{n}{policy\PYGZus{}start}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
     time      price      group  period
0       0   0.993428    control     0.0
1       1   0.223471    control     0.0
2       2   2.295377    control     0.0
3       3   4.546060    control     0.0
4       4   1.531693    control     0.0
..    ...        ...        ...     ...
195    95  88.270635  treatment     1.0
196    96  86.232285  treatment     1.0
197    97  88.807450  treatment     1.0
198    98  89.116417  treatment     1.0
199    99  87.214059  treatment     1.0

[200 rows x 4 columns]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now plot the time series corresponding to the treatment and control groups to visually explore potential effects.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Plot the data}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{k}{for} \PYG{n}{key}\PYG{p}{,} \PYG{n}{grp} \PYG{o+ow}{in} \PYG{n}{data}\PYG{o}{.}\PYG{n}{groupby}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{grp}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{grp}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{n}{key}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{84feae0987e374140a6fa635386f7cab5f08e821456e5fbda9f727c9fac02ae0}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see, the prices in the treatment group appears to have slightly changed after the policy. Using the \sphinxstylestrong{parallel trends assumption}, we check for the difference in the prices, assuming the price in the treatment group would have followed the same trend of the control group.

\sphinxAtStartPar
Before fitting a DiD model, we need to create the dummy variables related to the group and the interaction between the group and the policy.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create a dummy variable for the group}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{==} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add an interaction term for DiD}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interaction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{*} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{period}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We now fit a simple OLS model on the data:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}

\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{group}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{period}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{interaction}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R\PYGZhy{}squared:                       0.923
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.921
Method:                 Least Squares   F\PYGZhy{}statistic:                     778.6
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):          1.37e\PYGZhy{}108
Time:                        19:10:07   Log\PYGZhy{}Likelihood:                \PYGZhy{}679.31
No. Observations:                 200   AIC:                             1367.
Df Residuals:                     196   BIC:                             1380.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
===============================================================================
                  coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const          11.7991      1.032     11.431      0.000       9.763      13.835
group          50.3724      1.460     34.507      0.000      47.493      53.251
period         25.4865      1.460     17.459      0.000      22.608      28.365
interaction   \PYGZhy{}10.2401      2.064     \PYGZhy{}4.960      0.000     \PYGZhy{}14.312      \PYGZhy{}6.169
==============================================================================
Omnibus:                       58.622   Durbin\PYGZhy{}Watson:                   0.309
Prob(Omnibus):                  0.000   Jarque\PYGZhy{}Bera (JB):               10.369
Skew:                          \PYGZhy{}0.027   Prob(JB):                      0.00560
Kurtosis:                       1.886   Cond. No.                         6.85
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see that the \sphinxstylestrong{interaction coefficients captures the causal effect of the policy}, accurately showing that the policy caused an increase of 10 units in the prices of the treatment group. This interaction term captures the differential effect of the policy on the treatment group relative to the control group, accounting for time trends common to both groups. A significant negative coefficient suggests that the policy led to a \sphinxstylestrong{reduction in prices for the treatment group compared to the control group}.

\sphinxAtStartPar
The \sphinxstylestrong{interaction term} effectively isolates the impact of the policy on the treatment group by controlling for:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Any pre\sphinxhyphen{}existing differences between the treatment and control groups (coefficient “group”).

\item {} 
\sphinxAtStartPar
Any changes over time that would affect both groups equally (coefficient “period”).

\end{itemize}

\sphinxstepscope


\chapter{Interrupted Time Series}
\label{\detokenize{notebooks/interrupted_time_series:interrupted-time-series}}\label{\detokenize{notebooks/interrupted_time_series::doc}}
\sphinxAtStartPar
Interrupted time series (ITS) analysis is econometrics approach that allows us to evaluate the impact of an intervention or policy change that occurs at a specific point in time. Unlike DiD, ITS \sphinxstylestrong{does not require a control group} as it assumes that the data\sphinxhyphen{}generating process would have continued in a similar way without the introduction of the new policy.  ITS can be seen as a special case of a regression discontinuity design (RDD). In a typical RDD, the discontinuity is observed across different units based on a cutoff point. For example, the introduction of a special tax for plants over a certain size. ITS work in a similar way but instead of having the \sphinxstyleemphasis{discontinuity} based on a specific variable, the \sphinxstylestrong{discontinuity occurs over time}. Then, we can model the time series of interest using a \sphinxstylestrong{segmented regression} approach as in:
\label{equation:notebooks/interrupted_time_series:ac417391-cff4-4b71-953e-cd36a99a6a57}\begin{equation}
    y = \beta_0 + \beta_1 \, \text{time} + \beta_2 \, \text{period} + \beta_3 \, \text{time after policy} + \varepsilon
\end{equation}
\sphinxAtStartPar
where the regression coefficients are used to capture:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The baseline intercept (\(\beta_0\)).

\item {} 
\sphinxAtStartPar
The pre\sphinxhyphen{}policy slope (\(\beta_1\)).

\item {} 
\sphinxAtStartPar
The change in level at the introduction of the new policy (\(\beta_2\)). It should be noted that “period” is a dummy variable (0 or 1) that indicates whether the observation is from the period before the policy implementation (0) or after (1).

\item {} 
\sphinxAtStartPar
The change in slope after the introduction of the new policy (\(\beta_3\)).

\end{itemize}

\sphinxAtStartPar
Let’s now try to generate some data to try and apply this methodology in a practical \sphinxstylestrong{example}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate time series data}
\PYG{n}{n\PYGZus{}periods} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{time} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}
\PYG{n}{policy\PYGZus{}start} \PYG{o}{=} \PYG{l+m+mi}{50}  \PYG{c+c1}{\PYGZsh{} Time when the policy starts}

\PYG{c+c1}{\PYGZsh{} Generate prices with a trend and some noise}
\PYG{n}{prices} \PYG{o}{=} \PYG{l+m+mi}{50} \PYG{o}{+} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{time} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Introduce a policy effect (e.g., a reduction in prices)}
\PYG{n}{prices}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{:}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{10}

\PYG{c+c1}{\PYGZsh{} Create a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{time}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{prices}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the data}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{46c973ea979627693c857c2af3b88004cb39baf6de0b5859ff587a5b446c1f53}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Here, we introduced a policy at time step 50, which causes a price reduction of 10 units. Let’s now create the dummy variables and fit a segmented regression model to show if we are able to estimate the effect of the policy.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create the time after policy variable}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time\PYGZus{}after\PYGZus{}policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{period}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}

\PYG{c+c1}{\PYGZsh{} Define the independent variables}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{period}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time\PYGZus{}after\PYGZus{}policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Fit the OLS model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                  price   R\PYGZhy{}squared:                       0.993
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.993
Method:                 Least Squares   F\PYGZhy{}statistic:                     4425.
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):          9.73e\PYGZhy{}103
Time:                        19:10:46   Log\PYGZhy{}Likelihood:                \PYGZhy{}129.67
No. Observations:                 100   AIC:                             267.3
Df Residuals:                      96   BIC:                             277.8
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
=====================================================================================
                        coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const                50.0644      0.252    198.924      0.000      49.565      50.564
time                  0.4882      0.009     55.153      0.000       0.471       0.506
period               \PYGZhy{}9.3026      0.361    \PYGZhy{}25.742      0.000     \PYGZhy{}10.020      \PYGZhy{}8.585
time\PYGZus{}after\PYGZus{}policy     0.0056      0.013      0.448      0.655      \PYGZhy{}0.019       0.030
==============================================================================
Omnibus:                        0.634   Durbin\PYGZhy{}Watson:                   2.094
Prob(Omnibus):                  0.728   Jarque\PYGZhy{}Bera (JB):                0.440
Skew:                          \PYGZhy{}0.162   Prob(JB):                        0.802
Kurtosis:                       3.027   Cond. No.                         252.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see, the results show:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The baseline price before the policy was 50 (\(\beta_0\) = const).

\item {} 
\sphinxAtStartPar
The prices were increasing by approximately 0.5 units per time period before the policy (\(\beta_1\) = time).

\item {} 
\sphinxAtStartPar
There was an immediate reduction of 9.3 units (fairly close to 10) in the price level after the policy (\(\beta_2\) = period).

\item {} 
\sphinxAtStartPar
The trend (slope) of prices did not significantly change after the policy (\(\beta_3\) = time\_after\_policy).

\end{itemize}


\section{Fitting an ITS on the residuals of a time series models}
\label{\detokenize{notebooks/interrupted_time_series:fitting-an-its-on-the-residuals-of-a-time-series-models}}
\sphinxAtStartPar
Many real\sphinxhyphen{}world data might be characterised by high autocorrelation or more complex dynamics. In that case, it might be useful to firts fit a model and then examine the behaviour of the residuals to estimate the effect of the policy. For example, using a time series model (e.g., SARIMA) followed by ITS analysis on the residuals might have several advantages, such as:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Handling autocorrelation and seasonality: time series data often exhibit autocorrelation and seasonality, where current values are correlated with past values. Ignoring this aspect can lead to incorrect inferences and underestimated standard errors. By fitting a SARIMA model, we explicitly account for this, leading to more accurate residuals that reflect the underlying stochastic process.

\item {} 
\sphinxAtStartPar
Isolating the intervention effect: by first modeling the time\sphinxhyphen{}dependent structure of the data (i.e., the autocorrelation), we can better isolate the effect of the intervention. The residuals from the SARIMA model represent the portion of the time series that is not explained by its own past values, thus providing a clearer signal of any intervention effects.

\end{itemize}

\sphinxAtStartPar
Let’s try and generate some data, where we fit a SARIMA model on the pre\sphinxhyphen{}policy data, and use it to whiten the whole time series observed. Also in this case, we assume the \sphinxstylestrong{policy effect} is to reduce prices by 10 units.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Set random seed for reproducibility}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate time series data with seasonality, trend, and some noise}
\PYG{n}{n\PYGZus{}periods} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{time} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{date\PYGZus{}range}\PYG{p}{(}\PYG{n}{start}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{2024\PYGZhy{}01\PYGZhy{}01}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{periods}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{seasonal\PYGZus{}period} \PYG{o}{=} \PYG{l+m+mi}{7}
\PYG{n}{policy\PYGZus{}start} \PYG{o}{=} \PYG{l+m+mi}{100}  \PYG{c+c1}{\PYGZsh{} Time index when the policy starts}

\PYG{c+c1}{\PYGZsh{} Generate seasonal component}
\PYG{n}{seasonal\PYGZus{}effect} \PYG{o}{=} \PYG{l+m+mi}{10} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{n}{seasonal\PYGZus{}period} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate trend component}
\PYG{n}{trend\PYGZus{}effect} \PYG{o}{=} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate noise component}
\PYG{n}{noise} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Combine components}
\PYG{n}{prices} \PYG{o}{=} \PYG{l+m+mi}{50} \PYG{o}{+} \PYG{n}{seasonal\PYGZus{}effect} \PYG{o}{+} \PYG{n}{trend\PYGZus{}effect} \PYG{o}{+} \PYG{n}{noise}

\PYG{c+c1}{\PYGZsh{} Introduce a policy effect (e.g., a reduction in prices)}
\PYG{n}{prices}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{:}\PYG{p}{]} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{l+m+mi}{10}

\PYG{c+c1}{\PYGZsh{} Create a DataFrame}
\PYG{n}{its\PYGZus{}data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{time}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{prices}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Set the index to the datetime}
\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{set\PYGZus{}index}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{inplace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DatetimeIndex}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the data}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{,} \PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Electricity Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{8e0e41eef5a991e51c3b72f5e4bb6cdb4dc163d12659163edf1e8ee34cb8dfc1}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Since the policy has been introduced at the 100th observation, we can fint a SARIMA model on the first 100 observations, and then perform and ITS analysis on the reisudals.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{tsa}\PYG{n+nn}{.}\PYG{n+nn}{statespace}\PYG{n+nn}{.}\PYG{n+nn}{sarimax} \PYG{k+kn}{import} \PYG{n}{SARIMAX}

\PYG{c+c1}{\PYGZsh{} Fit a SARIMA model to the pre\PYGZhy{}intervention period}
\PYG{n}{pre\PYGZus{}policy\PYGZus{}data} \PYG{o}{=} \PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{p}{:}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}
\PYG{n}{sarima\PYGZus{}model} \PYG{o}{=} \PYG{n}{SARIMAX}\PYG{p}{(}\PYG{n}{pre\PYGZus{}policy\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{order}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{seasonal\PYGZus{}order}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{seasonal\PYGZus{}period}\PYG{p}{)}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{disp}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Get the residuals for the entire time series}
\PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{sarima\PYGZus{}model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{start}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{n}{end}\PYG{o}{=}\PYG{n}{n\PYGZus{}periods}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{dynamic}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the data}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{72d0f19c01b9ac659243d73157a3350599a0279350548826a30d0a276e65037d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As expected, given the change due to the new policy, the residuals before the policy (where we fitted the model) are quite low, while they are much higher after the policy was introduced.

\sphinxAtStartPar
We can now add the necessary dummy variables to perform the ITS analysis.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Create the time after policy variable}
\PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time\PYGZus{}after\PYGZus{}policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{p}{)}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{policy\PYGZus{}start}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{where}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{n}{its\PYGZus{}data}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                price  residuals  time\PYGZus{}after\PYGZus{}policy  policy
time                                                       
2024\PYGZhy{}01\PYGZhy{}01  50.496714  50.496714                  0       0
2024\PYGZhy{}01\PYGZhy{}02  57.780051   7.283374                  0       0
2024\PYGZhy{}01\PYGZhy{}03  60.596968   2.816929                  0       0
2024\PYGZhy{}01\PYGZhy{}04  56.161867  \PYGZhy{}4.435096                  0       0
2024\PYGZhy{}01\PYGZhy{}05  45.827009 \PYGZhy{}10.334864                  0       0
...               ...        ...                ...     ...
2024\PYGZhy{}07\PYGZhy{}14  52.067003  \PYGZhy{}9.606227                 95       1
2024\PYGZhy{}07\PYGZhy{}15  58.716143 \PYGZhy{}10.573412                 96       1
2024\PYGZhy{}07\PYGZhy{}16  67.672040  \PYGZhy{}9.758926                 97       1
2024\PYGZhy{}07\PYGZhy{}17  69.607488  \PYGZhy{}9.616665                 98       1
2024\PYGZhy{}07\PYGZhy{}18  63.095867 \PYGZhy{}11.235913                 99       1

[200 rows x 4 columns]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, let’s fit an segmented regression model, using OLS, as we did in the first part of the chapter.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the independent variables for the ITS model}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{time\PYGZus{}after\PYGZus{}policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{policy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Fit the OLS model on the residuals}
\PYG{n}{its\PYGZus{}model\PYGZus{}on\PYGZus{}residuals} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{y}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{its\PYGZus{}model\PYGZus{}on\PYGZus{}residuals}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:              residuals   R\PYGZhy{}squared:                       0.616
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.613
Method:                 Least Squares   F\PYGZhy{}statistic:                     158.3
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):           1.02e\PYGZhy{}41
Time:                        19:10:47   Log\PYGZhy{}Likelihood:                \PYGZhy{}563.84
No. Observations:                 200   AIC:                             1134.
Df Residuals:                     197   BIC:                             1144.
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
=====================================================================================
                        coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const                 0.3999      0.409      0.979      0.329      \PYGZhy{}0.406       1.206
time\PYGZus{}after\PYGZus{}policy     0.0006      0.014      0.041      0.967      \PYGZhy{}0.027       0.029
policy              \PYGZhy{}10.3131      0.908    \PYGZhy{}11.352      0.000     \PYGZhy{}12.105      \PYGZhy{}8.522
==============================================================================
Omnibus:                      353.785   Durbin\PYGZhy{}Watson:                   0.997
Prob(Omnibus):                  0.000   Jarque\PYGZhy{}Bera (JB):           114456.613
Skew:                           8.849   Prob(JB):                         0.00
Kurtosis:                     118.851   Cond. No.                         130.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how we were able to estimate the \sphinxstylestrong{effect of the policy} quite well.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot the residuals and the ITS fit}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{its\PYGZus{}data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{axvline}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{n}{policy\PYGZus{}start}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{red}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Policy Start}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{its\PYGZus{}data}\PYG{o}{.}\PYG{n}{index}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{its\PYGZus{}model\PYGZus{}on\PYGZus{}residuals}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}\PYG{p}{[}\PYG{l+m+mi}{8}\PYG{p}{:}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ITS Fit on Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ITS Analysis on Residuals}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{2af4dc20361fa4447a97c276dd9faf235e5597ad21d0f8ee3f597661b12146cb}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\part{IV. Interpretability}

\sphinxstepscope


\chapter{Overview}
\label{\detokenize{notebooks/preface_interpretability:overview}}\label{\detokenize{notebooks/preface_interpretability::doc}}
\sphinxAtStartPar
Interpretability is crucial for understanding and explaining how models make predictions or decisions. This section introduces techniques that offer insights into the importance of different features, the effect of individual variables, and the overall behaviour of the model. These methods enhance transparency and trust in the models used in electricity markets.

\sphinxAtStartPar
Understanding and interpreting the inner workings of machine learning models is essential for several reasons:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Transparency}: clear insights into how a model makes decisions help build trust among stakeholders, including regulators, operators, and consumers.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Accountability}: interpretability allows for better scrutiny and ensures that the model’s decisions can be explained and justified, which is particularly important in regulated industries like electricity markets.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Debugging and improvement}: by understanding which features influence the model’s predictions, data scientists and engineers can identify and correct issues, improving the model’s accuracy and robustness.

\end{itemize}

\sphinxAtStartPar
Despite providing some transparency, \sphinxstylestrong{these methods alone do not have causal guarantees} as they merely indicate how the predictions of the models are affected by changes in the covariates. Therefore, while they can show associations and effects within the model, they do not confirm causal relationships.


\section{Content of Interpretability chapters}
\label{\detokenize{notebooks/preface_interpretability:content-of-interpretability-chapters}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Chapter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Partial Dependence Plots
&
\sphinxAtStartPar
How to visualize the relationship between a feature and the predicted outcome, while averaging out the effects of all other features.
\\
\sphinxhline
\sphinxAtStartPar
Accumulated Local Effects
&
\sphinxAtStartPar
How to provide a more accurate and unbiased alternative to PDP by accounting for feature interactions.
\\
\sphinxhline
\sphinxAtStartPar
Impulse Response Functions
&
\sphinxAtStartPar
How to assess the dynamic impact of a change in one variable on another over time in a time series context.
\\
\sphinxhline
\sphinxAtStartPar
Shapley Values
&
\sphinxAtStartPar
How to obtain a measure of how each feature contributes to the predictions made by the model.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{Partial Dependence Plots}
\label{\detokenize{notebooks/partial_dependency_plots:partial-dependence-plots}}\label{\detokenize{notebooks/partial_dependency_plots::doc}}
\sphinxAtStartPar
Partial Dependence Plots (PDPs) help us understand the relationship between a feature (or two features) and the predicted outcome of a model. The key idea is to see how changing the value of a feature affects the prediction, while \sphinxstylestrong{averaging out} the effects of all other features. A PDP can be very useful to unveil the nature of a relationship, for example by letting us see whether it is linear or not.

\sphinxAtStartPar
Consider a case where we fit a model to predict the wholesale electricity price using as input the temperature, the time of the day, and the day of the week. Using PDPs, we would like to explore the specific form of influence that each variable has on the predicted response. To explore that, let’s now generate some data where the Price is a function of temperature, hour, and day. The effect we will generate, and that we expect to find from the PDPs are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Temperature}: the typical banana shape we have seen in previous examples, where prices increase as we move away from mild temperatures. This means that the price will be higher when the temperature is either very low or very high, and lower when the temperature is moderate, forming a U\sphinxhyphen{}shaped curve.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hour}: a sinusoidal pattern throughout the day, with two peaks and one trough. We expect higher prices in the morning (between 7 AM and 10 AM) and in the evening (between 6 PM and 10 PM) due to increased demand during these times. The prices will be lower in the afternoon (between 10 AM and 5 PM), reflecting decreased demand.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Day}: higher prices on Sundays compared to the rest of the week. This simulates the effect of increased electricity consumption on weekends when people are more likely to be at home.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{40}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Temperature in Celsius}
\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Time of day in hours}
\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Day of the week (0=Sunday, 6=Saturday)}

\PYG{n}{price} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperature} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Quadratic effect of temperature}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{6}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Smooth peak between 7 AM and 10 AM}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{22}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Smooth peak between 6 PM and 10 PM}
         \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{17}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Smooth trough between 10 AM and 5 PM}
         \PYG{l+m+mi}{7} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{24}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Sinusoidal effect throughout the day}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{p}{(}\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Higher prices on Sunday}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Noise}

\PYG{c+c1}{\PYGZsh{} Convert to DataFrame for convenience}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperature}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{day\PYGZus{}of\PYGZus{}week}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{price}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the first few rows}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Temperature       Hour  Day     Price
0    21.952540  14.229126    3 \PYGZhy{}2.562450
1    28.607575   0.241529    3  7.237870
2    24.110535  11.419829    3  5.621266
3    21.795327  17.010489    5  6.220472
4    16.946192   1.055410    4 \PYGZhy{}2.435281
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Using the \sphinxstylestrong{semi\sphinxhyphen{}parametric causal discovery methods} we discussed in the previous chapters, we might now be able to discover the true causal structure of the data\sphinxhyphen{}generating process. However, we will still not be able to see the \sphinxstylestrong{specific shape of the relationship} between a feature (or a set of features) and the predicted outcome. Now, we assume we already have some knowledge on the true causal structure, and want to use some approaches to further characterise the dependency patterns.

\sphinxAtStartPar
Imagine we fitted a model \(\hat{f}\) that predicts the electricity price using the remaining three features as input variables. Let \(h\) represent the hour variable. The partial dependence function \(\hat{f}(h)\) is defined as:
\label{equation:notebooks/partial_dependency_plots:3d646515-c4ec-4e5f-842c-6af149da6743}\begin{equation}
    \hat{f}(h) = \mathbb{E}_{\mathbf{x}}[\hat{f}(h, \mathbf{x})] = \int \hat{f}(h, \mathbf{x}) dP(\mathbf{x})
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(h\) is the feature of interest (e.g., the hour of the day). It can also be computed for a set of features instead of a single one.

\item {} 
\sphinxAtStartPar
\(\mathbf{x}\) is the vector of all other features in the dataset except \(h\).

\item {} 
\sphinxAtStartPar
\(\hat{f}\) is the model, and \(\hat{f}(h, \mathbf{x})\) is the prediction made by the model for given \(h\) and \(\mathbf{x}\) values.

\item {} 
\sphinxAtStartPar
\(\mathbb{E}_{\mathbf{x}}\) denotes the expectation over the marginal distribution of \(\mathbf{x}\).

\end{itemize}

\sphinxAtStartPar
The \sphinxstylestrong{partial dependence function} shows how the model’s prediction changes when \(h\) changes, keeping the other features constant. Taking the expected value of the model’s prediction over the distribution of \(\mathbf{x}\) effectively averages out the effects of all other features. Indeed, the The integral indicates that we are integrating (or averaging) the model’s predictions over the probability distribution of the features in \(\mathbf{x}\). This integration helps in isolating the effect of \(h\) on the predicted outcome.

\sphinxAtStartPar
In practice, we estimate \(\hat{f}(h)\) using the average prediction over the dataset:
\label{equation:notebooks/partial_dependency_plots:d698923b-2c73-4c81-a2aa-a7b9012653a6}\begin{equation}
    \hat{f}(h) \approx \frac{1}{n} \sum_{i=1}^n \hat{f}(h, \mathbf{x}_i)
\end{equation}
\sphinxAtStartPar
where \(n\) is the number of instances in the dataset, and \(\mathbf{x}_i\) is the \(i\)th observation of the covariates. This sum basically calculates the average prediction of the model when \(h\) is fixed at a specific value, and \(\mathbf{x}\) iterates through all the observed values in the dataset. Here is what we would do in practice:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fix the feature of interest}: pick a specific value of the feature \(h\) (e.g., Hour = 8 AM).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pair it with all other features}: for each data point in the dataset, replace the value of the feature \(h\) with the fixed value (e.g., set Hour to 8 AM for all instances), while keeping the other feature values \(\mathbf{x}\) as they are.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Make predictions}: use the fitted model to make predictions for all these modified data points.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Average these predictions}: calculate the average of these predictions. **This average actually represents the partial dependence value \(\hat{f}(h, \mathbf{x})\).

\end{enumerate}

\sphinxAtStartPar
By iteratively fixing \(h\) at specific values and averaging over all the remaining features, we are actually trying to \sphinxstylestrong{isolate the influence} of changing hours, while keeping the other values constant.

\sphinxAtStartPar
To see this in practice, let’s manually implement the PDP for the “Hour” feature. This will illustrate the steps involved in calculating and plotting a PDP. Here is a simple implementation of a function to estimate the \sphinxstylestrong{partial dependence function}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}pdp}\PYG{p}{(}\PYG{n}{feature\PYGZus{}name}\PYG{p}{,} \PYG{n}{feature\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pdp\PYGZus{}values} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{value} \PYG{o+ow}{in} \PYG{n}{feature\PYGZus{}values}\PYG{p}{:} \PYG{c+c1}{\PYGZsh{} Step 1}
        \PYG{n}{X\PYGZus{}temp} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}temp}\PYG{p}{[}\PYG{n}{feature\PYGZus{}name}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value} \PYG{c+c1}{\PYGZsh{} Step 2}
        \PYG{n}{predictions} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}temp}\PYG{p}{)} \PYG{c+c1}{\PYGZsh{} Step 3}
        \PYG{n}{pdp\PYGZus{}values}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{predictions}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Step 4}
    \PYG{k}{return} \PYG{n}{pdp\PYGZus{}values}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now apply it to a random forest regression model fitted to the data we generated before:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}

\PYG{c+c1}{\PYGZsh{} Split data into features and target}
\PYG{n}{x} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Train a Random Forest Regressor}
\PYG{n}{rf} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{rf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate PDP for the \PYGZdq{}Hour\PYGZdq{} feature}
\PYG{n}{hour\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{l+m+mi}{48}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Range of hour values to evaluate}
\PYG{n}{pdp\PYGZus{}hour} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp}\PYG{p}{(}\PYG{n}{feature\PYGZus{}name}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{feature\PYGZus{}values}\PYG{o}{=}\PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{o}{=}\PYG{n}{x}\PYG{p}{,} \PYG{n}{model}\PYG{o}{=}\PYG{n}{rf}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the manually calculated PDP}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{pdp\PYGZus{}hour}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{07ef68f969bea0382fd45d45ba1796f9fa0277993aeb9e4918674715711ec9fe}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how the PDP shows the pattern we expected, as the time of the day affects the price through the peak morning and evening hours. Now, we can apply the function we implemented to the remaining features too. To obtain more reliable estimates, we can \sphinxstylestrong{repeat the procedure many times}, and plot the mean dependence lines, along with some \sphinxstylestrong{Bootstrap confidence intervals}.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the range of values for each feature}
\PYG{n}{temperature\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{,} \PYG{l+m+mi}{70}\PYG{p}{)}
\PYG{n}{hour\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{l+m+mi}{48}\PYG{p}{)}
\PYG{n}{day\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate PDPs with bootstrapping}
\PYG{n}{n\PYGZus{}bootstraps} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}bootstraps}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Bootstrap sample}
    \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{x\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{x}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{]}
    \PYG{n}{y\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{]}
    
    \PYG{c+c1}{\PYGZsh{} Train model on bootstrap sample}
    \PYG{n}{model\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{model\PYGZus{}bootstrap}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{x\PYGZus{}bootstrap}\PYG{p}{,} \PYG{n}{y\PYGZus{}bootstrap}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Calculate PDPs for each feature}
    \PYG{n}{pdp\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    \PYG{n}{pdp\PYGZus{}hour} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    \PYG{n}{pdp\PYGZus{}day} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    
    \PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}temperature}\PYG{p}{)}
    \PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}hour}\PYG{p}{)}
    \PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}day}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We basically repeated the estimation procedure 100 times. Now, we can use a basic approach and simply take the empirical 2.5\% and 97.5\% percentiles to obtain a 95\% confidence interval for the partial dependence functions.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{)}
\PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{)}
\PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute mean and confidence intervals}
\PYG{n}{mean\PYGZus{}pdp\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}pdp\PYGZus{}hour} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}pdp\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}hour} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{pdp\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now plot the result!

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot PDPs with confidence intervals}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{fontsize} \PYG{o}{=} \PYG{l+m+mi}{18}  \PYG{c+c1}{\PYGZsh{} Set the font size for all labels}

\PYG{c+c1}{\PYGZsh{} Temperature}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{mean\PYGZus{}pdp\PYGZus{}temperature}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Partial Dependence}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{ci\PYGZus{}temperature}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}temperature}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Hour}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{mean\PYGZus{}pdp\PYGZus{}hour}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Partial Dependence}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{ci\PYGZus{}hour}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}hour}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Day}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{mean\PYGZus{}pdp\PYGZus{}day}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Partial Dependence}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{ci\PYGZus{}day}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}day}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day of Week}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Predicted Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{164f9f12eb914a81607f3fe25e35d129d2c906b6006eb78e1d2f70964cf43a41}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Finally, we can also fix the level of \sphinxstylestrong{two variables}, and average over the remaining ones, to see the combined effect on the predicted response. In this case, we can see the results in the form of \sphinxstylestrong{contour plots}. Here is a simple extension of the previous function, to compute PDP for two variables:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}pdp\PYGZus{}two\PYGZus{}features}\PYG{p}{(}\PYG{n}{feature\PYGZus{}names}\PYG{p}{,} \PYG{n}{feature\PYGZus{}values1}\PYG{p}{,} \PYG{n}{feature\PYGZus{}values2}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{pdp\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values1}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values2}\PYG{p}{)}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{value1} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values1}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{for} \PYG{n}{j}\PYG{p}{,} \PYG{n}{value2} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values2}\PYG{p}{)}\PYG{p}{:}
            \PYG{n}{X\PYGZus{}temp} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
            \PYG{n}{X\PYGZus{}temp}\PYG{p}{[}\PYG{n}{feature\PYGZus{}names}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value1}
            \PYG{n}{X\PYGZus{}temp}\PYG{p}{[}\PYG{n}{feature\PYGZus{}names}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{]} \PYG{o}{=} \PYG{n}{value2}
            \PYG{n}{predictions} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}temp}\PYG{p}{)}
            \PYG{n}{pdp\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{n}{j}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{predictions}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{pdp\PYGZus{}values}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate PDPs for the feature combinations}
\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}hour} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp\PYGZus{}two\PYGZus{}features}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{rf}\PYG{p}{)}
\PYG{n}{pdp\PYGZus{}temperature\PYGZus{}day} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp\PYGZus{}two\PYGZus{}features}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{rf}\PYG{p}{)}
\PYG{n}{pdp\PYGZus{}hour\PYGZus{}day} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}pdp\PYGZus{}two\PYGZus{}features}\PYG{p}{(}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{x}\PYG{p}{,} \PYG{n}{rf}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the PDPs as contour plots}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature and Hour}
\PYG{n}{contour1} \PYG{o}{=} \PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{contourf}\PYG{p}{(}\PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{pdp\PYGZus{}temperature\PYGZus{}hour}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{levels}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{contour1}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Temperature and Day}
\PYG{n}{contour2} \PYG{o}{=} \PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{contourf}\PYG{p}{(}\PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{pdp\PYGZus{}temperature\PYGZus{}day}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{levels}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{contour2}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Hour and Day}
\PYG{n}{contour3} \PYG{o}{=} \PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{contourf}\PYG{p}{(}\PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{pdp\PYGZus{}hour\PYGZus{}day}\PYG{o}{.}\PYG{n}{T}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{magma}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{levels}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}
\PYG{n}{fig}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{contour3}\PYG{p}{,} \PYG{n}{ax}\PYG{o}{=}\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{03161585e4d5cd37b60891b918eb7760b93a3b8c3785693c671fec393282149e}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxstepscope


\chapter{Accumulated Local Effects}
\label{\detokenize{notebooks/accumulated_local_effects:accumulated-local-effects}}\label{\detokenize{notebooks/accumulated_local_effects::doc}}
\sphinxAtStartPar
Just like PDPs, accumulated local effects (ALE) plots are designed to investigate the impact of varying the levels of one feature on the predicted response. The main benefit of ALE plots is that they are deemed most effective in the presence of \sphinxstylestrong{correlated variables}.

\sphinxAtStartPar
\sphinxstylestrong{Issues with PDPs}: when features in a dataset are correlated, PDPs can be misleading. Since PDPs average predictions over the marginal distribution of the features, and do not consider the correlation patterns, they can sometimes result in \sphinxstylestrong{unrealistic data points}. For instance, if we predict electricity prices based on temperature, time of day, and day of the week, PDPs might consider unlikely combinations, such as high electricity prices at 3 AM on weekdays.

\sphinxAtStartPar
There are two potential solutions to this issue:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Averaging over the conditional distribution}, instead of the marginal, thereby excluding the inclusion of unrealistic data points.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Averaging over differences in predictions}, instead of averaging directly the predictions.

\end{enumerate}

\sphinxAtStartPar
The second option is the one used by ALE plots, which compute \sphinxstylestrong{differences in predictions within small intervals}, ensuring the estimates are based on realistic data points. This approach makes ALE plots more reliable when features are correlated.

\sphinxAtStartPar
In practice, the ALE for the \(j\)th feature, \(x_j\), is estimated by:
\label{equation:notebooks/accumulated_local_effects:e7d7e1b2-4ab9-46e2-bc18-7b38e07e274b}\begin{equation}
    \hat{f}_{j,\text{ALE}}(x) = \sum_{k=1}^{k(x)} \frac{1}{n_j(k)} \sum_{i : x_j^{(i)} \in N_j(k)} \left( \hat{f}(z_{k,j}, x_{-j}^{(i)}) - \hat{f}(z_{k-1,j}, x_{-j}^{(i)}) \right)
\end{equation}
\sphinxAtStartPar
where \(N_j(k)\) is the \(k\)th interval for feature \(x_j\), \(z_{k,j}\) is the upper boundary of the \(k\)th interval, \(x_{-j}^{(i)}\) are all the other features except \(x_j\), for the \(i\)th data point, and \(n_j(k)\) is the number of data points in the \(k\)th interval.

\sphinxAtStartPar
The previous equation represents the following steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Divide the feature into intervals}: split the range of the variable of interest \(h\) into \(m\) intervals, often based on quantiles to ensure an equal number of data points in each interval.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute local effects}: for each interval, compute the difference in model predictions when \(h\) is replaced by the interval’s lower and upper bounds.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Accumulate local effects}: sum up the local effects within each interval to obtain the accumulated effect.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Center the ALE plot}: subtract the mean accumulated effect to center the plot around zero.

\end{enumerate}

\sphinxAtStartPar
Thus, we are simply calculating the average difference in predictions when the feature of interest is changed from the lower to the upper boundary of the interval, and accumulating these differences across all intervals.

\sphinxAtStartPar
This process is expected to perform better than PDP for correlated features because it accounts for the dependencies between features, ensuring that the differences in predictions are computed based on realistic data points. By focusing on local changes within intervals, ALE plots provide a more accurate representation of feature effects in the presence of correlations.

\sphinxAtStartPar
Here is a simple manual implementation of ALE:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}

\PYG{k}{def} \PYG{n+nf}{calculate\PYGZus{}ale}\PYG{p}{(}\PYG{n}{feature\PYGZus{}name}\PYG{p}{,} \PYG{n}{feature\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{p}{,} \PYG{n}{num\PYGZus{}intervals}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{interval\PYGZus{}edges} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{n+nb}{min}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values}\PYG{p}{)}\PYG{p}{,} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{feature\PYGZus{}values}\PYG{p}{)}\PYG{p}{,} \PYG{n}{num\PYGZus{}intervals} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{ale\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{num\PYGZus{}intervals}\PYG{p}{)}
    \PYG{n}{X\PYGZus{}temp} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
    
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{num\PYGZus{}intervals}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{lower} \PYG{o}{=} \PYG{n}{interval\PYGZus{}edges}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}
        \PYG{n}{upper} \PYG{o}{=} \PYG{n}{interval\PYGZus{}edges}\PYG{p}{[}\PYG{n}{i} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{]}
        
        \PYG{n}{in\PYGZus{}interval} \PYG{o}{=} \PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{feature\PYGZus{}name}\PYG{p}{]} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{n}{lower}\PYG{p}{)} \PYG{o}{\PYGZam{}} \PYG{p}{(}\PYG{n}{X}\PYG{p}{[}\PYG{n}{feature\PYGZus{}name}\PYG{p}{]} \PYG{o}{\PYGZlt{}} \PYG{n}{upper}\PYG{p}{)}
        \PYG{k}{if} \PYG{n}{in\PYGZus{}interval}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{)} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{:}
            \PYG{k}{continue}
        
        \PYG{n}{X\PYGZus{}temp}\PYG{p}{[}\PYG{n}{feature\PYGZus{}name}\PYG{p}{]} \PYG{o}{=} \PYG{n}{lower}
        \PYG{n}{preds\PYGZus{}lower} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}temp}\PYG{p}{)}
        
        \PYG{n}{X\PYGZus{}temp}\PYG{p}{[}\PYG{n}{feature\PYGZus{}name}\PYG{p}{]} \PYG{o}{=} \PYG{n}{upper}
        \PYG{n}{preds\PYGZus{}upper} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}temp}\PYG{p}{)}
        
        \PYG{n}{ale\PYGZus{}values}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{preds\PYGZus{}upper}\PYG{p}{[}\PYG{n}{in\PYGZus{}interval}\PYG{p}{]} \PYG{o}{\PYGZhy{}} \PYG{n}{preds\PYGZus{}lower}\PYG{p}{[}\PYG{n}{in\PYGZus{}interval}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{n}{ale\PYGZus{}accumulated} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{ale\PYGZus{}values}\PYG{p}{)}
    \PYG{n}{ale\PYGZus{}accumulated} \PYG{o}{\PYGZhy{}}\PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ale\PYGZus{}accumulated}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{interval\PYGZus{}edges}\PYG{p}{[}\PYG{p}{:}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ale\PYGZus{}accumulated}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now generate some data to show how they work in practice. For simplicity, we will use the same example sued to show PDPs.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{40}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Temperature in Celsius}
\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Time of day in hours}
\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Day of the week (0=Sunday, 6=Saturday)}

\PYG{c+c1}{\PYGZsh{} Simulate electricity price with desired patterns}
\PYG{n}{price} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mf}{0.2} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperature} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Quadratic effect of temperature}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{6}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Peak between 7 AM and 10 AM}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{22}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Peak between 6 PM and 10 PM}
         \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{17}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Trough between 10 AM and 5 PM}
         \PYG{l+m+mi}{7} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{24}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Sinusoidal effect throughout the day}
         \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{p}{(}\PYG{l+m+mi}{1} \PYG{o}{\PYGZhy{}} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cos}\PYG{p}{(}\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{7}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+}  \PYG{c+c1}{\PYGZsh{} Higher prices on Sunday with smooth transition}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Noise}

\PYG{c+c1}{\PYGZsh{} Convert to DataFrame for convenience}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperature}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{day\PYGZus{}of\PYGZus{}week}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{price}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the first few rows}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Temperature       Hour  Day      Price
0    21.952540  14.229126    3   1.830310
1    28.607575   0.241529    3  16.398082
2    24.110535  11.419829    3   8.902574
3    21.795327  17.010489    5   9.614504
4    16.946192   1.055410    4   3.108261
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now fit a regression model (e.g., random forest), and compute the ALE:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}


\PYG{c+c1}{\PYGZsh{} Split data into features and target}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Train a Random Forest Regressor}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate ALE for \PYGZdq{}Hour\PYGZdq{} feature}
\PYG{n}{hour\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{l+m+mi}{48}\PYG{p}{)}
\PYG{n}{intervals\PYGZus{}hour}\PYG{p}{,} \PYG{n}{ale\PYGZus{}hour} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}ale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We now used intervals of 30 minutes, which are typically referred to as \sphinxstylestrong{settlement periods (SPs)} in the electricity markets.

\sphinxAtStartPar
Let’s plot the resulting ALE plot:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot the ALE for \PYGZdq{}Hour\PYGZdq{} feature}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}hour}\PYG{p}{,} \PYG{n}{ale\PYGZus{}hour}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE for Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xticks}\PYG{p}{(}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{yticks}\PYG{p}{(}\PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{6}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{0457745a5244de64760823c3cdbee2a48f01546e1b8ca891da74aa244b7c1eb6}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we did for the PDPs, we can run the procedure multiple times for each feature to construct \sphinxstylestrong{Bootstrap confidence intervals} for the estimated ALE plots.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the range of values for each feature}
\PYG{n}{temperature\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{35}\PYG{p}{,} \PYG{l+m+mi}{70}\PYG{p}{)}
\PYG{n}{hour\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{l+m+mi}{48}\PYG{p}{)}
\PYG{n}{day\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{7}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate ALEs with bootstrapping}
\PYG{n}{n\PYGZus{}bootstraps} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
\PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

\PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}bootstraps}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Bootstrap sample}
    \PYG{n}{indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
    \PYG{n}{X\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{X}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{]}
    \PYG{n}{y\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{y}\PYG{o}{.}\PYG{n}{iloc}\PYG{p}{[}\PYG{n}{indices}\PYG{p}{]}
    
    \PYG{c+c1}{\PYGZsh{} Train model on bootstrap sample}
    \PYG{n}{model\PYGZus{}bootstrap} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{model\PYGZus{}bootstrap}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}bootstrap}\PYG{p}{,} \PYG{n}{y\PYGZus{}bootstrap}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Calculate ALEs for each feature}
    \PYG{n}{intervals\PYGZus{}temperature}\PYG{p}{,} \PYG{n}{ale\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}ale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{temperature\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    \PYG{n}{intervals\PYGZus{}hour}\PYG{p}{,} \PYG{n}{ale\PYGZus{}hour} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}ale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hour\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    \PYG{n}{intervals\PYGZus{}day}\PYG{p}{,} \PYG{n}{ale\PYGZus{}day} \PYG{o}{=} \PYG{n}{calculate\PYGZus{}ale}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{day\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{model\PYGZus{}bootstrap}\PYG{p}{)}
    
    \PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ale\PYGZus{}temperature}\PYG{p}{)}
    \PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ale\PYGZus{}hour}\PYG{p}{)}
    \PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{ale\PYGZus{}day}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Convert results to numpy arrays for easier manipulation}
\PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{)}
\PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{)}
\PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute mean and confidence intervals}
\PYG{n}{mean\PYGZus{}ale\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}ale\PYGZus{}hour} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}ale\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{ale\PYGZus{}temperature\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}hour} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{ale\PYGZus{}hour\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{ale\PYGZus{}day\PYGZus{}bootstraps}\PYG{p}{,} \PYG{p}{[}\PYG{l+m+mf}{2.5}\PYG{p}{,} \PYG{l+m+mf}{97.5}\PYG{p}{]}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s plot the results

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plot ALEs with confidence intervals}
\PYG{n}{fig}\PYG{p}{,} \PYG{n}{ax} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplots}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}

\PYG{n}{fontsize} \PYG{o}{=} \PYG{l+m+mi}{8}  \PYG{c+c1}{\PYGZsh{} Set the font size for all labels}

\PYG{c+c1}{\PYGZsh{} Temperature}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}temperature}\PYG{p}{,} \PYG{n}{mean\PYGZus{}ale\PYGZus{}temperature}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}temperature}\PYG{p}{,} \PYG{n}{ci\PYGZus{}temperature}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}temperature}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE for Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Hour}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}hour}\PYG{p}{,} \PYG{n}{mean\PYGZus{}ale\PYGZus{}hour}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}hour}\PYG{p}{,} \PYG{n}{ci\PYGZus{}hour}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}hour}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE for Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Day}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}day}\PYG{p}{,} \PYG{n}{mean\PYGZus{}ale\PYGZus{}day}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidth}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n}{intervals\PYGZus{}day}\PYG{p}{,} \PYG{n}{ci\PYGZus{}day}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci\PYGZus{}day}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE for Day of Week}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day of Week}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{ALE}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}
\PYG{n}{ax}\PYG{p}{[}\PYG{l+m+mi}{2}\PYG{p}{]}\PYG{o}{.}\PYG{n}{tick\PYGZus{}params}\PYG{p}{(}\PYG{n}{axis}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{both}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{which}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{major}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{labelsize}\PYG{o}{=}\PYG{n}{fontsize}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{3f1ba0a3324bc5cb0cb48fb4b626f95cc703f7f0e85e0de5c02d4807a2728fc4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
As we can see, the effects of Hour and Temperature variables have been properly identified. To try and see the potential benefits ovet PDPs, we could generated correlated data and see if ALE plots offer improvements,

\sphinxstepscope


\chapter{Impulse Response Functions}
\label{\detokenize{notebooks/impulse_response_functions:impulse-response-functions}}\label{\detokenize{notebooks/impulse_response_functions::doc}}
\sphinxAtStartPar
Conversely from PDPs and ALEs, impulse response functions (IRFs) are particularly suited for \sphinxstylestrong{time series}. They are a key tool in time series analysis, particularly in the context of \sphinxstylestrong{vector autoregressive (VAR) models}. They describe the reaction of the system (i.e., the time series) to a temporary shock in one of the variables. In the context of electricity markets, IRFs can help us understand how a shock in one factor, such as temperature or demand, impacts electricity prices over time.

\sphinxAtStartPar
In electricity markets, understanding the dynamic relationships between different factors is crucial. For example, a sudden increase in temperature might lead to higher electricity demand due to air conditioning, which in turn could increase electricity prices. IRFs allow us to quantify and visualize these dynamic responses, providing insights into the temporal effects of shocks on the market.

\sphinxAtStartPar
Consider a VAR(\(p\)) model with \(k\) variables. The order \(p\) determines the number of lagged observations included in the model, reflecting the extent to which past values influence the current value. The model is given by:
\label{equation:notebooks/impulse_response_functions:321e8354-e7a8-412f-921b-ec658e1490f5}\begin{equation}
    \mathbf{y}_t = \mathbf{A}_1 \mathbf{y}_{t-1} + \mathbf{A}_2 \mathbf{y}_{t-2} + \cdots + \mathbf{A}_p \mathbf{y}_{t-p} + \mathbf{u}_t
\end{equation}
\sphinxAtStartPar
where \(\mathbf{y}_t\) is a \(k \times 1\) vector of time series variables at time \(t\), \(\mathbf{A}_i\) are the \(k \times k\) coefficient matrices, and \(\mathbf{u}_t\) is a \(k \times 1\) vector of error terms (shocks).

\sphinxAtStartPar
An impulse response function measures the effect of a one\sphinxhyphen{}time shock to one of the variables in \(\mathbf{u}_t\) on the current and future values of the variables in \(\mathbf{y}_t\).

\sphinxAtStartPar
The estimation of the IRFs is divided into two main steps:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Fit a VAR model}: estimate the coefficients \(\mathbf{A}_i\) of the VAR model using the time series data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Impulse response calculation}: compute the IRFs by iterating the VAR model forward in time, starting from the shock.

\end{enumerate}

\sphinxAtStartPar
In practice, measuring the effect of a shock means to:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Generate a shock}: introduce a one\sphinxhyphen{}time shock to one of the variables in the error term vector \(\mathbf{u}_t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Propagate the shock}: use the estimated VAR model to propagate the shock through the system, calculating the response of each variable in \(\mathbf{y}_t\) over time.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Visualize the response}: plot the response of the variables to the shock over time, illustrating the dynamic effects of the shock on the system.

\end{enumerate}

\sphinxAtStartPar
The response of each variable to the shock can be tracked over multiple time periods, providing a comprehensive view of the temporal dynamics within the system. This approach helps in understanding how an initial disturbance affects the system both immediately and in the future, offering valuable insights into the behavior and interactions of the variables in the model.

\sphinxAtStartPar
\sphinxstylestrong{Let’s clarify what a shock is in practice}:
In a VAR model, the error term (often referred to as “shock” or “innovation”) represents unexpected changes or disturbances in the system. These are components of the time series that cannot be explained by the past values of the variables in the model. For instance, in an electricity market, the error term for demand could represent unexpected changes in electricity demand due to sudden weather changes, unforeseen industrial activities, or other random events. When we introduce a shock to one of the error terms in the VAR model, we simulate an unexpected change in one of the variables (the one corresponding to the same index of the shock). The IRF then shows us how this shock propagates through the system over time.

\sphinxAtStartPar
Here’s how it works step\sphinxhyphen{}by\sphinxhyphen{}step:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Initial shock}: suppose we introduce a shock to the error term for electricity demand. This means we simulate an unexpected increase or decrease in demand at time \(t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Immediate effect}: this shock directly impacts the demand variable at time \(t\). Since the demand variable is part of the VAR model, this immediate change is reflected in the model’s equation for demand.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Propagation through the system}: the change in demand affects future values of demand (due to the autoregressive nature of the model) and can also affect other variables in the system, such as price. This is because the future values of all variables in a VAR model depend on past values of all variables. For example, an increase in demand could lead to an increase in electricity prices due to higher demand pressures. This effect is captured in the VAR model coefficients.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic responses}: by iterating the VAR model forward in time, the IRF shows how the initial shock to demand affects both demand and price (and potentially other variables) over multiple future time periods.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Practical example}: consider a VAR(2) model with two variables, demand \(D\) and price \(P\). The model equations might look like this:
\label{equation:notebooks/impulse_response_functions:5d6aca01-280b-4d57-ae5e-3473f3482013}\begin{equation}
\begin{aligned}
    D_t &= a_{10} + a_{11} D_{t-1} + a_{12} P_{t-1} + a_{13} D_{t-2} + a_{14} P_{t-2} + u_{Dt} \\
    P_t &= b_{10} + b_{11} D_{t-1} + b_{12} P_{t-1} + b_{13} D_{t-2} + b_{14} P_{t-2} + u_{Pt}
\end{aligned}
\end{equation}
\sphinxAtStartPar
where \(D_t\) and \(P_t\) are the demand and price at time \(t\), \(a_{ij}\) and \(b_{ij}\) are the model coefficients, \(u_{Dt}\) and \(u_{Pt}\) are the error terms (shocks) for demand and price at time \(t\).

\sphinxAtStartPar
Step\sphinxhyphen{}by\sphinxhyphen{}step simulation of a shock:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Introduce a shock}: suppose we introduce a positive shock to \(u_{Dt}\) at time \(t\), representing an unexpected increase in demand.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Immediate Effect}: this shock increases \(D_t\), as \(u_{Dt}\) is part of the equation for \(D_t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Model Iteration}: at time \(t+1\), the increased demand \(D_t\) affects both \(D_{t+1}\) and \(P_{t+1}\) through the VAR model equations. The model will show how the increase in \(D_t\) (demand) influences \(P_{t+1}\) (price) and \(D_{t+1}\) (future demand).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Propagation}: this process continues iteratively, showing how the initial shock propagates through time and affects the system.

\end{enumerate}

\sphinxAtStartPar
In summary, the noise or error terms in a VAR model allow us to introduce and \sphinxstylestrong{simulate unexpected changes in the variables}. By analyzing how these shocks affect the system over time using IRFs, we can understand the dynamic interactions between variables. This approach provides valuable insights into the \sphinxstylestrong{temporal dependencies} and relationships within the system, which are crucial for effective modeling and forecasting in electricity markets.

\sphinxAtStartPar
Let’s now consider a practical example in Python, by generating some time\sphinxhyphen{}dependent data related to temperature, demand, and price.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{time\PYGZus{}index} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{date\PYGZus{}range}\PYG{p}{(}\PYG{n}{start}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{1/1/2020}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{periods}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{freq}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{D}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{demand} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi}\PYG{o}{/}\PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}
\PYG{n}{price} \PYG{o}{=} \PYG{l+m+mf}{0.5} \PYG{o}{*} \PYG{n}{temperature} \PYG{o}{+} \PYG{l+m+mf}{0.3} \PYG{o}{*} \PYG{n}{demand} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mf}{0.2}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create a DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperature}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Demand}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{demand}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{price}\PYG{p}{\PYGZcb{}}\PYG{p}{,} \PYG{n}{index}\PYG{o}{=}\PYG{n}{time\PYGZus{}index}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Display the first few rows}
\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
            Temperature    Demand     Price
2020\PYGZhy{}01\PYGZhy{}01     0.882026  1.648682  0.861781
2020\PYGZhy{}01\PYGZhy{}02     0.295135  0.076651  0.122687
2020\PYGZhy{}01\PYGZhy{}03     0.678620  0.155690  0.605949
2020\PYGZhy{}01\PYGZhy{}04     1.402179  1.312848  1.225997
2020\PYGZhy{}01\PYGZhy{}05     1.305441  0.275470  0.863388
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We now proceed with \sphinxstylestrong{step 1 of the analysis}, which means to fit a VAR model to the data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{tsa}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k+kn}{import} \PYG{n}{VAR}

\PYG{c+c1}{\PYGZsh{} Fit a VAR model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{VAR}\PYG{p}{(}\PYG{n}{data}\PYG{p}{)}
\PYG{n}{results} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{maxlags}\PYG{o}{=}\PYG{l+m+mi}{15}\PYG{p}{,} \PYG{n}{ic}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{aic}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print summary of the model}
\PYG{c+c1}{\PYGZsh{} results.summary()}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can now move to \sphinxstylestrong{step 2}, by introducing shocks in each variable and observing the temporal dependencies and connections among them.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Compute IRFs}
\PYG{n}{irf} \PYG{o}{=} \PYG{n}{results}\PYG{o}{.}\PYG{n}{irf}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Compute IRFs for 10 periods ahead}

\PYG{c+c1}{\PYGZsh{} Plot IRFs}
\PYG{n}{irf}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{orth}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{4b64f1d82791e972b6c48d0e99250b0c089c75a919db9e0755c3b703a0b48ba4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Each subplot represents the response of one variable to a one\sphinxhyphen{}time shock in another variable (or itself). The blue line indicates the estimated impulse response, while the dashed lines represent the confidence intervals. Generally, a shock to a variable has a significant immediate effect on that variable, which then diminishes over time. Temperature shocks tend to increase demand and price initially, aligning with the expectation that higher temperatures drive up electricity demand and prices. Demand shocks significantly affect both demand and price, reflecting the typical market behavior where increased demand leads to higher prices. Price shocks also impact demand and price, though the effects stabilize over time. These IRFs provide insights into the dynamic interactions in the electricity market, showing how shocks propagate through the system.

\sphinxstepscope


\chapter{Shapley Additive Explanations}
\label{\detokenize{notebooks/shapley:shapley-additive-explanations}}\label{\detokenize{notebooks/shapley::doc}}

\section{Shapley values}
\label{\detokenize{notebooks/shapley:shapley-values}}
\sphinxAtStartPar
Shapley additive explanations (SHAP) is a method to explain individual predictions of machine learning models by attributing the output of a model to its input features. It is based on cooperative game theory and provides a way to fairly distribute the “payout” (prediction) among the “players” (features).

\sphinxAtStartPar
In cooperative game theory, the Shapley value is used to distribute the total gains to players based on their contributions. For a set of features \(N\) and a prediction function \(f\), the Shapley value \(\phi_i\) for feature \(i\) is defined as:
\label{equation:notebooks/shapley:ec9e77c7-5231-4a3f-89b8-64ec425c799b}\begin{equation} 
    \phi_i(f) = \sum_{S \subseteq N \setminus \{i\}} \frac{|S|!(|N| - |S| - 1)!}{|N|!} \left[ f(S \cup \{i\}) - f(S) \right]
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(S\) is a subset of features not including \(i\).

\item {} 
\sphinxAtStartPar
\(f(S)\) is the prediction function evaluated with features in \(S\) present and others absent.

\item {} 
\sphinxAtStartPar
\(|S|\) is the number of features in subset \(S\).

\item {} 
\sphinxAtStartPar
\(|N|\) is the total number of features.

\end{itemize}

\sphinxAtStartPar
This formula calculates the \sphinxstylestrong{contribution of a specific feature to the model’s prediction} by averaging how much the prediction changes when the feature is added to all possible combinations of other features, with each combination weighted to ensure fairness.

\sphinxAtStartPar
\sphinxstylestrong{A simple example to show how it works: power plants as features}

\sphinxAtStartPar
Let’s make an example to understand how this formula works in practice. Imagine a scenario where different power plants contribute to the total electricity generation. We want to fairly distribute the revenue generated from selling electricity among these power plants based on their contribution to the total electricity generation.

\sphinxAtStartPar
Let’s say we have power plants \(A\), \(B\), and \(C\). The total revenue from selling electricity is influenced by the contribution of each power plant. Then, we have that
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(N\) is the set of all power plants, \(N = \{A, B, C\}\).

\item {} 
\sphinxAtStartPar
\(S\) is a subset of power plants.

\item {} 
\sphinxAtStartPar
\(f(S)\) is the revenue generated by the subset \(S\) of power plants.

\item {} 
\sphinxAtStartPar
\(\phi_i(f)\) is the Shapley value for power plant \(i\), representing its fair share of the total revenue.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step procedure}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Identify all subsets without plant \(A\)}: the possible subsets \(S\) without \(A\) are: \{\}, \{B\}, \{C\}, and \{B, C\}.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Marginal contribution of adding plant \(A\)}: for each subset \(S\), calculate the additional revenue generated by adding plant \(A\) to \(S\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Calculate marginal contributions}: the impact of adding a specific plant (or feature) to a subset of other plants (or features) in terms of the change in the power production (or model’s prediction). Essentially, they quantify how much the outcome changes when a particular plant is added to different groups of other plants. We will compute one marginsal contribution for each subset:
\begin{itemize}
\item {} 
\sphinxAtStartPar
For \{\}: \(f(\{A\}) - f(\{\})\)

\item {} 
\sphinxAtStartPar
For \{B\}: \(f(\{A, B\}) - f(\{B\})\)

\item {} 
\sphinxAtStartPar
For \{C\}: \(f(\{A, C\}) - f(\{C\})\)

\item {} 
\sphinxAtStartPar
For \{B, C\}: \(f(\{A, B, C\}) - f(\{B, C\})\)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Weight Each Contribution}: the weight for each marginal contribution is
\label{equation:notebooks/shapley:be7d0560-a291-4615-913d-20d293daef41}\begin{equation}
       $\frac{|S|!(|N| - |S| - 1)!}{|N|!}
   \end{equation}
\sphinxAtStartPar
It ensures that the contribution of each feature is averaged fairly across all possible combinations of features:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|S|!\) is the factorial of the size of the subset \(S\), and represents the number of ways to arrange the features in subset \(S\).

\item {} 
\sphinxAtStartPar
\((|N| - |S| - 1)!\) is the factorial of the number of features not in subset \(S\) or the feature \(i\) being evaluated, and represents the number of ways to arrange the remaining features excluding the feature \(i\).

\item {} 
\sphinxAtStartPar
\(|N|!\) is the factorial of the total number of features, and represents the total number of ways to arrange all the features.

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
Let’s compute the Shapley value for plant \(A\):
\label{equation:notebooks/shapley:ab7d7ab1-1ffa-4efc-8044-034940b45bce}\begin{equation} \phi_A(f) = \sum_{S \subseteq N \setminus \{A\}} \frac{|S|!(|N| - |S| - 1)!}{|N|!} \left[ f(S \cup \{A\}) - f(S) \right] \end{equation}
\sphinxAtStartPar
Where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|N| = 3\) (total number of power plants)

\item {} 
\sphinxAtStartPar
\(|S|\) is the size of subset \(S\)

\end{itemize}

\sphinxAtStartPar
Breaking it down for each subset \(S\):
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For \(S = \{\}\)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|S| = 0\)

\item {} 
\sphinxAtStartPar
Marginal contribution: \(f(\{A\}) - f(\{\})\)

\item {} 
\sphinxAtStartPar
Weight: \(\frac{0!(3-0-1)!}{3!} = \frac{1 \cdot 2}{6} = \frac{1}{3}\)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For \(S = \{B\}\)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|S| = 1\)

\item {} 
\sphinxAtStartPar
Marginal contribution: \(f(\{A, B\}) - f(\{B\})\)

\item {} 
\sphinxAtStartPar
Weight: \(\frac{1!(3-1-1)!}{3!} = \frac{1 \cdot 1}{6} = \frac{1}{6}\)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For \(S = \{C\}\)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|S| = 1\)

\item {} 
\sphinxAtStartPar
Marginal contribution: \(f(\{A, C\}) - f(\{C\})\)

\item {} 
\sphinxAtStartPar
Weight: \(\frac{1!(3-1-1)!}{3!} = \frac{1 \cdot 1}{6} = \frac{1}{6}\)

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{For \(S = \{B, C\}\)}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(|S| = 2\)

\item {} 
\sphinxAtStartPar
Marginal contribution: \(f(\{A, B, C\}) - f(\{B, C\})\)

\item {} 
\sphinxAtStartPar
Weight: \(\frac{2!(3-2-1)!}{3!} = \frac{2 \cdot 1}{6} = \frac{1}{3}\)

\end{itemize}

\end{enumerate}

\sphinxAtStartPar
Once we have the weights for the possible subsets (or coalitions), we can \sphinxstylestrong{sum up the contributions}:
\label{equation:notebooks/shapley:8da8daaa-5418-4bd5-b9fb-8da3019ea58b}\begin{equation}
    \phi_A(f) = \frac{1}{3} \left[ f(\{A\}) - f(\{\}) \right] + \frac{1}{6} \left[ f(\{A, B\}) - f(\{B\}) \right] + \frac{1}{6} \left[ f(\{A, C\}) - f(\{C\}) \right] + \frac{1}{3} \left[ f(\{A, B, C\}) - f(\{B, C\}) \right]
\end{equation}
\sphinxAtStartPar
The Shapley value \(\phi_A(f)\) represents the fair share of revenue that power plant \(A\) should receive based on its contribution to the total revenue generated, considering all possible combinations of power plants. By applying this process, we can similarly calculate the Shapley values for power plants \(B\) and \(C\), ensuring that each plant receives a fair share of the revenue based on its contribution to the overall electricity generation.


\section{Connection between Shapley values and SHAP}
\label{\detokenize{notebooks/shapley:connection-between-shapley-values-and-shap}}
\sphinxAtStartPar
Shapley values come from cooperative game theory and provide a way to fairly distribute the total gain (e.g., revenue) among players (e.g., power plants). The Shapley value for each player (feature) is calculated based on the average marginal contribution of that player across all possible subsets of players. Instead, SHAP adapts Shapley values to explain the output of machine learning models. It provides a way to understand how each feature contributes to an individual prediction. SHAP uses the same principles as Shapley values but applies them in the context of model predictions. We just have to consider the prediction of a machine learning model as the “total gain” in a cooperative game, and the features of the model are the “players” contributing to the prediction.


\section{Practical SHAP implementation}
\label{\detokenize{notebooks/shapley:practical-shap-implementation}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Model training}: train your machine learning model on your data.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{SHAP explainer}: use a SHAP explainer to compute SHAP values for the trained model. SHAP explainers are designed to efficiently compute the Shapley values for machine learning models. The SHAP library provides different types of explainers such as KernelExplainer, TreeExplainer, and DeepExplainer, depending on the model type.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Compute SHAP values}: for a given instance \(x\), compute the SHAP values for all features. These values indicate the contribution of each feature to the prediction.

\end{enumerate}

\sphinxAtStartPar
Let’s now \sphinxstylestrong{generate some data} to apply SHAP, as we did for PDPs and ALEs.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}


\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{temperature} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{40}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Temperature in Celsius}
\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{uniform}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{24}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Time of day in hours}
\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{7}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Day of the week (0=Sunday, 6=Saturday)}

\PYG{n}{price} \PYG{o}{=} \PYG{p}{(}\PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{p}{(}\PYG{n}{temperature} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{20}\PYG{p}{)}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2} \PYG{o}{+}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{6}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{+}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{4}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{18}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{22}\PYG{p}{)} \PYG{o}{+}
         \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{5} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{7}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZgt{}}\PYG{o}{=} \PYG{l+m+mi}{10}\PYG{p}{)} \PYG{o}{*} \PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day} \PYG{o}{\PYGZlt{}}\PYG{o}{=} \PYG{l+m+mi}{17}\PYG{p}{)} \PYG{o}{+}
         \PYG{l+m+mi}{7} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sin}\PYG{p}{(}\PYG{p}{(}\PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{)} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{pi} \PYG{o}{/} \PYG{l+m+mi}{24}\PYG{p}{)} \PYG{o}{+}
         \PYG{l+m+mi}{5} \PYG{o}{*} \PYG{p}{(}\PYG{n}{day\PYGZus{}of\PYGZus{}week} \PYG{o}{==} \PYG{l+m+mi}{0}\PYG{p}{)} \PYG{o}{+}
         \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n\PYGZus{}samples}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mi}{2}\PYG{p}{)}

\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{temperature}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{time\PYGZus{}of\PYGZus{}day}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{day\PYGZus{}of\PYGZus{}week}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{price}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{n}{data}\PYG{o}{.}\PYG{n}{head}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
   Temperature       Hour  Day     Price
0    21.952540  14.229126    3 \PYGZhy{}2.562450
1    28.607575   0.241529    3  7.237870
2    24.110535  11.419829    3  5.621266
3    21.795327  17.010489    5  6.220472
4    16.946192   1.055410    4 \PYGZhy{}2.435281
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can then train a random forest model on the data we just generated and then apply SHAP to explain its predictions.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{ensemble} \PYG{k+kn}{import} \PYG{n}{RandomForestRegressor}

\PYG{c+c1}{\PYGZsh{} Split data into features and target}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}
\PYG{n}{y} \PYG{o}{=} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Price}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Train a Random Forest Regressor}
\PYG{n}{rf} \PYG{o}{=} \PYG{n}{RandomForestRegressor}\PYG{p}{(}\PYG{n}{n\PYGZus{}estimators}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{random\PYGZus{}state}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{rf}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{y}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
RandomForestRegressor(random\PYGZus{}state=0)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can implement SHAP for the random forest using the \sphinxcode{\sphinxupquote{TreeExplainer}} function from the \sphinxcode{\sphinxupquote{shap}} library:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{shap}

\PYG{c+c1}{\PYGZsh{} Use SHAP to explain the model predictions}
\PYG{n}{explainer} \PYG{o}{=} \PYG{n}{shap}\PYG{o}{.}\PYG{n}{TreeExplainer}\PYG{p}{(}\PYG{n}{rf}\PYG{p}{)}
\PYG{n}{shap\PYGZus{}values} \PYG{o}{=} \PYG{n}{explainer}\PYG{o}{.}\PYG{n}{shap\PYGZus{}values}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, we can visualize the results using the \sphinxstylestrong{summary plot}, which shows the distribution of Shapley values for all features across the dataset.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Summary plot for all instances}
\PYG{n}{shap}\PYG{o}{.}\PYG{n}{summary\PYGZus{}plot}\PYG{p}{(}\PYG{n}{shap\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{plot\PYGZus{}type}\PYG{o}{=}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{dot}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{49cdf2bf341b2183462e4af9eb9f2e3d78b7cd2ade136a99e1604c6b72fafc6a}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The \sphinxstylestrong{dependence plot} highlights how a specific feature (e.g., Temperature) influences the prediction.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dependence plot for a specific feature without interaction feature}
\PYG{n}{shap}\PYG{o}{.}\PYG{n}{dependence\PYGZus{}plot}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{shap\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{interaction\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{7f7c3e4daeb5c19eb41fd68333ae66b2aed862dc3ee8c240ffb535a17317b6df}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can also see the same plot including the interaction between Temperature and Hour:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dependence plot for a specific feature with interaction}
\PYG{n}{shap}\PYG{o}{.}\PYG{n}{dependence\PYGZus{}plot}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{shap\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{interaction\PYGZus{}index}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{7c01924bbdd0c336acc123b3c461c66ac8438c4a5ab85a55dcee6acd80224202}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Then, we can also plot the dependence plots for Hour and Day, to see if SHAP is able to unveil the daily and weekly profiles.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dependence plot for a specific feature}
\PYG{n}{shap}\PYG{o}{.}\PYG{n}{dependence\PYGZus{}plot}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Hour}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{shap\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{interaction\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{7bba15c23528beb3bbdebf45f11902e196da09934401d0e75155469f726c60bd}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Dependence plot for a specific feature}
\PYG{n}{shap}\PYG{o}{.}\PYG{n}{dependence\PYGZus{}plot}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Day}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{shap\PYGZus{}values}\PYG{p}{,} \PYG{n}{X}\PYG{p}{,} \PYG{n}{interaction\PYGZus{}index}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{192c5294c756ded7347572f49657ce855a3b593b5307a3868c742caa1c2c6bef}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Shapley values offer a robust method for interpreting machine learning models by providing a fair and comprehensive attribution of feature contributions to individual predictions. In the context of electricity markets, this can help stakeholders understand the impact of various factors on electricity prices and make more informed decisions.

\sphinxstepscope


\part{V. Experiments and Data Collection}

\sphinxstepscope


\chapter{Overview}
\label{\detokenize{notebooks/preface_designs:overview}}\label{\detokenize{notebooks/preface_designs::doc}}
\sphinxAtStartPar
Collecting relevant data is always the first step for a good statistical analysis. In this context, a designed experiment refers to the structured and methodical approach to planning, conducting, analysing, and interpreting controlled experiments. This part covers techniques for setting up experiments to ensure that the results are valid, reliable, and can be used to draw causal conclusions. Effective experimental design is key to deriving actionable insights and making evidence\sphinxhyphen{}based decisions for several reasons:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Validity}: ensures that the experiments measure what they are intended to measure, providing accurate results.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Reliability}: guarantees that the results are consistent and replicable across different trials and settings.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Causality}: facilitates the identification of causal relationships by controlling for confounding variables and ensuring that the observed effects are due to the treatments applied.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Efficiency}: optimizes the use of resources, minimizing the cost and time required to conduct experiments while maximizing the information gained.

\end{itemize}

\sphinxAtStartPar
Poor experimental design can lead to several issues:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Bias}: without proper control and randomization, experiments can produce biased results, leading to incorrect conclusions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Confounding}: failing to account for confounding variables can obscure the true relationship between the treatment and the outcome.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Lack of generalizability}: experiments that are not well\sphinxhyphen{}designed may produce results that cannot be generalized to other settings, populations, or time periods.

\end{itemize}


\section{Content of Data Collection chapters}
\label{\detokenize{notebooks/preface_designs:content-of-data-collection-chapters}}

\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Chapter
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Description
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
Design of Experiments
&
\sphinxAtStartPar
Techniques for planning and conducting controlled experiments to draw causal conclusions.
\\
\sphinxhline
\sphinxAtStartPar
Active Learning
&
\sphinxAtStartPar
Methods for sequentially selecting data points to be labeled in a way that improves model performance.
\\
\sphinxhline
\sphinxAtStartPar
A/B Testing
&
\sphinxAtStartPar
Practical applications of A/B testing to compare two versions of a variable to determine which performs better.
\\
\sphinxhline
\sphinxAtStartPar
Multi\sphinxhyphen{}Armed Bandits
&
\sphinxAtStartPar
Approaches to balance exploration and exploitation in experimental settings to optimize decision\sphinxhyphen{}making.
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxstepscope


\chapter{A/B Testing}
\label{\detokenize{notebooks/AB_testing:a-b-testing}}\label{\detokenize{notebooks/AB_testing::doc}}
\sphinxAtStartPar
A/B testing is one of the most commonly employed experimental framework. It is a powerful method for comparing two versions of a variable to determine which one performs better in terms of a given metric. This technique is widely used in various fields, including marketing, product development, and website optimization, to make data\sphinxhyphen{}driven decisions. In this tutorial, we’ll explore the basics of A/B testing, illustrate a simple example, and delve into more advanced topics, such as network effects and complex statistical considerations.


\section{A simple example}
\label{\detokenize{notebooks/AB_testing:a-simple-example}}
\sphinxAtStartPar
Consider a scenario where we want to compare the effectiveness of two different marketing campaigns aimed at encouraging customers to use more energy during off\sphinxhyphen{}peak hours. We’ll define two groups:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Group A (control group)}: receives the standard marketing message.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Group B (treatment group)}: receives a new, enhanced marketing message.

\end{itemize}

\sphinxAtStartPar
The goal is to determine which marketing message results in higher usage of energy during off\sphinxhyphen{}peak hours.

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step procedure:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Random assignment}: randomly assign customers to either Group A or Group B to ensure that each group is representative of the overall population.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Implementation}: send the standard marketing message to Group A, and the enhanced marketing message to Group B.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Measurement}: after a predetermined period, measure the amount of energy used by customers in both groups during off\sphinxhyphen{}peak hours.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Analysis}: compare the average energy usage between the two groups using statistical tests, such as a \sphinxstylestrong{t\sphinxhyphen{}test}, to determine if the difference is statistically significant.

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{from} \PYG{n+nn}{scipy} \PYG{k+kn}{import} \PYG{n}{stats}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{group\PYGZus{}a} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Control group}
\PYG{n}{group\PYGZus{}b} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{loc}\PYG{o}{=}\PYG{l+m+mi}{55}\PYG{p}{,} \PYG{n}{scale}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Treatment group}

\PYG{c+c1}{\PYGZsh{} Perform t\PYGZhy{}test}
\PYG{n}{t\PYGZus{}stat}\PYG{p}{,} \PYG{n}{p\PYGZus{}value} \PYG{o}{=} \PYG{n}{stats}\PYG{o}{.}\PYG{n}{ttest\PYGZus{}ind}\PYG{p}{(}\PYG{n}{group\PYGZus{}b}\PYG{p}{,} \PYG{n}{group\PYGZus{}a}\PYG{p}{)}

\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{T\PYGZhy{}statistic: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{t\PYGZus{}stat}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{P\PYGZhy{}value: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}value}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Interpretation}
\PYG{k}{if} \PYG{n}{p\PYGZus{}value} \PYG{o}{\PYGZlt{}} \PYG{l+m+mf}{0.05}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{The enhanced marketing message significantly increases energy usage during off\PYGZhy{}peak hours.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{k}{else}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{There is no significant difference between the two marketing messages.}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
T\PYGZhy{}statistic: 4.755
P\PYGZhy{}value: 0.000
The enhanced marketing message significantly increases energy usage during off\PYGZhy{}peak hours.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\section{Statistical considerations}
\label{\detokenize{notebooks/AB_testing:statistical-considerations}}
\sphinxAtStartPar
A/B testing involves several statistical concepts that needs to be taken into account to ensure valid and reliable results. These includes:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Power analysis}: to determine the sample size needed to detect a significant effect.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Multiple testing}: to adjust for the increased likelihood of Type I errors when conducting multiple comparisons (e.g., using Bonferroni correction).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Heterogeneous treatment effects}: to explore how treatment effects vary across different subgroups.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Network effects}: to eestimate treatment effects in networks where there is interference between users or consumers.

\end{enumerate}


\subsection{Power analysis}
\label{\detokenize{notebooks/AB_testing:power-analysis}}
\sphinxAtStartPar
Here we demonstrate how to determine the minimum number of participants needed in each group of an A/B test to ensure reliable results. This process, called power analysis, helps us calculate the sample size required to detect a meaningful difference between two groups with a high probability. Power analysis is a technique used to figure out the smallest sample size that can reliably detect an effect in an experiment. It balances the need to find real differences with the desire to avoid wasting resources on excessively large samples. Essentially, it helps ensure that our A/B test is both effective and efficient.

\sphinxAtStartPar
\sphinxstylestrong{Key parameters:}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Effect size}: this measures the expected difference between the groups. It is a standardized value that reflects how large an effect we expect the treatment to have. A common way to select the effect size is to use previous research or pilot studies to estimate the likely impact. In our example, we chose an effect size of 0.5, indicating a moderate difference that we want to detect. The effect size is a measure of the magnitude of the difference between two groups. It is not just the raw difference between the means of the two groups, but rather a standardised measure that accounts for the variability within the data. This allows for a more meaningful comparison across different contexts and studies.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Significance level (alpha)}: this is the threshold for determining statistical significance, often set at 0.05. It represents the probability of rejecting the null hypothesis (concluding there is an effect) when it is actually true (a false positive). We selected an alpha of 0.05, which is standard in many fields, meaning we are willing to accept a 5\% chance of a false positive.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Power}: this represents the probability of correctly rejecting the null hypothesis when it is false, i.e., detecting a true effect. A common target for power is 0.8, or 80\%, meaning we want an 80\% chance of detecting the effect if it truly exists. This balance ensures that we have a high likelihood of finding true effects without requiring excessively large sample sizes.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{power} \PYG{k+kn}{import} \PYG{n}{TTestIndPower}

\PYG{c+c1}{\PYGZsh{} Parameters}
\PYG{n}{effect\PYGZus{}size} \PYG{o}{=} \PYG{l+m+mf}{0.5}
\PYG{n}{alpha} \PYG{o}{=} \PYG{l+m+mf}{0.05}
\PYG{n}{power} \PYG{o}{=} \PYG{l+m+mf}{0.8}

\PYG{c+c1}{\PYGZsh{} Calculate required sample size}
\PYG{n}{analysis} \PYG{o}{=} \PYG{n}{TTestIndPower}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{sample\PYGZus{}size} \PYG{o}{=} \PYG{n}{analysis}\PYG{o}{.}\PYG{n}{solve\PYGZus{}power}\PYG{p}{(}\PYG{n}{effect\PYGZus{}size}\PYG{o}{=}\PYG{n}{effect\PYGZus{}size}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{n}{alpha}\PYG{p}{,} \PYG{n}{power}\PYG{o}{=}\PYG{n}{power}\PYG{p}{,} \PYG{n}{alternative}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{two\PYGZhy{}sided}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Required sample size per group: }\PYG{l+s+si}{\PYGZob{}}\PYG{n+nb}{int}\PYG{p}{(}\PYG{n}{sample\PYGZus{}size}\PYG{p}{)}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Required sample size per group: 63
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
After running the power analysis with these parameters, the output tells us the required sample size per group. For instance, if the result is 63, it means we need at least 63 participants in both the treatment and control groups. This ensures that our A/B test has enough power to detect a moderate effect with an 80\% chance, while keeping the probability of a false positive at 5\%. This helps in making sure our experiment is well\sphinxhyphen{}designed and our conclusions are reliable.


\subsection{Multiple testing}
\label{\detokenize{notebooks/AB_testing:multiple-testing}}
\sphinxAtStartPar
When conducting multiple tests, the probability of making at least one type I error (false positive) increases. To address this, we use the Bonferroni correction, which adjusts the significance level to control the family\sphinxhyphen{}wise error rate (FWER). The adjusted significance level is given by:
\label{equation:notebooks/AB_testing:501a1be6-906d-4fd6-a5f8-2cb51abd07c2}\begin{equation}\alpha_{\text{adj}} = \frac{\alpha}{m}\end{equation}
\sphinxAtStartPar
where \(\alpha\) is the original significance level, and \(m\) is the number of tests.

\sphinxAtStartPar
Let’s consider an example where a utility company is testing five different marketing campaigns to see if they significantly increase the adoption of a new energy\sphinxhyphen{}saving device.

\sphinxAtStartPar
\sphinxstylestrong{Scenario}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The company runs five separate A/B tests, one for each marketing campaign.

\item {} 
\sphinxAtStartPar
The null hypothesis for each test is that the campaign has no effect on adoption rates.

\item {} 
\sphinxAtStartPar
The significance level for the tests is initially set at 0.05.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step procedure:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Set parameters}: original significance level ((\textbackslash{}alpha)) and number of tests.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Simulate p\sphinxhyphen{}values}: generate p\sphinxhyphen{}values from the five tests. In a realistic scenario, these p\sphinxhyphen{}values would come from actual data analysis.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Apply Bonferroni correction}: adjust the significance level using the Bonferroni method, and determine which hypotheses to reject based on the corrected p\sphinxhyphen{}values.

\end{enumerate}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{stats}\PYG{n+nn}{.}\PYG{n+nn}{multitest} \PYG{k+kn}{import} \PYG{n}{multipletests}

\PYG{c+c1}{\PYGZsh{} Parameters}
\PYG{n}{alpha} \PYG{o}{=} \PYG{l+m+mf}{0.05}  \PYG{c+c1}{\PYGZsh{} Original significance level}
\PYG{n}{num\PYGZus{}tests} \PYG{o}{=} \PYG{l+m+mi}{5}  \PYG{c+c1}{\PYGZsh{} Number of tests}

\PYG{c+c1}{\PYGZsh{} Adjusted significance level using Bonferroni correction}
\PYG{n}{alpha\PYGZus{}adj} \PYG{o}{=} \PYG{n}{alpha} \PYG{o}{/} \PYG{n}{num\PYGZus{}tests}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Adjusted significance level: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{alpha\PYGZus{}adj}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Simulate p\PYGZhy{}values from 5 tests}
\PYG{c+c1}{\PYGZsh{} Realistically, these p\PYGZhy{}values would be the result of statistical tests on actual data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{p\PYGZus{}values} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{0.02}\PYG{p}{,} \PYG{l+m+mf}{0.04}\PYG{p}{,} \PYG{l+m+mf}{0.20}\PYG{p}{,} \PYG{l+m+mf}{0.01}\PYG{p}{,} \PYG{l+m+mf}{0.03}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Simulated p\PYGZhy{}values for illustration}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Original p\PYGZhy{}values: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{p\PYGZus{}values}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Apply Bonferroni correction}
\PYG{n}{reject}\PYG{p}{,} \PYG{n}{pvals\PYGZus{}corrected}\PYG{p}{,} \PYG{n}{\PYGZus{}}\PYG{p}{,} \PYG{n}{\PYGZus{}} \PYG{o}{=} \PYG{n}{multipletests}\PYG{p}{(}\PYG{n}{p\PYGZus{}values}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{n}{alpha}\PYG{p}{,} \PYG{n}{method}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{bonferroni}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Corrected p\PYGZhy{}values: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pvals\PYGZus{}corrected}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Reject null hypothesis: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{reject}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Interpretation of results}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{p}{(}\PYG{n}{pval}\PYG{p}{,} \PYG{n}{corr\PYGZus{}pval}\PYG{p}{,} \PYG{n}{rej}\PYG{p}{)} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n+nb}{zip}\PYG{p}{(}\PYG{n}{p\PYGZus{}values}\PYG{p}{,} \PYG{n}{pvals\PYGZus{}corrected}\PYG{p}{,} \PYG{n}{reject}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Test }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: Original p\PYGZhy{}value = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{pval}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Corrected p\PYGZhy{}value = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{corr\PYGZus{}pval}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{, Reject null hypothesis = }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{rej}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Adjusted significance level: 0.01
Original p\PYGZhy{}values: [0.02 0.04 0.2  0.01 0.03]
Corrected p\PYGZhy{}values: [0.1  0.2  1.   0.05 0.15]
Reject null hypothesis: [False False False  True False]
Test 1: Original p\PYGZhy{}value = 0.02, Corrected p\PYGZhy{}value = 0.1, Reject null hypothesis = False
Test 2: Original p\PYGZhy{}value = 0.04, Corrected p\PYGZhy{}value = 0.2, Reject null hypothesis = False
Test 3: Original p\PYGZhy{}value = 0.2, Corrected p\PYGZhy{}value = 1.0, Reject null hypothesis = False
Test 4: Original p\PYGZhy{}value = 0.01, Corrected p\PYGZhy{}value = 0.05, Reject null hypothesis = True
Test 5: Original p\PYGZhy{}value = 0.03, Corrected p\PYGZhy{}value = 0.15, Reject null hypothesis = False
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
This correction ensures that we control the family\sphinxhyphen{}wise error rate, reducing the risk of false positives when conducting multiple tests. Here we used fictional p\sphinxhyphen{}values: \sphinxcode{\sphinxupquote{{[}0.02, 0.04, 0.20, 0.01, 0.03{]}}}. In practice, these would be obtained from statistical tests on actual experimental data.


\subsection{Heterogeneous treatment effects}
\label{\detokenize{notebooks/AB_testing:heterogeneous-treatment-effects}}
\sphinxAtStartPar
Heterogeneous treatment effects occur when the effect of a treatment varies across different subgroups of the population. Identifying these differences is crucial for understanding how different groups respond to the treatment and for making targeted decisions.

\sphinxAtStartPar
Suppose an electricity company is testing a new marketing campaign to encourage customers to switch to a green energy plan. The company wants to understand if the campaign’s effectiveness varies by customer age. Let’s simulate this case with a simple example, where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
We generate data for 1000 customers with ages between 18 and 70.

\item {} 
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{treatment}} variable indicates whether the customer received the marketing campaign (1) or not (0).

\item {} 
\sphinxAtStartPar
The \sphinxcode{\sphinxupquote{response}} variable represents whether the customer switched to the green energy plan, influenced by their age, the treatment, and the interaction between age and treatment.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k}{as} \PYG{n+nn}{sm}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{n} \PYG{o}{=} \PYG{l+m+mi}{1000}
\PYG{n}{age} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randint}\PYG{p}{(}\PYG{l+m+mi}{18}\PYG{p}{,} \PYG{l+m+mi}{70}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{treatment} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}
\PYG{n}{age\PYGZus{}treatment} \PYG{o}{=} \PYG{n}{age} \PYG{o}{*} \PYG{n}{treatment}
\PYG{n}{response} \PYG{o}{=} \PYG{l+m+mi}{5} \PYG{o}{+} \PYG{l+m+mi}{3} \PYG{o}{*} \PYG{n}{treatment} \PYG{o}{+} \PYG{l+m+mf}{0.1} \PYG{o}{*} \PYG{n}{age} \PYG{o}{+} \PYG{l+m+mf}{0.05} \PYG{o}{*} \PYG{n}{age\PYGZus{}treatment} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{n}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create DataFrame}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{age}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{treatment}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{response}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age\PYGZus{}treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{n}{age\PYGZus{}treatment}\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit the model}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age\PYGZus{}treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot interaction}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{viridis}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Age}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response by Age and Treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{colorbar}\PYG{p}{(}\PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Treatment}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:               response   R\PYGZhy{}squared:                       0.912
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.912
Method:                 Least Squares   F\PYGZhy{}statistic:                     3446.
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):               0.00
Time:                        19:09:38   Log\PYGZhy{}Likelihood:                \PYGZhy{}1409.1
No. Observations:                1000   AIC:                             2826.
Df Residuals:                     996   BIC:                             2846.
Df Model:                           3                                         
Covariance Type:            nonrobust                                         
=================================================================================
                    coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const             5.2533      0.135     38.770      0.000       4.987       5.519
treatment         2.5726      0.194     13.260      0.000       2.192       2.953
age               0.0967      0.003     33.110      0.000       0.091       0.102
age\PYGZus{}treatment     0.0584      0.004     13.948      0.000       0.050       0.067
==============================================================================
Omnibus:                        1.105   Durbin\PYGZhy{}Watson:                   2.009
Prob(Omnibus):                  0.576   Jarque\PYGZhy{}Bera (JB):                1.138
Skew:                          \PYGZhy{}0.080   Prob(JB):                        0.566
Kurtosis:                       2.959   Cond. No.                         369.
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\noindent\sphinxincludegraphics{{4dff86fe7059baa7563ae0a1a8f5f6d231185499cebc00b757b307dea6337937}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
In the results:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The coefficient for \sphinxcode{\sphinxupquote{treatment}} indicates the average effect of the marketing campaign.

\item {} 
\sphinxAtStartPar
The coefficient for \sphinxcode{\sphinxupquote{age}} shows the effect of age on the response.

\item {} 
\sphinxAtStartPar
The coefficient for \sphinxcode{\sphinxupquote{age\_treatment}} reveals how the treatment effect changes with age. A significant coefficient suggests that the treatment effect varies across different age groups.

\end{itemize}

\sphinxAtStartPar
This example demonstrates how to estimate heterogeneous treatment effects in an A/B testing context. By including interaction terms in the regression model, we can identify how the treatment effect varies across different subgroups, providing valuable insights for tailoring strategies to specific segments. This approach helps maximize the effectiveness of interventions by understanding and leveraging the heterogeneity in treatment effects.


\subsection{Network effects}
\label{\detokenize{notebooks/AB_testing:network-effects}}
\sphinxAtStartPar
Traditional A/B testing methods, relying on the stable unit treatment value assumption (SUTVA), assume that each individual’s response to a treatment is independent of others. However, in many real\sphinxhyphen{}world scenarios, especially in social networks and interconnected systems, this assumption does not hold. Users’ behaviors and outcomes can be influenced by their peers, leading to network effects. Network A/B testing is an advanced technique that takes into account the influence of social connections and interactions when evaluating the effectiveness of treatments or interventions {[}\hyperlink{cite.bibliography:id19}{GXBH15}{]}.

\sphinxAtStartPar
\sphinxstylestrong{Relevance of network A/B testing:}
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Capturing spillover effects}: in social networks, the effect of a treatment on one individual can spill over to their connected peers. For instance, in marketing campaigns, a customer’s decision to adopt a new product might influence their friends to do the same.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Accurate estimation of treatment effects}: by considering network effects, we can obtain more accurate estimates of the Average Treatment Effect (ATE). Ignoring these effects can lead to biased results and incorrect conclusions.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimizing interventions}: understanding how treatments propagate through networks can help in designing more effective interventions. For example, targeting influential nodes in a network can amplify the overall impact of a campaign.

\end{enumerate}

\sphinxAtStartPar
In network A/B testing, we need to account for two main factors:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Direct treatment effect}: The impact of the treatment on the individual who receives it.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Indirect treatment effect}: The impact of the treatment on individuals connected to those who receive it.

\end{itemize}

\sphinxAtStartPar
To estimate these effects, we use a model that includes both the treatment status of the individual and the treatment status of their neighbors.

\sphinxAtStartPar
Consider a scenario where an electricity company wants to test a new marketing campaign to encourage customers to switch to a green energy plan. The company knows that customers are likely to influence each other’s decisions through social interactions. Therefore, they want to account for these network effects in their A/B testing. We will simulate a network of customers, randomly assign them to treatment or control groups, and then estimate the treatment effects considering the influence of their connected peers.

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step procedure:}

\sphinxAtStartPar
\sphinxstylestrong{Generating the network:} we use an Erdős\sphinxhyphen{}Rényi model to create a random network. In this model, each pair of nodes (customers) has a fixed probability of being connected.

\sphinxAtStartPar
\sphinxstylestrong{Simulating treatment assignment:} we randomly assign each node (customer) to either the treatment group (Z=1) or the control group (Z=0) with equal probability. This represents whether a customer receives the new marketing campaign or not.

\sphinxAtStartPar
\sphinxstylestrong{Simulating the response variable:} The response variable \(Y\) represents the outcome we are interested in measuring, such as whether a customer switched to the green energy plan. The outcome for each customer is influenced by their own treatment status \(Z\) and the treatment statuses of their connected neighbors, which we account for using the adjacency matrix \(A\).

\sphinxAtStartPar
The formula used for simulating the response is:
\label{equation:notebooks/AB_testing:8dac2600-cf67-4cdb-a156-4434a2050057}\begin{equation}
    Y = \beta_0 + \beta_1 Z + \beta_2 (A \cdot Z) + \epsilon
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\beta_0\) is the intercept.

\item {} 
\sphinxAtStartPar
\(\beta_1\) is the direct effect of the treatment.

\item {} 
\sphinxAtStartPar
\(\beta_2\) captures the network effects (i.e., how the treatment status of neighbors influences the outcome).

\item {} 
\sphinxAtStartPar
\(\epsilon\) is random noise.

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{networkx} \PYG{k}{as} \PYG{n+nn}{nx}

\PYG{c+c1}{\PYGZsh{} Generate a random graph}
\PYG{n}{n\PYGZus{}samples} \PYG{o}{=} \PYG{l+m+mi}{200}
\PYG{n}{G} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{erdos\PYGZus{}renyi\PYGZus{}graph}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{l+m+mf}{0.05}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{A} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{to\PYGZus{}numpy\PYGZus{}array}\PYG{p}{(}\PYG{n}{G}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Simulate data}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{Z} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{,} \PYG{n}{p}\PYG{o}{=}\PYG{p}{[}\PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{]}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Random assignment to treatment or control}
\PYG{n}{Y} \PYG{o}{=} \PYG{l+m+mi}{2} \PYG{o}{+} \PYG{l+m+mf}{1.5} \PYG{o}{*} \PYG{n}{Z} \PYG{o}{+} \PYG{l+m+mf}{0.8} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{)} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{normal}\PYG{p}{(}\PYG{n}{size}\PYG{o}{=}\PYG{n}{n\PYGZus{}samples}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Network effects}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The response \(Y\) is simulated by combining the direct effect of the treatment (\(Z\)) and the indirect network effect (\(A \cdot Z\)). The term \(\beta_2 (A \cdot Z)\) captures the influence of the treatment status of neighboring nodes on the outcome.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Plotting the network}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{8}\PYG{p}{,} \PYG{l+m+mi}{8}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{)}
\PYG{n}{pos} \PYG{o}{=} \PYG{n}{nx}\PYG{o}{.}\PYG{n}{spring\PYGZus{}layout}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{seed}\PYG{o}{=}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{node\PYGZus{}color} \PYG{o}{=} \PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{if} \PYG{n}{z} \PYG{o}{==} \PYG{l+m+mi}{1} \PYG{k}{else} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}} \PYG{k}{for} \PYG{n}{z} \PYG{o+ow}{in} \PYG{n}{Z}\PYG{p}{]}
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw\PYGZus{}networkx\PYGZus{}nodes}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,} \PYG{n}{node\PYGZus{}color}\PYG{o}{=}\PYG{n}{node\PYGZus{}color}\PYG{p}{,} \PYG{n}{node\PYGZus{}size}\PYG{o}{=}\PYG{l+m+mi}{60}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{nx}\PYG{o}{.}\PYG{n}{draw\PYGZus{}networkx\PYGZus{}edges}\PYG{p}{(}\PYG{n}{G}\PYG{p}{,} \PYG{n}{pos}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Network Visualization with Treatment Assignments (Red: Treatment, Blue: Control)}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{02afdca8b6093be64e40425e9d571b4ff0a65ff7455d51fa3c1998315a3f029d}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We now prepare the aata for fitting a linear regression model. In particular, we create the feature matrix \(X\), which includes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The treatment status \(Z\) of each customer.

\item {} 
\sphinxAtStartPar
The sum of the treatment statuses of their neighbors, calculated as \(A \cdot Z\).

\end{itemize}

\sphinxAtStartPar
Then, we can fit a linear regression model to estimate the coefficients \(\beta_1\) and \(\beta_2\), which correspond to the direct treatment effect and the network effect, respectively.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Prepare the data for regression}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{column\PYGZus{}stack}\PYG{p}{(}\PYG{p}{[}\PYG{n}{Z}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{dot}\PYG{p}{(}\PYG{n}{A}\PYG{p}{,} \PYG{n}{Z}\PYG{p}{)}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Add intercept}

\PYG{c+c1}{\PYGZsh{} Fit the model using statsmodels}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{sm}\PYG{o}{.}\PYG{n}{OLS}\PYG{p}{(}\PYG{n}{Y}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{n}{model}\PYG{o}{.}\PYG{n}{summary}\PYG{p}{(}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
                            OLS Regression Results                            
==============================================================================
Dep. Variable:                      y   R\PYGZhy{}squared:                       0.746
Model:                            OLS   Adj. R\PYGZhy{}squared:                  0.744
Method:                 Least Squares   F\PYGZhy{}statistic:                     290.0
Date:                Sun, 07 Jul 2024   Prob (F\PYGZhy{}statistic):           1.98e\PYGZhy{}59
Time:                        19:09:39   Log\PYGZhy{}Likelihood:                \PYGZhy{}276.44
No. Observations:                 200   AIC:                             558.9
Df Residuals:                     197   BIC:                             568.8
Df Model:                           2                                         
Covariance Type:            nonrobust                                         
==============================================================================
                 coef    std err          t      P\PYGZgt{}|t|      [0.025      0.975]
\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}
const          2.1994      0.191     11.502      0.000       1.822       2.577
x1             1.5484      0.138     11.249      0.000       1.277       1.820
x2             0.7663      0.035     21.954      0.000       0.697       0.835
==============================================================================
Omnibus:                        9.767   Durbin\PYGZhy{}Watson:                   2.140
Prob(Omnibus):                  0.008   Jarque\PYGZhy{}Bera (JB):               13.614
Skew:                           0.315   Prob(JB):                      0.00111
Kurtosis:                       4.112   Cond. No.                         15.3
==============================================================================

Notes:
[1] Standard Errors assume that the covariance matrix of the errors is correctly specified.
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The model summary provides the estimates for \(\beta_1\) (direct treatment effect) and \(\beta_2\) (network effect).
The estimated Average Treatment Effect (ATE) considering network effects is extracted from the coefficient \(\beta_1\).

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate ATE considering network effects}
\PYG{n}{ATE} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{params}\PYG{p}{[}\PYG{l+m+mi}{1}\PYG{p}{]}  \PYG{c+c1}{\PYGZsh{} Coefficient for treatment}
\PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{ATE considering network effects: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{ATE}\PYG{l+s+si}{:}\PYG{l+s+s2}{.3f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
ATE considering network effects: 1.548
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
By simulating the response variable to include both direct and network effects, and then using regression analysis to estimate these effects, we can accurately measure the influence of treatments in interconnected systems. This approach helps account for the spillover effects that are common in social networks and other interconnected environments.

\sphinxstepscope


\chapter{Multi\sphinxhyphen{}Armed Bandits}
\label{\detokenize{notebooks/bandits:multi-armed-bandits}}\label{\detokenize{notebooks/bandits::doc}}
\sphinxAtStartPar
Multi\sphinxhyphen{}armed bandit (MAB) problems are a class of sequential decision\sphinxhyphen{}making problems that model the trade\sphinxhyphen{}off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one\sphinxhyphen{}armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls.

\sphinxAtStartPar
The MAB problem can be formalized as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Arms}: let \(K\) be the number of arms (choices or actions) available.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Rewards}: each arm \(k \in \{1, 2, \ldots, K\}\) provides a reward \(r_k(t)\) at time \(t\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Objective}: the objective is to maximize the cumulative reward over \(T\) rounds, \(\sum_{t=1}^{T} r_k(t)\).

\end{itemize}

\sphinxAtStartPar
Imagine you are a decision\sphinxhyphen{}maker in an electricity market, tasked with optimizing various strategies such as dynamic pricing, demand response programs, or marketing campaigns. Each strategy can be thought of as a slot machine (arm), each with an unknown probability of success (reward). You have a limited budget and need to decide how to allocate resources across these strategies to maximize your overall reward. The dilemma is that the more you explore different strategies to learn their effectiveness, the less you have left to exploit the best\sphinxhyphen{}performing strategy. This trade\sphinxhyphen{}off between exploration (trying different options to gather information) and exploitation (using known information to maximize reward) lies at the heart of the multi\sphinxhyphen{}armed bandit (MAB) problem.

\sphinxAtStartPar
\sphinxstylestrong{Practical applications} of MAB problems in electricity markets might include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic pricing}: suppose you need to determine the optimal pricing strategy for electricity during peak and off\sphinxhyphen{}peak hours. You could implement different pricing models (arms) and observe consumer reactions (rewards). Using MAB, you can dynamically adjust the pricing strategies based on observed data to maximize revenue without running prolonged inefficient experiments.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Demand response programmes}: consider various incentive schemes to encourage consumers to reduce usage during peak times. Each scheme can be tested (explored) initially, and based on which ones yield the highest reductions in usage (exploitation), more resources can be allocated to the most effective programs.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Marketing campaigns}: you may have multiple marketing strategies to promote energy\sphinxhyphen{}efficient appliances. Initially, you allocate equal resources to all strategies to see which performs best. As data comes in, you shift more resources to the campaigns that show higher engagement and conversion rates, optimizing your overall marketing budget.

\end{enumerate}


\section{MAB vs. A/B testing}
\label{\detokenize{notebooks/bandits:mab-vs-a-b-testing}}
\sphinxAtStartPar
Traditional methods for testing different options, such as A/B testing, involve splitting resources equally across different strategies (pure exploration), but this can be inefficient and costly, as it doesn’t adapt to the observed performance of the strategies. To this extent, the key \sphinxstylestrong{limitations of A/B testing} are:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Resource inefficiency}: equally distributing resources among all strategies can waste time and opportunities on less\sphinxhyphen{}performing options.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Costly}: every test interaction involves costs related to market operations, consumer interactions, and potential financial impacts.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Non\sphinxhyphen{}personalized}: A/B testing typically identifies a winner for the majority, which may not be optimal for all segments of the market.

\end{itemize}

\sphinxAtStartPar
On the other side, the \sphinxstylestrong{advantages of MAB approaches} include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Dynamic allocation}: MAB algorithms initially explore all options but gradually allocate more resources to the best\sphinxhyphen{}performing strategies, improving overall efficiency.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Higher success rates}: by continuously adapting to performance data, MAB approaches can increase the overall success rate of the strategies implemented.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Contextual personalization}: advanced MAB variants like contextual bandits tailor strategies to different market segments, enhancing personalization and engagement.

\end{itemize}


\section{Key concepts and high\sphinxhyphen{}level example}
\label{\detokenize{notebooks/bandits:key-concepts-and-high-level-example}}
\sphinxAtStartPar
The two key conepts in MAB problems are:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exploration vs. exploitation}: where \sphinxstylestrong{exploration} means trying out different arms to gather more information about their rewards, and \sphinxstylestrong{exploitation} refers to choosing the arm that is currently believed to provide the highest reward.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regret}: the difference between the reward obtained by the optimal arm and the reward obtained by the algorithm. The goal is to minimize regret over time.

\end{enumerate}

\sphinxAtStartPar
Let’s break down the MAB problem step by step, focusing on the concepts of exploration, reward, and how they are computed.

\sphinxAtStartPar
\sphinxstylestrong{Step\sphinxhyphen{}by\sphinxhyphen{}step illustration}

\sphinxAtStartPar
\sphinxstylestrong{Setup}

\sphinxAtStartPar
Imagine you have three slot machines (arms) in a casino, each with an unknown probability of payout (reward). Initially, you do not know which machine is the best, so you need to \sphinxstylestrong{explore} by trying out each machine. Over time, as you gather more information, you start to get an idea about the different machines and can \sphinxstylestrong{exploit} the available information by choosing the machine that seems to give the highest reward based on your observations.

\sphinxAtStartPar
\sphinxstylestrong{True reward probabilities}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Slot Machine 1: 0.3

\item {} 
\sphinxAtStartPar
Slot Machine 2: 0.5

\item {} 
\sphinxAtStartPar
Slot Machine 3: 0.7

\end{itemize}

\sphinxAtStartPar
These probabilities are unknown to you. Your goal is to find out which machine has the highest probability of giving a payout by trying them out.

\sphinxAtStartPar
\sphinxstylestrong{Step 1: initial exploration}

\sphinxAtStartPar
To start, you need to try each machine a few times to get an idea of their payouts.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} True reward probabilities}
\PYG{n}{true\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.3}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.7}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Number of times to pull each machine initially}
\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{=} \PYG{l+m+mi}{10}

\PYG{c+c1}{\PYGZsh{} Simulate initial exploration}
\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{l+m+mi}{42}\PYG{p}{)}
\PYG{n}{initial\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

\PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{true\PYGZus{}means}\PYG{p}{)}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{initial\PYGZus{}pulls}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{true\PYGZus{}means}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{k}{else} \PYG{l+m+mi}{0}
        \PYG{n}{rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
    \PYG{n}{initial\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Print initial rewards}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{rewards} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{initial\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Rewards for Machine }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{rewards}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Rewards for Machine 1: [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]
Rewards for Machine 2: [1, 0, 0, 1, 1, 1, 1, 0, 1, 1]
Rewards for Machine 3: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{Step 2: compute average rewards}

\sphinxAtStartPar
After the initial exploration, compute the average reward for each machine to get an estimate of their payout probabilities.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate average rewards}
\PYG{n}{average\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)} \PYG{k}{for} \PYG{n}{rewards} \PYG{o+ow}{in} \PYG{n}{initial\PYGZus{}rewards}\PYG{p}{]}

\PYG{c+c1}{\PYGZsh{} Print average rewards}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{avg} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{average\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Average reward for Machine }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{avg}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Average reward for Machine 1: 0.30
Average reward for Machine 2: 0.70
Average reward for Machine 3: 0.90
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The mean of the rewards obtained from the initial pulls for each machine. This gives an estimate of the payout probability for each machine.

\sphinxAtStartPar
\sphinxstylestrong{Step 3: exploitation}

\sphinxAtStartPar
Based on the average rewards, you start choosing the machine that seems to give the highest reward more often.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Number of additional rounds}
\PYG{n}{additional\PYGZus{}rounds} \PYG{o}{=} \PYG{l+m+mi}{70}

\PYG{c+c1}{\PYGZsh{} Continue simulation}
\PYG{n}{total\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{initial\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{cumulative\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{*} \PYG{l+m+mi}{3} \PYG{o}{+} \PYG{n}{additional\PYGZus{}rounds}\PYG{p}{)}
\PYG{n}{cumulative\PYGZus{}rewards}\PYG{p}{[}\PYG{p}{:}\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{*} \PYG{l+m+mi}{3}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{n+nb}{sum}\PYG{p}{(}\PYG{n}{rewards}\PYG{p}{)} \PYG{k}{for} \PYG{n}{rewards} \PYG{o+ow}{in} \PYG{n}{initial\PYGZus{}rewards}\PYG{p}{]}\PYG{p}{)}\PYG{o}{.}\PYG{n}{repeat}\PYG{p}{(}\PYG{n}{initial\PYGZus{}pulls}\PYG{p}{)}

\PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{additional\PYGZus{}rounds}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Choose the machine with the highest average reward}
    \PYG{n}{best\PYGZus{}machine} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{average\PYGZus{}rewards}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Simulate pulling the best machine}
    \PYG{n}{reward} \PYG{o}{=} \PYG{l+m+mi}{1} \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{true\PYGZus{}means}\PYG{p}{[}\PYG{n}{best\PYGZus{}machine}\PYG{p}{]} \PYG{k}{else} \PYG{l+m+mi}{0}
    \PYG{n}{total\PYGZus{}rewards}\PYG{p}{[}\PYG{n}{best\PYGZus{}machine}\PYG{p}{]}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Update average rewards}
    \PYG{n}{average\PYGZus{}rewards}\PYG{p}{[}\PYG{n}{best\PYGZus{}machine}\PYG{p}{]} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{total\PYGZus{}rewards}\PYG{p}{[}\PYG{n}{best\PYGZus{}machine}\PYG{p}{]}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Update cumulative rewards}
    \PYG{n}{cumulative\PYGZus{}rewards}\PYG{p}{[}\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{*} \PYG{l+m+mi}{3} \PYG{o}{+} \PYG{n}{t}\PYG{p}{]} \PYG{o}{=} \PYG{n}{cumulative\PYGZus{}rewards}\PYG{p}{[}\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{*} \PYG{l+m+mi}{3} \PYG{o}{+} \PYG{n}{t} \PYG{o}{\PYGZhy{}} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{+} \PYG{n}{reward}

\PYG{c+c1}{\PYGZsh{} Print final average rewards}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{avg} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{average\PYGZus{}rewards}\PYG{p}{)}\PYG{p}{:}
    \PYG{n+nb}{print}\PYG{p}{(}\PYG{l+s+sa}{f}\PYG{l+s+s2}{\PYGZdq{}}\PYG{l+s+s2}{Final average reward for Machine }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{i}\PYG{+w}{ }\PYG{o}{+}\PYG{+w}{ }\PYG{l+m+mi}{1}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{: }\PYG{l+s+si}{\PYGZob{}}\PYG{n}{avg}\PYG{l+s+si}{:}\PYG{l+s+s2}{.2f}\PYG{l+s+si}{\PYGZcb{}}\PYG{l+s+s2}{\PYGZdq{}}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
Final average reward for Machine 1: 0.30
Final average reward for Machine 2: 0.64
Final average reward for Machine 3: 0.71
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
At each step, we define as \sphinxstylestrong{best machine} the machine with the highest average reward so far, and we continue pulling the best machine and update the average rewards based on the new observations.

\sphinxAtStartPar
\sphinxstylestrong{Step \$: regret calculation}

\sphinxAtStartPar
Regret is a measure of how much worse our algorithm performs compared to if we had always chosen the best possible machine (the one with the highest true mean reward). If we knew in advance which machine had the highest probability of giving us a prize, we would always choose that machine. However, since we do not know this in advance, we have to try out all the machines to gather information. While we are trying out all the machines (exploring), we might choose the less optimal machines sometimes, which gives us less reward compared to the best machine.
Regret is the difference between the total reward we would have gotten by always choosing the best machine and the total reward we actually got by following our algorithm.

\sphinxAtStartPar
We have:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Optimal reward: this is the reward we would have gotten if we had always chosen the best machine. It is calculated as the cumulative sum of the highest true mean reward over all rounds.

\item {} 
\sphinxAtStartPar
Algorithm reward: this is the reward we actually got by following our algorithm, which includes exploration and exploitation steps.

\item {} 
\sphinxAtStartPar
Regret: the difference between the optimal reward and the algorithm reward,

\end{itemize}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} True means and optimal reward}
\PYG{n}{optimal\PYGZus{}mean} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{true\PYGZus{}means}\PYG{p}{)}
\PYG{n}{optimal\PYGZus{}cumulative\PYGZus{}rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{p}{[}\PYG{n}{optimal\PYGZus{}mean}\PYG{p}{]} \PYG{o}{*} \PYG{p}{(}\PYG{n}{initial\PYGZus{}pulls} \PYG{o}{*} \PYG{l+m+mi}{3} \PYG{o}{+} \PYG{n}{additional\PYGZus{}rounds}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Calculate regret}
\PYG{n}{regret} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}cumulative\PYGZus{}rewards} \PYG{o}{\PYGZhy{}} \PYG{n}{cumulative\PYGZus{}rewards}

\PYG{c+c1}{\PYGZsh{} Plot cumulative rewards and regret}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{cumulative\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Algorithm Reward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{optimal\PYGZus{}cumulative\PYGZus{}rewards}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Optimal Reward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Round}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Reward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Reward Over Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{subplot}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{regret}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regret}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{r}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{.8}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Round}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regret}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Regret Over Time}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}

\PYG{n}{plt}\PYG{o}{.}\PYG{n}{tight\PYGZus{}layout}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{e39fd570dc5a8b85da05d3fd80c48a1489edb91b9853c70fea7d436b76dff296}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
\sphinxstylestrong{How to read these graphs}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Cumulative reward}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
This graph shows the total reward accumulated over time by the algorithm.

\item {} 
\sphinxAtStartPar
The x\sphinxhyphen{}axis represents the number of rounds (time), and the y\sphinxhyphen{}axis represents the cumulative reward.

\item {} 
\sphinxAtStartPar
Initially, during the exploration phase, the rewards might increase slowly because the algorithm is trying out different arms to gather information.

\item {} 
\sphinxAtStartPar
As the algorithm gathers more information and starts to exploit the best\sphinxhyphen{}performing arm, the slope of the cumulative reward graph should increase, indicating a faster accumulation of rewards.

\item {} 
\sphinxAtStartPar
The optimal cumulative reward line represents the reward we would have accumulated if we had always chosen the best arm from the beginning. This line has a constant, steep slope, indicating the highest possible reward accumulation rate.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Regret}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
This graph shows the difference between the optimal cumulative reward and the cumulative reward obtained by the algorithm.

\item {} 
\sphinxAtStartPar
The x\sphinxhyphen{}axis represents the number of rounds (time), and the y\sphinxhyphen{}axis represents the regret.

\item {} 
\sphinxAtStartPar
At the beginning, the regret might increase rapidly because the algorithm is exploring and might be choosing suboptimal arms, leading to lower rewards compared to the optimal arm.

\item {} 
\sphinxAtStartPar
As the algorithm starts to exploit the best\sphinxhyphen{}performing arm more frequently, the rate at which regret increases should slow down. Ideally, the regret graph will start to flatten out, indicating that the algorithm is performing close to optimally.

\item {} 
\sphinxAtStartPar
The ideal scenario is to have a regret graph that flattens out as early as possible, showing that the algorithm quickly learned to choose the best arm.

\end{itemize}

\end{enumerate}


\section{Key algorithms for MAB problems}
\label{\detokenize{notebooks/bandits:key-algorithms-for-mab-problems}}
\sphinxAtStartPar
In this section, we will explain three popular algorithms for solving the multi\sphinxhyphen{}armed bandit (MAB) problem: epsilon\sphinxhyphen{}greedy, upper confidence bound (UCB), and Thompson sampling. We will also compare their performance against a random strategy over multiple simulation runs.


\subsection{Epsilon\sphinxhyphen{}greedy}
\label{\detokenize{notebooks/bandits:epsilon-greedy}}
\sphinxAtStartPar
The Epsilon\sphinxhyphen{}Greedy algorithm balances exploration and exploitation by choosing a random arm with a small probability (\(\epsilon\)) and the best\sphinxhyphen{}known arm with a large probability (1 \sphinxhyphen{} \(\epsilon\)). Think of \(\epsilon\) as your curiosity factor. A higher \(\epsilon\) means you are more curious and willing to try different options even if you know some options perform well. A lower \(\epsilon\) means you are more inclined to stick with what you know works best.

\sphinxAtStartPar
\sphinxstylestrong{Mechanism}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exploration}: with a small probability (\(\epsilon\)), you choose a random arm to try out new options.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Exploitation}: with a large probability (1 \sphinxhyphen{} \(\epsilon\)), you choose the arm that has given you the highest average reward so far.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Formulation}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimated mean reward}: for each arm \(k\), you keep track of the average reward it has given you until this time \(t\), \(\hat{\mu}_k(t)\).

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updating the Estimate}: every time you pull an arm and get a reward, you update the average reward for that arm using the formula:
\label{equation:notebooks/bandits:eb34614a-62aa-4147-8d74-871a6320c8d9}\begin{equation}
    \hat{\mu}_k(t+1) = \hat{\mu}_k(t) + \frac{r_k(t) - \hat{\mu}_k(t)}{n_k(t)}
  \end{equation}
\end{itemize}

\sphinxAtStartPar
where \(\hat{\mu}_k(t)\) is the current average reward for arm \(k\) at time \(t\), \(r_k(t)\) is the reward you get from arm \(k\) at time \(t\), and \(n_k(t)\) is the number of times you have pulled arm \(k\) up to time \(t\).

\sphinxAtStartPar
Intuitively, when you receive a new reward for an arm, you need to update your estimate of the arm’s average reward. The goal is to make sure that this estimate becomes more accurate as you gather more data. To update the estimate of the average reward, we need to balance:
\begin{itemize}
\item {} 
\sphinxAtStartPar
New information: the reward you just observed provides new information about the arm’s performance.

\item {} 
\sphinxAtStartPar
Past experience the average reward you have observed so far reflects your past experience with that arm.

\end{itemize}

\sphinxAtStartPar
The update formula balances these two by adjusting the current average reward (\(\hat{\mu}_k(t)\)) using the new reward (\(r_k(t)\)). The difference between the new reward and the current average reward is adjusted by \(\frac{1}{n_k(t)}\). This fraction gets smaller as \(n_k(t)\), the number of times the arm has been pulled, increases. The key reasons for this is that:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Diminishing impact: the fraction \(\frac{1}{n_k(t)}\) ensures that the impact of new rewards diminishes over time. This means that early rewards have a bigger influence on your estimate, helping you quickly form an initial understanding of the arm’s performance.

\item {} 
\sphinxAtStartPar
As you pull the arm more times, you gather more information, and your estimate becomes more stable. New rewards should have less impact on the estimate because you already have a lot of information. Later rewards refine your estimate more subtly, preventing large swings in the estimate and ensuring that the estimate converges to the true average reward as you gather more data.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Simple example to illustrate how it works}
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{First Pull}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Suppose you pull an arm for the first time and get a reward of 10.

\item {} 
\sphinxAtStartPar
Your initial estimate \(\hat{\mu}_k(1)\) is 10 because it’s the only data point you have.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Second Pull}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
You pull the same arm again and get a reward of 8.

\item {} 
\sphinxAtStartPar
You update your estimate:
\label{equation:notebooks/bandits:62642f5d-53d2-41ce-a081-d2ac74177f27}\begin{equation}
    \hat{\mu}_k(2) = 10 + \frac{8 - 10}{2} = 9
    \end{equation}
\item {} 
\sphinxAtStartPar
The new reward adjusts the estimate significantly because you only have two data points.

\end{itemize}

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Hundredth Pull}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
After pulling the arm 99 times, suppose your current estimate is 9.5.

\item {} 
\sphinxAtStartPar
You pull the arm one more time and get a reward of 9.

\item {} 
\sphinxAtStartPar
You update your estimate:
\label{equation:notebooks/bandits:f0b9cba0-db7e-41c0-af91-fe8d64c63438}\begin{equation}
    \hat{\mu}_k(100) = 9.5 + \frac{9 - 9.5}{100} = 9.495
    \end{equation}
\item {} 
\sphinxAtStartPar
The new reward barely changes the estimate because you have a lot of data, making the estimate more stable.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
By using this adjustment, you ensure that your estimate of the average reward for each arm becomes more accurate and stable over time, leading to better decision\sphinxhyphen{}making in the epsilon\sphinxhyphen{}greedy algorithm.

\sphinxAtStartPar
\sphinxstylestrong{Implementation}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{epsilon\PYGZus{}greedy}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n\PYGZus{}arms} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{)}
    \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{counts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{total\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}rounds}\PYG{p}{)}\PYG{p}{:}
        \PYG{k}{if} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{epsilon}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Exploration: choose a random arm}
            \PYG{n}{arm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Exploitation: choose the best arm so far}
            \PYG{n}{arm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{rewards} \PYG{o}{/} \PYG{p}{(}\PYG{n}{counts} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{)}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Simulate pulling the arm}
        \PYG{n}{reward} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{arms}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]}
        \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{counts}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{total\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{total\PYGZus{}rewards}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Upper confidence bound (UCB)}
\label{\detokenize{notebooks/bandits:upper-confidence-bound-ucb}}
\sphinxAtStartPar
This algorithm selects arms based on the upper confidence bounds (UCBs) of their estimated rewards. The UCB is a combination of the estimated reward and a term that accounts for the uncertainty in the estimate. The key idea is to decide which options to try out by balancing between choosing the option that seems the best based on what you know so far and exploring less\sphinxhyphen{}tried options to discover their potential. It does this by considering both the estimated reward of each option and the uncertainty around that estimate.

\sphinxAtStartPar
\sphinxstylestrong{Mechanism}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
At each time step \(t\), select the arm \(k\) that maximizes the upper confidence bound \(UCB_k(t)\).

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Formulation}:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The upper confidence bound for arm \(k\) at time \(t\) is given by:

\end{itemize}
\label{equation:notebooks/bandits:ac8d4640-b0b4-40a8-8264-df1534ff15c6}\begin{equation}
    UCB_k(t) = \hat{\mu}_k(t) + c \sqrt{\frac{\ln t}{n_k(t)}}
\end{equation}
\sphinxAtStartPar
which is composed of two terms:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Estimated reward}: the term \(\hat{\mu}_k(t)\) represents our current best guess of the mean reward for arm \(k\) based on the rewards we have observed so far. This is our current estimate of the average reward for arm \(k\). It is updated every time we pull the arm and observe a reward, using the same updating mechanism as in the epsilon\sphinxhyphen{}greedy algorithm.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Uncertainty term}: The term \(c \sqrt{\frac{\ln t}{n_k(t)}}\) represents the uncertainty or confidence interval around the estimated reward. It ensures that arms with fewer pulls (higher uncertainty) are given a higher chance of being selected. Here, \(n_k(t)\) is the number of times arm \(k\) has been selected up to time \(t\), and \(c\) is a confidence parameter that controls the degree of exploration. Let’s break down the components of this term to better understand how it works:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(\ln t\): The natural logarithm of the current time step \(t\). As time progresses, this term grows, but at a decreasing rate.

\item {} 
\sphinxAtStartPar
\(\frac{1}{n_k(t)}\): the reciprocal of the number of times arm \(k\) has been pulled. This term decreases as we pull the arm more often.

\item {} 
\sphinxAtStartPar
Square root: the square root ensures that the uncertainty term grows more slowly as the number of pulls increases.

\item {} 
\sphinxAtStartPar
Confidence parameter \(c\): this parameter controls how much weight we give to the uncertainty term. A higher \(c\) value means more exploration.

\end{itemize}

\end{itemize}

\sphinxAtStartPar
Why This Works:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Early stages}: at the beginning of the process, \(n_k(t)\) is small for all arms, making the uncertainty term large. This encourages exploration of all arms to gather initial information.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Later stages}: as \(n_k(t)\) increases for an arm, the uncertainty term decreases. This means that the algorithm will increasingly favor arms with higher estimated rewards, but will still occasionally explore other arms to ensure they are not overlooked. Over time, the algorithm balances between exploiting machines with high estimated rewards and exploring machines with higher uncertainty to ensure it does not miss out on potentially better options.

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Implementation}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k}{def} \PYG{n+nf}{ucb}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n\PYGZus{}arms} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{)}
    \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{counts} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{total\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{t} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{ucb\PYGZus{}values} \PYG{o}{=} \PYG{n}{rewards} \PYG{o}{/} \PYG{p}{(}\PYG{n}{counts} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{)} \PYG{o}{+} \PYG{n}{c} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sqrt}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{log}\PYG{p}{(}\PYG{n}{t}\PYG{p}{)} \PYG{o}{/} \PYG{p}{(}\PYG{n}{counts} \PYG{o}{+} \PYG{l+m+mf}{1e\PYGZhy{}5}\PYG{p}{)}\PYG{p}{)}
        \PYG{n}{arm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{ucb\PYGZus{}values}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Simulate pulling the arm}
        \PYG{n}{reward} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{arms}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]}
        \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{n}{reward}
        \PYG{n}{counts}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{total\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{total\PYGZus{}rewards}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Thompson sampling}
\label{\detokenize{notebooks/bandits:thompson-sampling}}
\sphinxAtStartPar
Thompson sampling balances between trying out different options (exploration) and sticking with the best\sphinxhyphen{}known option (exploitation) by using probability distributions to model the uncertainty of each option’s rewards.

\sphinxAtStartPar
\sphinxstylestrong{Mechanism}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Maintain a probability distribution}: for each option (or arm), we keep track of a probability distribution that represents our belief about the likelihood of getting a reward from that option. In Thompson sampling, we use the Beta distribution for this purpose.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Sample a value}: from these distributions, we randomly sample a value for each option. These sampled values represent our current guess about the expected reward from each option. This step introduces randomness, ensuring that we occasionally try out arms that have fewer successes but might have potential.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Choose the best option}: we select the option with the highest sampled value. This means that we are more likely to choose options that have shown higher rewards in the past, but we still occasionally try out other options to gather more information about them.

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{Formulation}:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Beta distribution}: the reward probability of each arm is assumed to follow a Beta distribution. The Beta distribution is a family of continuous probability distributions defined on the interval \sphinxcode{\sphinxupquote{{[}0, 1{]}}}, parameterized by two positive shape parameters, \(\alpha\) and \(\beta\). The distribution represents the posterior probability of the success rate of a Bernoulli trial (a trial with two outcomes, success or failure). The first parameter, \(\alpha_k\) represents the number of successes (rewards), while \(\beta_k\) represents the number of failures (no rewards) for arm \(k\). For example, if we have an arm (option) with \(\alpha = 5\) and \(\beta = 3\), the Beta distribution would give us a distribution of possible success probabilities for this arm, based on the 5 successes and 3 failures observed so far. When we get new data, we can easily update the Beta distribution with simple calculations. This is because the Beta distribution is a “conjugate prior” for the Bernoulli distribution, which means their mathematical properties align perfectly for easy updates.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updating the distribution}: when we pull an arm \(k \) and observe a reward \(r_k(t)\), we update the parameters of the Beta distribution as follows:
\begin{itemize}
\item {} 
\sphinxAtStartPar
If we get a reward (\(r_k(t) = 1 \)), we increase \(\alpha_k\) by 1, indicating one more success:
\label{equation:notebooks/bandits:5cdfea3a-72e7-4981-89c0-9f19f6d5b9c4}\begin{equation}
         \alpha_k = \alpha_k + r_k(t)
     \end{equation}
\item {} 
\sphinxAtStartPar
If we do not get a reward (\(r_k(t) = 0 \)), we increase \(\beta_k\) by 1, indicating one more failure:
\label{equation:notebooks/bandits:38110a04-bee8-432a-94b0-09468fc4c802}\begin{equation}
         \beta_k = \beta_k + 1 - r_k(t)
     \end{equation}
\end{itemize}

\end{enumerate}

\sphinxAtStartPar
\sphinxstylestrong{In practice}:
Initially, both \(\alpha\) and \(\beta\) start at zero since we have not observed any rewards or failures yet. However, the Beta distribution with parameters \(\alpha = 0\) and \(\beta = 0\) is undefined because the Beta distribution requires positive parameters. So, we add 1 to each of these two values:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Initial prior}: by starting with \(\alpha = 1\) and \(\beta = 1\), we assume a weak prior belief that each arm has an equal probability of success and failure. This is often referred to as a “non\sphinxhyphen{}informative” or “uniform” prior.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Practicality}: it ensures that the Beta distribution is always defined, allowing us to sample from it even before any data is observed.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Conjugate prior}: When we update the Beta distribution with observed data, adding 1 ensures that our initial prior belief is combined with the observed data correctly.

\end{itemize}

\sphinxAtStartPar
Suppose we have an arm that has been pulled 10 times, resulting in 7 successes and 3 failures:
\begin{itemize}
\item {} 
\sphinxAtStartPar
Successes: 7

\item {} 
\sphinxAtStartPar
Failures: 3

\end{itemize}

\sphinxAtStartPar
For Thompson sampling:
\begin{itemize}
\item {} 
\sphinxAtStartPar
The \(\alpha\) parameter (successes) is \(7 + 1 = 8\)

\item {} 
\sphinxAtStartPar
The \(\beta\) parameter (failures) is \(3 + 1 = 4\)

\end{itemize}

\sphinxAtStartPar
This gives us a Beta distribution with parameters \(\alpha = 8\) and \(\beta = 4\), from which we can sample to estimate the probability of success for this arm.

\sphinxAtStartPar
\sphinxstylestrong{Implementation}:

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Thompson Sampling algorithm}
\PYG{k}{def} \PYG{n+nf}{thompson\PYGZus{}sampling}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{n\PYGZus{}arms} \PYG{o}{=} \PYG{n+nb}{len}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{)}
    \PYG{n}{successes} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{failures} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}
    \PYG{n}{total\PYGZus{}rewards} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{\PYGZus{}} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}rounds}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{sampled\PYGZus{}probs} \PYG{o}{=} \PYG{p}{[}\PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{beta}\PYG{p}{(}\PYG{n}{successes}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{failures}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}arms}\PYG{p}{)}\PYG{p}{]}
        \PYG{n}{arm} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{sampled\PYGZus{}probs}\PYG{p}{)}

        \PYG{c+c1}{\PYGZsh{} Simulate pulling the arm}
        \PYG{n}{reward} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{p}{)} \PYG{o}{\PYGZlt{}} \PYG{n}{arms}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]}
        \PYG{k}{if} \PYG{n}{reward}\PYG{p}{:}
            \PYG{n}{successes}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{k}{else}\PYG{p}{:}
            \PYG{n}{failures}\PYG{p}{[}\PYG{n}{arm}\PYG{p}{]} \PYG{o}{+}\PYG{o}{=} \PYG{l+m+mi}{1}
        \PYG{n}{total\PYGZus{}rewards}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{reward}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{cumsum}\PYG{p}{(}\PYG{n}{total\PYGZus{}rewards}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Comparing the three algorithms with random sampling}
\label{\detokenize{notebooks/bandits:comparing-the-three-algorithms-with-random-sampling}}
\sphinxAtStartPar
Now, we can run a comprehensive example where we compare the performance of epsilon\sphinxhyphen{}greedy, UCB, and Thompson sampling against a random strategy.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Define the true reward probabilities for each arm}
\PYG{n}{true\PYGZus{}means} \PYG{o}{=} \PYG{p}{[}\PYG{l+m+mf}{0.1}\PYG{p}{,} \PYG{l+m+mf}{0.5}\PYG{p}{,} \PYG{l+m+mf}{0.8}\PYG{p}{]}
\PYG{n}{n\PYGZus{}rounds} \PYG{o}{=} \PYG{l+m+mi}{100}
\PYG{n}{n\PYGZus{}simulations} \PYG{o}{=} \PYG{l+m+mi}{100}

\PYG{c+c1}{\PYGZsh{} Simulation to collect rewards}
\PYG{k}{def} \PYG{n+nf}{simulate}\PYG{p}{(}\PYG{n}{algorithm}\PYG{p}{,} \PYG{n}{arms}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{o}{=}\PYG{l+m+mi}{100}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{o}{=}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{rewards} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{zeros}\PYG{p}{(}\PYG{p}{(}\PYG{n}{n\PYGZus{}simulations}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{p}{)}\PYG{p}{)}
    \PYG{k}{for} \PYG{n}{i} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}simulations}\PYG{p}{)}\PYG{p}{:}
        \PYG{n}{rewards}\PYG{p}{[}\PYG{n}{i}\PYG{p}{,} \PYG{p}{:}\PYG{p}{]} \PYG{o}{=} \PYG{n}{algorithm}\PYG{p}{(}\PYG{n}{arms}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{o}{=}\PYG{n}{n\PYGZus{}rounds}\PYG{p}{,} \PYG{o}{*}\PYG{o}{*}\PYG{n}{kwargs}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{rewards}

\PYG{c+c1}{\PYGZsh{} Run simulations}
\PYG{n}{rewards\PYGZus{}random} \PYG{o}{=} \PYG{n}{simulate}\PYG{p}{(}\PYG{n}{epsilon\PYGZus{}greedy}\PYG{p}{,} \PYG{n}{true\PYGZus{}means}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{1.0}\PYG{p}{)}  \PYG{c+c1}{\PYGZsh{} Pure exploration (random)}
\PYG{n}{rewards\PYGZus{}epsilon\PYGZus{}greedy} \PYG{o}{=} \PYG{n}{simulate}\PYG{p}{(}\PYG{n}{epsilon\PYGZus{}greedy}\PYG{p}{,} \PYG{n}{true\PYGZus{}means}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{p}{,} \PYG{n}{epsilon}\PYG{o}{=}\PYG{l+m+mf}{0.1}\PYG{p}{)}
\PYG{n}{rewards\PYGZus{}ucb} \PYG{o}{=} \PYG{n}{simulate}\PYG{p}{(}\PYG{n}{ucb}\PYG{p}{,} \PYG{n}{true\PYGZus{}means}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+m+mi}{2}\PYG{p}{)}
\PYG{n}{rewards\PYGZus{}thompson} \PYG{o}{=} \PYG{n}{simulate}\PYG{p}{(}\PYG{n}{thompson\PYGZus{}sampling}\PYG{p}{,} \PYG{n}{true\PYGZus{}means}\PYG{p}{,} \PYG{n}{n\PYGZus{}simulations}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can the compare them in terms of \sphinxstylestrong{cumulative reward}, showing how the total reward accumulates over time for each algorithm.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate mean cumulative rewards}
\PYG{n}{mean\PYGZus{}rewards\PYGZus{}random} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{rewards\PYGZus{}random}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards\PYGZus{}epsilon\PYGZus{}greedy} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{rewards\PYGZus{}epsilon\PYGZus{}greedy}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards\PYGZus{}ucb} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{rewards\PYGZus{}ucb}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}rewards\PYGZus{}thompson} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{rewards\PYGZus{}thompson}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot cumulative rewards}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}rewards\PYGZus{}random}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}rewards\PYGZus{}epsilon\PYGZus{}greedy}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Epsilon\PYGZhy{}Greedy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}rewards\PYGZus{}ucb}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{UCB}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}rewards\PYGZus{}thompson}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thompson Sampling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Round}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Reward}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Reward Comparison}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{c151f6ec889a3236b5975f1fef17aa0241136ee2a56669400bf1a6500764ec76}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
and here in terms of \sphinxstylestrong{regret}, showing the difference between the reward of the optimal machine and the reward obtained by each algorithm.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Calculate regret}
\PYG{n}{optimal\PYGZus{}mean} \PYG{o}{=} \PYG{n+nb}{max}\PYG{p}{(}\PYG{n}{true\PYGZus{}means}\PYG{p}{)}
\PYG{n}{regret\PYGZus{}random} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}mean} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{mean\PYGZus{}rewards\PYGZus{}random}
\PYG{n}{regret\PYGZus{}epsilon\PYGZus{}greedy} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}mean} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{mean\PYGZus{}rewards\PYGZus{}epsilon\PYGZus{}greedy}
\PYG{n}{regret\PYGZus{}ucb} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}mean} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{mean\PYGZus{}rewards\PYGZus{}ucb}
\PYG{n}{regret\PYGZus{}thompson} \PYG{o}{=} \PYG{n}{optimal\PYGZus{}mean} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{arange}\PYG{p}{(}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{n}{n\PYGZus{}rounds} \PYG{o}{+} \PYG{l+m+mi}{1}\PYG{p}{)} \PYG{o}{\PYGZhy{}} \PYG{n}{mean\PYGZus{}rewards\PYGZus{}thompson}

\PYG{c+c1}{\PYGZsh{} Plot regret}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{regret\PYGZus{}random}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linestyle}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{\PYGZhy{}\PYGZhy{}}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{regret\PYGZus{}epsilon\PYGZus{}greedy}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Epsilon\PYGZhy{}Greedy}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{c}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{regret\PYGZus{}ucb}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{UCB}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{m}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{regret\PYGZus{}thompson}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Thompson Sampling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{lw}\PYG{o}{=}\PYG{l+m+mi}{3}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{b}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Round}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Regret}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cumulative Regret Comparison}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{2bf49d4e478ea502c2d579867e38585edb7c08bc556cf141c8fad9527c041120}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Through these plots, we can visually compare the efficiency and performance of different algorithms in balancing exploration and exploitation, highlighting the advantages of more sophisticated MAB strategies over a random approach.

\sphinxstepscope


\chapter{Design of Experiments}
\label{\detokenize{notebooks/design_of_experiments:design-of-experiments}}\label{\detokenize{notebooks/design_of_experiments::doc}}
\sphinxAtStartPar
Design of experiments (DoE) is a systematic approach to planning, conducting, analysing, and interpreting controlled experiments {[}\hyperlink{cite.bibliography:id16}{Mon17}{]}. It aims to ensure that the results are valid, reliable, and can be used to draw causal conclusions. DoE is essential in various fields, including engineering, medicine, agriculture, and computer science.

\sphinxAtStartPar
The \sphinxstylestrong{key concepts} of DoE include:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Randomisation}: Randomly assigning subjects to different treatment groups to eliminate bias.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Replication}: Repeating the experiment to estimate variability and ensure reliability.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Blocking}: Grouping similar experimental units and randomising within these blocks to reduce variability.

\end{itemize}

\sphinxAtStartPar
A good mantra for dealing with the presence of confounders is \sphinxstylestrong{block what you can, randomise what you cannot}. What do we mean by this? In experimental design, controlling for confounders is crucial to ensure that the results are valid and reliable. Confounders are variables that can affect both the independent variable and the dependent variable, potentially leading to biased or incorrect conclusions about the relationship between these variables. The mantra “block what you can, randomise what you cannot” provides a practical approach to managing confounding variables.
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Block what you can}: blocking is a technique used to control for known sources of variability that are not of primary interest but could influence the outcome of the experiment. By grouping similar experimental units into blocks, we can account for these sources of variability and reduce their impact on the results. For example, in an experiment to test the efficiency of different battery storage systems, we might block by the type of battery used. Each block would contain all types of batteries, ensuring that the variation due to the battery type is controlled for and does not confound the results. Blocking helps isolate the effect of the primary factors being studied by accounting for the variability due to other known factors.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Randomise what you cannot}: randomisation is used to control for unknown or uncontrollable sources of variability. By randomly assigning experimental units to different treatment groups, we ensure that these sources of variability are evenly distributed across all groups, reducing the risk of bias. For example, if we are conducting an experiment to test different charging rates and temperatures on battery efficiency, and we cannot control the environmental conditions precisely, we can randomly assign the charging rates and temperatures to different batteries. This way, any unknown or uncontrollable factors (like slight variations in ambient temperature) are equally likely to affect all treatment groups, minimising their impact on the results. Randomisation helps ensure that the treatment groups are comparable and that the observed effects are due to the treatments themselves rather than other extraneous factors.

\end{enumerate}


\section{Some important designs}
\label{\detokenize{notebooks/design_of_experiments:some-important-designs}}
\sphinxAtStartPar
In most cases, DoE is used to analyse the relationship between the input and output factors of a process. Various tests are performed to see the effect of varying levels of input factors on the response. For example, in the context of electricity markets, we might be interested in designing an experiment to understand the effect of charge rate and temperature on the efficiency of a battery storage system. By systematically varying these factors, researchers can identify optimal operating conditions and interactions between variables. The critical aspect of DoE is the way the factor levels are varied throughout the experiment.


\subsection{Factorial Designs}
\label{\detokenize{notebooks/design_of_experiments:factorial-designs}}
\sphinxAtStartPar
One of the most common types of design is the factorial design, where in each complete trial or replicate of the experiment all possible combinations of the levels of the factors are investigated. A \sphinxstylestrong{\(2^K\) factorial design} is a special case of factorial design where each of the \(K\) factors is studied at two levels, often referred to as low (\sphinxhyphen{}1) and high (1). This design is highly efficient for exploring the main effects and interactions among factors. In the context of a battery storage system, suppose we are interested in the effects of two factors: charge rate and temperature. Each factor can be set at two levels: low (\sphinxhyphen{}1) and high (1). This is the data that would be collected using a \(2^2\) factorial design, without replications:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Run
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Charge Rate (C)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Temperature (°C)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Efficiency (\%)
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
85
\\
\sphinxhline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
80
\\
\sphinxhline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
90
\\
\sphinxhline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
75
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\sphinxAtStartPar
The figure below shows a factorial experiment where both charge rate and temperature can be set at two levels. The black dots represent the points where the tests are performed, namely where the response is measured.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{import} \PYG{n+nn}{pandas} \PYG{k}{as} \PYG{n+nn}{pd}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Data for factorial design}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Charge\PYGZus{}Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Efficiency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{85}\PYG{p}{,} \PYG{l+m+mi}{80}\PYG{p}{,} \PYG{l+m+mi}{90}\PYG{p}{,} \PYG{l+m+mi}{75}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Generate grid for plotting}
\PYG{n}{x1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{x2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{)}
\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plotting the factorial design points}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Charge\PYGZus{}Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{black}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{s}\PYG{o}{=}\PYG{l+m+mi}{200}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Charge Rate (coded)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature (coded)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factorial Design Points}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Annotating the points with their efficiency values}
\PYG{k}{for} \PYG{n}{i}\PYG{p}{,} \PYG{n}{txt} \PYG{o+ow}{in} \PYG{n+nb}{enumerate}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Efficiency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{plt}\PYG{o}{.}\PYG{n}{annotate}\PYG{p}{(}\PYG{n}{txt}\PYG{p}{,} \PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Charge\PYGZus{}Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{[}\PYG{n}{i}\PYG{p}{]}\PYG{o}{*}\PYG{l+m+mf}{1.1}\PYG{p}{)}\PYG{p}{,} \PYG{n}{fontsize}\PYG{o}{=}\PYG{l+m+mi}{12}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{ha}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{center}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlim}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.2}\PYG{p}{,} \PYG{l+m+mf}{1.2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{f7e784e52a2c904c1b464f513b32b207ffab397905596e6438b07eb112dca708}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The effect of a factor is defined as the change in response produced by a change in the level of the factor. Assuming linearity in the factor effects within the design space, an ordinary least squares (OLS) regression model is usually fit on the experimental data.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{formula}\PYG{n+nn}{.}\PYG{n+nn}{api} \PYG{k+kn}{import} \PYG{n}{ols}
\PYG{k+kn}{import} \PYG{n+nn}{seaborn} \PYG{k}{as} \PYG{n+nn}{sns}

\PYG{c+c1}{\PYGZsh{} Fit the model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{ols}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Efficiency \PYGZti{} Charge\PYGZus{}Rate * Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{c+c1}{\PYGZsh{} print(model.summary())}

\PYG{c+c1}{\PYGZsh{} Plotting the interaction}
\PYG{n}{sns}\PYG{o}{.}\PYG{n}{lmplot}\PYG{p}{(}\PYG{n}{x}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Charge\PYGZus{}Rate}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{y}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Efficiency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{hue}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Temperature}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{data}\PYG{o}{=}\PYG{n}{data}\PYG{p}{,} \PYG{n}{markers}\PYG{o}{=}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{s}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ci}\PYG{o}{=}\PYG{k+kc}{None}\PYG{p}{,} \PYG{n}{palette}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{muted}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{height}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{n}{aspect}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Interaction Plot}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{992a06447076d83762a3fc38fc13057d043b479a1aa37adba99b6afac0b5dba7}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
The interaction plot visualises the relationship between the charge rate and the efficiency of the battery storage system, with temperature as the moderating variable. Each line in the plot represents the effect of charge rate on efficiency at a specific temperature level, indicated by different colors and markers. The x\sphinxhyphen{}axis shows the coded values for charge rate (with \sphinxhyphen{}1 representing a low charge rate and 1 representing a high charge rate), while the y\sphinxhyphen{}axis displays the battery efficiency in percentage.

\sphinxAtStartPar
From this plot, readers can observe how the efficiency changes with varying charge rates and how this relationship is influenced by temperature.
\begin{itemize}
\item {} 
\sphinxAtStartPar
If the lines for different temperatures are \sphinxstylestrong{parallel}, it indicates that there is no interaction between charge rate and temperature—each factor independently affects efficiency.

\item {} 
\sphinxAtStartPar
If the lines are \sphinxstylestrong{not parallel}, this suggests an interaction effect, meaning the impact of charge rate on efficiency depends on the temperature level. For instance, one might notice that increasing the charge rate significantly boosts efficiency at a lower temperature but not at a higher temperature. This insight is crucial for optimising the battery storage system, as it highlights the importance of considering both factors together rather than in isolation.

\end{itemize}

\sphinxAtStartPar
Overall, the interaction plot provides a clear and intuitive way to understand complex relationships between multiple factors, helping researchers and practitioners to make informed decisions about optimising the system’s performance.


\subsection{Central composite designs}
\label{\detokenize{notebooks/design_of_experiments:central-composite-designs}}
\sphinxAtStartPar
The central composite design (CCD) is an extension of factorial design used for building a second\sphinxhyphen{}order (quadratic) model without needing to perform a complete three\sphinxhyphen{}level factorial experiment. It includes:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Factorial points}: these are points from a standard \(2^k\) factorial design, where \(k\) is the number of factors. The factorial points allow for the estimation of the main effects and interaction effects.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Centre points}: these are the midpoints of all factor levels and are replicated to provide an estimate of the experimental error. Centre points help detect curvature in the response surface.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Axial points}: these points are added at a distance \(\alpha\) from the centre along each axis of the factor space. Axial points enable the estimation of the quadratic terms, providing the necessary information to fit a second\sphinxhyphen{}order model.

\end{itemize}

\sphinxAtStartPar
The CCD is highly efficient for exploring the quadratic response surface and identifying the optimal settings of the input factors. By combining these points, CCDs provide a balanced and comprehensive design for understanding the effects of factors and their interactions. Here you can see an example of how to obtain a CCD by adding centre points and axial points to a regular \(2^k\) factorial design.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{pyDOE2} \PYG{k+kn}{import} \PYG{n}{ccdesign}

\PYG{c+c1}{\PYGZsh{} Create a CCD design}
\PYG{n}{ccd} \PYG{o}{=} \PYG{n}{ccdesign}\PYG{p}{(}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{center}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{4}\PYG{p}{,} \PYG{l+m+mi}{4}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the CCD design}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{ccd}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,} \PYG{n}{ccd}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factor 1}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Factor 2}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Central Composite Design}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{28a0e41480029dcd8e94e35e276265aedb8392f162a39de168c3771df57e9e2b}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Let’s now consider an example where we want to use a CCD to build a model for optimising the output of a solar power plant based on two controllable variables: cleaning frequency (\(x_1\)) and tilt angle (\(x_2\)). Using a CCD and measuring the value of the response (\(y\)) at the design locations, we would collect this dataset:


\begin{savenotes}\sphinxattablestart
\sphinxthistablewithglobalstyle
\centering
\begin{tabulary}{\linewidth}[t]{TTTT}
\sphinxtoprule
\sphinxstyletheadfamily 
\sphinxAtStartPar
Run
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Cleaning Frequency (times/month)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Tilt Angle (degrees)
&\sphinxstyletheadfamily 
\sphinxAtStartPar
Output (kWh)
\\
\sphinxmidrule
\sphinxtableatstartofbodyhook
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
500
\\
\sphinxhline
\sphinxAtStartPar
2
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
520
\\
\sphinxhline
\sphinxAtStartPar
3
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
\sphinxhyphen{}1
&
\sphinxAtStartPar
540
\\
\sphinxhline
\sphinxAtStartPar
4
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
1
&
\sphinxAtStartPar
560
\\
\sphinxhline
\sphinxAtStartPar
5
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
550
\\
\sphinxhline
\sphinxAtStartPar
6
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
570
\\
\sphinxhline
\sphinxAtStartPar
7
&
\sphinxAtStartPar
\sphinxhyphen{}1.414
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
530
\\
\sphinxhline
\sphinxAtStartPar
8
&
\sphinxAtStartPar
1.414
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
580
\\
\sphinxhline
\sphinxAtStartPar
9
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
\sphinxhyphen{}1.414
&
\sphinxAtStartPar
540
\\
\sphinxhline
\sphinxAtStartPar
10
&
\sphinxAtStartPar
0
&
\sphinxAtStartPar
1.414
&
\sphinxAtStartPar
580
\\
\sphinxbottomrule
\end{tabulary}
\sphinxtableafterendhook\par
\sphinxattableend\end{savenotes}

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{statsmodels}\PYG{n+nn}{.}\PYG{n+nn}{tools} \PYG{k+kn}{import} \PYG{n}{add\PYGZus{}constant}
\PYG{k+kn}{from} \PYG{n+nn}{mpl\PYGZus{}toolkits}\PYG{n+nn}{.}\PYG{n+nn}{mplot3d} \PYG{k+kn}{import} \PYG{n}{Axes3D}

\PYG{c+c1}{\PYGZsh{} Data for CCD}
\PYG{n}{data} \PYG{o}{=} \PYG{n}{pd}\PYG{o}{.}\PYG{n}{DataFrame}\PYG{p}{(}\PYG{p}{\PYGZob{}}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning\PYGZus{}Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.414}\PYG{p}{,} \PYG{l+m+mf}{1.414}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt\PYGZus{}Angle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.414}\PYG{p}{,} \PYG{l+m+mf}{1.414}\PYG{p}{]}\PYG{p}{,}
    \PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Output}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{:} \PYG{p}{[}\PYG{l+m+mi}{500}\PYG{p}{,} \PYG{l+m+mi}{520}\PYG{p}{,} \PYG{l+m+mi}{540}\PYG{p}{,} \PYG{l+m+mi}{560}\PYG{p}{,} \PYG{l+m+mi}{550}\PYG{p}{,} \PYG{l+m+mi}{570}\PYG{p}{,} \PYG{l+m+mi}{530}\PYG{p}{,} \PYG{l+m+mi}{580}\PYG{p}{,} \PYG{l+m+mi}{540}\PYG{p}{,} \PYG{l+m+mi}{580}\PYG{p}{]}
\PYG{p}{\PYGZcb{}}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Define the design matrix for the quadratic model}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{column\PYGZus{}stack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning\PYGZus{}Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt\PYGZus{}Angle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,}
                     \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning\PYGZus{}Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt\PYGZus{}Angle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,}
                     \PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning\PYGZus{}Frequency}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{o}{*}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt\PYGZus{}Angle}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Add constant (intercept)}
\PYG{n}{X} \PYG{o}{=} \PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Fit the model}
\PYG{n}{model} \PYG{o}{=} \PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Output}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Prepare data for 3D surface plot}
\PYG{n}{x1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}
\PYG{n}{x2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}
\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{meshgrid}\PYG{p}{(}\PYG{n}{x1}\PYG{p}{,} \PYG{n}{x2}\PYG{p}{)}
\PYG{n}{X1\PYGZus{}flat} \PYG{o}{=} \PYG{n}{X1}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{X2\PYGZus{}flat} \PYG{o}{=} \PYG{n}{X2}\PYG{o}{.}\PYG{n}{flatten}\PYG{p}{(}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Create design matrix for predictions}
\PYG{n}{X\PYGZus{}pred} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{column\PYGZus{}stack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{ones\PYGZus{}like}\PYG{p}{(}\PYG{n}{X1\PYGZus{}flat}\PYG{p}{)}\PYG{p}{,} \PYG{n}{X1\PYGZus{}flat}\PYG{p}{,} \PYG{n}{X2\PYGZus{}flat}\PYG{p}{,}
                          \PYG{n}{X1\PYGZus{}flat}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X2\PYGZus{}flat}\PYG{o}{*}\PYG{o}{*}\PYG{l+m+mi}{2}\PYG{p}{,} \PYG{n}{X1\PYGZus{}flat}\PYG{o}{*}\PYG{n}{X2\PYGZus{}flat}\PYG{p}{)}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Predict the response}
\PYG{n}{Y\PYGZus{}pred} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}pred}\PYG{p}{)}\PYG{o}{.}\PYG{n}{reshape}\PYG{p}{(}\PYG{n}{X1}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the response surface}
\PYG{n}{fig} \PYG{o}{=} \PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{ax} \PYG{o}{=} \PYG{n}{fig}\PYG{o}{.}\PYG{n}{add\PYGZus{}subplot}\PYG{p}{(}\PYG{l+m+mi}{111}\PYG{p}{,} \PYG{n}{projection}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{3d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{plot\PYGZus{}surface}\PYG{p}{(}\PYG{n}{X1}\PYG{p}{,} \PYG{n}{X2}\PYG{p}{,} \PYG{n}{Y\PYGZus{}pred}\PYG{p}{,} \PYG{n}{cmap}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{viridis}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{edgecolor}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{none}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning Frequency (times/month)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt Angle (degrees)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{ax}\PYG{o}{.}\PYG{n}{set\PYGZus{}zlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Output (kWh)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{title}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Response Surface using CCD}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{g+gt}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZhy{}}
\PYG{n+ne}{NameError}\PYG{g+gWhitespace}{                                 }Traceback (most recent call last)
\PYG{n}{Cell} \PYG{n}{In}\PYG{p}{[}\PYG{l+m+mi}{4}\PYG{p}{]}\PYG{p}{,} \PYG{n}{line} \PYG{l+m+mi}{20}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{17} \PYG{n}{X} \PYG{o}{=} \PYG{n}{add\PYGZus{}constant}\PYG{p}{(}\PYG{n}{X}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{19} \PYG{c+c1}{\PYGZsh{} Fit the model}
\PYG{n+ne}{\PYGZhy{}\PYGZhy{}\PYGZhy{}\PYGZgt{} }\PYG{l+m+mi}{20} \PYG{n}{model} \PYG{o}{=} \PYG{n}{OLS}\PYG{p}{(}\PYG{n}{data}\PYG{p}{[}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Output}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{]}\PYG{p}{,} \PYG{n}{X}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{p}{)}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{22} \PYG{c+c1}{\PYGZsh{} Prepare data for 3D surface plot}
\PYG{g+gWhitespace}{     }\PYG{l+m+mi}{23} \PYG{n}{x1} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linspace}\PYG{p}{(}\PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{l+m+mi}{30}\PYG{p}{)}

\PYG{n+ne}{NameError}: name \PYGZsq{}OLS\PYGZsq{} is not defined
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Quadratic effects}: the CCD allows for the estimation of both linear and quadratic effects, providing a more accurate representation of the response surface.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Optimisation}: by examining the response surface plot, we can identify the optimal settings for cleaning frequency and tilt angle to maximise the solar power plant’s output.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Balanced design}: the inclusion of centre and axial points ensures that the design is balanced and provides sufficient information to detect curvature in the response surface.

\end{itemize}

\sphinxAtStartPar
This is a simple example of response surface methodology (RSM) {[}\hyperlink{cite.bibliography:id17}{MMAC16}{]}, which is a collection of statistical and mathematical techniques useful for developing, improving, and optimizing processes . It uses quantitative data from appropriate experiments to determine regression models and identify the optimal conditions. The goal is to optimize this response by finding the best settings of the input variables.


\subsection{Space\sphinxhyphen{}filling designs}
\label{\detokenize{notebooks/design_of_experiments:space-filling-designs}}
\sphinxAtStartPar
Space\sphinxhyphen{}filling designs aim to cover the experimental space uniformly. This is particularly important for \sphinxstylestrong{computer experiments} {[}\hyperlink{cite.bibliography:id18}{SWNW03}{]} where the objective is to explore the entire input space efficiently. Unlike physical experiments where replication helps estimate variability and improve reliability, computer experiments often use deterministic simulators, making replication less useful. Instead, the focus is on spreading out the points as much as possible to gain comprehensive insights across the entire input space.


\subsubsection{Sobol sequences}
\label{\detokenize{notebooks/design_of_experiments:sobol-sequences}}
\sphinxAtStartPar
One example of designs for computer experiments are Sobol sequences. Sobol sequences are a type of low\sphinxhyphen{}discrepancy sequence used to generate space\sphinxhyphen{}filling designs. They provide a quasi\sphinxhyphen{}random sequence that covers the input space uniformly. Sobol sequences are particularly useful in high\sphinxhyphen{}dimensional spaces and are known for their good uniformity properties, making them ideal for numerical integration and simulation tasks.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{qmc}

\PYG{c+c1}{\PYGZsh{} Define the number of points and dimensions (2 in this case: Cleaning Frequency and Tilt Angle)}
\PYG{n}{n\PYGZus{}points} \PYG{o}{=} \PYG{l+m+mi}{20}
\PYG{n}{dimensions} \PYG{o}{=} \PYG{l+m+mi}{2}

\PYG{c+c1}{\PYGZsh{} Generate Sobol sequence}
\PYG{n}{sobol} \PYG{o}{=} \PYG{n}{qmc}\PYG{o}{.}\PYG{n}{Sobol}\PYG{p}{(}\PYG{n}{d}\PYG{o}{=}\PYG{n}{dimensions}\PYG{p}{,} \PYG{n}{scramble}\PYG{o}{=}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{points} \PYG{o}{=} \PYG{n}{sobol}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n\PYGZus{}points}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Scale points to the desired range (e.g., Cleaning Frequency: 0\PYGZhy{}4 times/month, Tilt Angle: 0\PYGZhy{}35 degrees)}
\PYG{n}{cleaning\PYGZus{}frequency} \PYG{o}{=} \PYG{n}{points}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{4}  \PYG{c+c1}{\PYGZsh{} Scale to 0\PYGZhy{}4}
\PYG{n}{tilt\PYGZus{}angle} \PYG{o}{=} \PYG{n}{points}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{35}         \PYG{c+c1}{\PYGZsh{} Scale to 0\PYGZhy{}35}

\PYG{c+c1}{\PYGZsh{} Plot the space\PYGZhy{}filling design}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{cleaning\PYGZus{}frequency}\PYG{p}{,} \PYG{n}{tilt\PYGZus{}angle}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning Frequency (times/month)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt Angle (degrees)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{74e6543d68671a54d679aeda8d9c29b1ae88ffd92b75ec010496eafff07d6fb4}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}

\subsubsection{Latin hypercubes}
\label{\detokenize{notebooks/design_of_experiments:latin-hypercubes}}
\sphinxAtStartPar
Latin Hypercube Sampling (LHS) is another method used to create space\sphinxhyphen{}filling designs. It divides the range of each input variable into equal intervals and ensures that each interval is sampled exactly once. This approach ensures a more uniform coverage of the input space compared to simple random sampling. LHS is particularly effective in reducing variance and ensuring that the sample points are well\sphinxhyphen{}distributed across the entire range of each input variable.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{from} \PYG{n+nn}{scipy}\PYG{n+nn}{.}\PYG{n+nn}{stats} \PYG{k+kn}{import} \PYG{n}{qmc}

\PYG{c+c1}{\PYGZsh{} Define the number of points and dimensions (2 in this case: Cleaning Frequency and Tilt Angle)}
\PYG{n}{n\PYGZus{}points} \PYG{o}{=} \PYG{l+m+mi}{20}
\PYG{n}{dimensions} \PYG{o}{=} \PYG{l+m+mi}{2}

\PYG{c+c1}{\PYGZsh{} Generate Latin Hypercube Sampling points}
\PYG{n}{lhs} \PYG{o}{=} \PYG{n}{qmc}\PYG{o}{.}\PYG{n}{LatinHypercube}\PYG{p}{(}\PYG{n}{d}\PYG{o}{=}\PYG{n}{dimensions}\PYG{p}{)}
\PYG{n}{points} \PYG{o}{=} \PYG{n}{lhs}\PYG{o}{.}\PYG{n}{random}\PYG{p}{(}\PYG{n}{n}\PYG{o}{=}\PYG{n}{n\PYGZus{}points}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Scale points to the desired range (e.g., Cleaning Frequency: 0\PYGZhy{}4 times/month, Tilt Angle: 0\PYGZhy{}35 degrees)}
\PYG{n}{cleaning\PYGZus{}frequency} \PYG{o}{=} \PYG{n}{points}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{0}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{4}  \PYG{c+c1}{\PYGZsh{} Scale to 0\PYGZhy{}4}
\PYG{n}{tilt\PYGZus{}angle} \PYG{o}{=} \PYG{n}{points}\PYG{p}{[}\PYG{p}{:}\PYG{p}{,} \PYG{l+m+mi}{1}\PYG{p}{]} \PYG{o}{*} \PYG{l+m+mi}{35}         \PYG{c+c1}{\PYGZsh{} Scale to 0\PYGZhy{}35}

\PYG{c+c1}{\PYGZsh{} Plot the space\PYGZhy{}filling design}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{5}\PYG{p}{,} \PYG{l+m+mi}{5}\PYG{p}{)}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{scatter}\PYG{p}{(}\PYG{n}{cleaning\PYGZus{}frequency}\PYG{p}{,} \PYG{n}{tilt\PYGZus{}angle}\PYG{p}{,} \PYG{n}{c}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{k}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{linewidths}\PYG{o}{=}\PYG{l+m+mi}{5}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Cleaning Frequency (times/month)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Tilt Angle (degrees)}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{grid}\PYG{p}{(}\PYG{k+kc}{True}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{fa2688114cf4e2c6ab47018d6c713a17bf37d56422f0d1655a245cdb341450e8}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
Space\sphinxhyphen{}filling designs are essential for computer experiments where the goal is to explore the entire input space efficiently. By ensuring uniform coverage, these designs provide comprehensive insights into the system’s behaviour, improving the accuracy and robustness of predictive models. Both Sobol sequences and Latin Hypercube Sampling are effective methods for generating space\sphinxhyphen{}filling designs, each with its unique advantages in providing uniform and well\sphinxhyphen{}distributed sample points across the input space.


\section{Design optimality}
\label{\detokenize{notebooks/design_of_experiments:design-optimality}}
\sphinxAtStartPar
Design optimality involves selecting an experimental design that provides the most information about the parameters of interest with the least experimental effort. Optimal designs are tailored to achieve specific statistical goals, ensuring that experiments are both efficient and informative. Here are some common criteria used to achieve design optimality:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{D\sphinxhyphen{}optimality}: This criterion maximizes the determinant of the information matrix. A D\sphinxhyphen{}optimal design provides the most precise parameter estimates by ensuring that the volume of the confidence ellipsoid for the estimated parameters is minimized. In essence, it spreads the experimental points in such a way that they collectively capture as much information as possible about the parameters.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{A\sphinxhyphen{}optimality}: This criterion minimizes the trace of the inverse information matrix. By focusing on reducing the average variance of the parameter estimates, A\sphinxhyphen{}optimal designs aim to make the overall estimation process more efficient. This means that the parameters can be estimated with lower average uncertainty.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{G\sphinxhyphen{}optimality}: This criterion minimizes the maximum prediction variance within the design space. G\sphinxhyphen{}optimal designs are concerned with ensuring that the model’s predictions are as accurate as possible across the entire experimental region. This approach is particularly useful when the goal is to have a model that performs well uniformly across the design space.

\end{itemize}

\sphinxstepscope


\chapter{Active Learning}
\label{\detokenize{notebooks/active_learning:active-learning}}\label{\detokenize{notebooks/active_learning::doc}}
\sphinxAtStartPar
Active learning is a machine learning technique that aims to select the most informative data points for training a model. In the context of causal inference, active learning can be particularly useful for efficiently estimating parameters when data collection is limited or expensive. By selectively choosing the most informative samples, we can improve the estimation accuracy of causal effects with fewer data points.

\sphinxAtStartPar
Active learning involves iteratively selecting the most informative data points to be labeled and added to the training set. The main scenarios of active learning include:
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Pool\sphinxhyphen{}based active learning}: starting with a large pool of unlabeled data, the algorithm selects the most informative samples to label.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Stream\sphinxhyphen{}based active learning}: samples arrive in a stream, and the algorithm decides whether to label each incoming sample.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Query synthesis}: the algorithm generates new samples to query an oracle for their labels.

\end{enumerate}

\sphinxAtStartPar
Consider the case where we want to estimate the effect of a variable using a linear model of the kind:
\label{equation:notebooks/active_learning:e82d94a7-0794-4157-a65f-47efe639775d}\begin{equation}
    y = X \beta + \epsilon
\end{equation}
\sphinxAtStartPar
where:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\(y\) is the response variable.

\item {} 
\sphinxAtStartPar
\(X\) is the design matrix containing the input variables.

\item {} 
\sphinxAtStartPar
\(\beta\) is the vector of coefficients.

\item {} 
\sphinxAtStartPar
\(\epsilon\) is the error term, assumed to be normally distributed with mean zero and variance \(\sigma^2\).

\end{itemize}

\sphinxAtStartPar
The goal is to estimate the coefficients \(\beta\) using the observed data. The variance of the coefficients \(\beta\) can be computed as:
\label{equation:notebooks/active_learning:eb593e10-a2aa-49bf-a79b-f48b7470338f}\begin{equation}
    \text{Var}(\beta) = \sigma^2 (X^T X)^{-1}
\end{equation}
\sphinxAtStartPar
where \(\sigma^2\) is the variance of the residuals, and \((X^T X)^{-1}\) is the inverse of the information matrix.

\sphinxAtStartPar
In the context of linear models, \sphinxstylestrong{prediction variance} is a measure of the uncertainty associated with the predictions made by the model.
In active learning, we can use the prediction variance to identify the most informative data points. The idea is to select the points that, when added to the training set, will most reduce the uncertainty in the model’s predictions. This is connected with the concept of \sphinxstylestrong{D\sphinxhyphen{}optimality} discussed in the previous chapter. Indeed, using active learning to select the most informative data points based on prediction variance is analogous to D\sphinxhyphen{}optimality. By choosing points that maximize the reduction in prediction variance, we effectively seek to create a design that is close to D\sphinxhyphen{}optimal.

\sphinxAtStartPar
Let’s create a simple function to compute the prediction variance for a linear model

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{k+kn}{import} \PYG{n+nn}{numpy} \PYG{k}{as} \PYG{n+nn}{np}
\PYG{k+kn}{from} \PYG{n+nn}{sklearn}\PYG{n+nn}{.}\PYG{n+nn}{linear\PYGZus{}model} \PYG{k+kn}{import} \PYG{n}{LinearRegression}
\PYG{k+kn}{import} \PYG{n+nn}{matplotlib}\PYG{n+nn}{.}\PYG{n+nn}{pyplot} \PYG{k}{as} \PYG{n+nn}{plt}

\PYG{c+c1}{\PYGZsh{} Function to compute variance of coefficients}
\PYG{k}{def} \PYG{n+nf}{compute\PYGZus{}variance}\PYG{p}{(}\PYG{n}{X}\PYG{p}{,} \PYG{n}{sigma\PYGZus{}squared}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{XTX\PYGZus{}inv} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{X}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{X}\PYG{p}{)}
    \PYG{k}{return} \PYG{n}{sigma\PYGZus{}squared} \PYG{o}{*} \PYG{n}{XTX\PYGZus{}inv}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Next, we define the function that performs one iteration of active learning. Now, we assume we are in the \sphinxstylestrong{pool\sphinxhyphen{}based} setting of active learning.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Function to perform active learning iteration}
\PYG{k}{def} \PYG{n+nf}{active\PYGZus{}learning\PYGZus{}iteration}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Train linear regression model}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred\PYGZus{}initial} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{)}
    \PYG{n}{residuals} \PYG{o}{=} \PYG{n}{y\PYGZus{}initial} \PYG{o}{\PYGZhy{}} \PYG{n}{y\PYGZus{}pred\PYGZus{}initial}
    \PYG{n}{sigma\PYGZus{}squared} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{residuals}\PYG{p}{)}
    \PYG{n}{variance} \PYG{o}{=} \PYG{n}{compute\PYGZus{}variance}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{sigma\PYGZus{}squared}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Select the most informative data points (highest prediction variance)}
    \PYG{n}{prediction\PYGZus{}variances} \PYG{o}{=} \PYG{n}{sigma\PYGZus{}squared} \PYG{o}{*} \PYG{n}{np}\PYG{o}{.}\PYG{n}{sum}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X\PYGZus{}pool} \PYG{o}{@} \PYG{n}{np}\PYG{o}{.}\PYG{n}{linalg}\PYG{o}{.}\PYG{n}{inv}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{o}{.}\PYG{n}{T} \PYG{o}{@} \PYG{n}{X\PYGZus{}initial}\PYG{p}{)}\PYG{p}{)} \PYG{o}{*} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{1}\PYG{p}{)}
    \PYG{n}{most\PYGZus{}informative\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{argmax}\PYG{p}{(}\PYG{n}{prediction\PYGZus{}variances}\PYG{p}{)}
    \PYG{n}{X\PYGZus{}most\PYGZus{}informative} \PYG{o}{=} \PYG{n}{X\PYGZus{}pool}\PYG{p}{[}\PYG{n}{most\PYGZus{}informative\PYGZus{}idx}\PYG{p}{]}
    \PYG{n}{y\PYGZus{}most\PYGZus{}informative} \PYG{o}{=} \PYG{n}{y\PYGZus{}pool}\PYG{p}{[}\PYG{n}{most\PYGZus{}informative\PYGZus{}idx}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} Add to the labeled dataset}
    \PYG{n}{X\PYGZus{}initial} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}most\PYGZus{}informative}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}initial} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}most\PYGZus{}informative}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Remove from the pool}
    \PYG{n}{X\PYGZus{}pool} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{most\PYGZus{}informative\PYGZus{}idx}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pool} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{y\PYGZus{}pool}\PYG{p}{,} \PYG{n}{most\PYGZus{}informative\PYGZus{}idx}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{variance}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
What do we do, iteratively, in the active learning routine:
\begin{itemize}
\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Training the model}: we fit a linear regression model to the initial labeled dataset.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Computing residuals and variance}: the residuals (differences between actual and predicted values) are computed to estimate the variance \(\sigma^2\) of the residuals.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Selecting the most Informative data point}: the prediction variances are computed for the pool of unlabeled data points. The point with the highest prediction variance is selected as the most informative.

\item {} 
\sphinxAtStartPar
\sphinxstylestrong{Updating the labeled dataset}: the selected data point is added to the labeled dataset, and it is removed from the pool.

\end{itemize}

\sphinxAtStartPar
For comparison, we also define a random sampling approach, where instead of selecting points with high prediction variance, we just sample data at random from the pool.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Function to perform random sampling iteration}
\PYG{k}{def} \PYG{n+nf}{random\PYGZus{}sampling\PYGZus{}iteration}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{)}\PYG{p}{:}
    \PYG{c+c1}{\PYGZsh{} Train linear regression model}
    \PYG{n}{model} \PYG{o}{=} \PYG{n}{LinearRegression}\PYG{p}{(}\PYG{p}{)}\PYG{o}{.}\PYG{n}{fit}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pred\PYGZus{}initial} \PYG{o}{=} \PYG{n}{model}\PYG{o}{.}\PYG{n}{predict}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{)}
    \PYG{n}{residuals} \PYG{o}{=} \PYG{n}{y\PYGZus{}initial} \PYG{o}{\PYGZhy{}} \PYG{n}{y\PYGZus{}pred\PYGZus{}initial}
    \PYG{n}{sigma\PYGZus{}squared} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{var}\PYG{p}{(}\PYG{n}{residuals}\PYG{p}{)}
    \PYG{n}{variance} \PYG{o}{=} \PYG{n}{compute\PYGZus{}variance}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{sigma\PYGZus{}squared}\PYG{p}{)}
    
    \PYG{c+c1}{\PYGZsh{} Randomly select a data point}
    \PYG{n}{random\PYGZus{}idx} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{X\PYGZus{}pool}\PYG{o}{.}\PYG{n}{shape}\PYG{p}{[}\PYG{l+m+mi}{0}\PYG{p}{]}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{X\PYGZus{}random} \PYG{o}{=} \PYG{n}{X\PYGZus{}pool}\PYG{p}{[}\PYG{n}{random\PYGZus{}idx}\PYG{p}{]}
    \PYG{n}{y\PYGZus{}random} \PYG{o}{=} \PYG{n}{y\PYGZus{}pool}\PYG{p}{[}\PYG{n}{random\PYGZus{}idx}\PYG{p}{]}

    \PYG{c+c1}{\PYGZsh{} Add to the labeled dataset}
    \PYG{n}{X\PYGZus{}initial} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{vstack}\PYG{p}{(}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}random}\PYG{p}{)}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}initial} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}random}\PYG{p}{)}

    \PYG{c+c1}{\PYGZsh{} Remove from the pool}
    \PYG{n}{X\PYGZus{}pool} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{random\PYGZus{}idx}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    \PYG{n}{y\PYGZus{}pool} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{delete}\PYG{p}{(}\PYG{n}{y\PYGZus{}pool}\PYG{p}{,} \PYG{n}{random\PYGZus{}idx}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
    
    \PYG{k}{return} \PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{np}\PYG{o}{.}\PYG{n}{diag}\PYG{p}{(}\PYG{n}{variance}\PYG{p}{)}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Now, we define the function to run the entire active learning simulation, comparing active learning with random sampling over multiple iterations and runs.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Function to perform the full active learning simulation}
\PYG{k}{def} \PYG{n+nf}{run\PYGZus{}active\PYGZus{}learning\PYGZus{}simulation\PYGZus{}with\PYGZus{}data\PYGZus{}regeneration}\PYG{p}{(}\PYG{n}{n\PYGZus{}runs}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{n\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{20}\PYG{p}{)}\PYG{p}{:}
    \PYG{n}{variances\PYGZus{}active\PYGZus{}runs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
    \PYG{n}{variances\PYGZus{}random\PYGZus{}runs} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

    \PYG{k}{for} \PYG{n}{run} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}runs}\PYG{p}{)}\PYG{p}{:}  \PYG{c+c1}{\PYGZsh{} Running multiple simulations with different seeds}
        \PYG{c+c1}{\PYGZsh{} Generate synthetic data}
        \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{seed}\PYG{p}{(}\PYG{n}{run}\PYG{p}{)}
        \PYG{n}{X} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{rand}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{,} \PYG{l+m+mi}{2}\PYG{p}{)}
        \PYG{n}{true\PYGZus{}coefficients} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{p}{[}\PYG{l+m+mf}{1.5}\PYG{p}{,} \PYG{o}{\PYGZhy{}}\PYG{l+m+mf}{2.0}\PYG{p}{]}\PYG{p}{)}
        \PYG{n}{y} \PYG{o}{=} \PYG{n}{X} \PYG{o}{@} \PYG{n}{true\PYGZus{}coefficients} \PYG{o}{+} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{randn}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)} \PYG{o}{*} \PYG{l+m+mf}{0.5}
        
        \PYG{c+c1}{\PYGZsh{} Initial small labeled dataset}
        \PYG{n}{initial\PYGZus{}indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{random}\PYG{o}{.}\PYG{n}{choice}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{,} \PYG{n}{size}\PYG{o}{=}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{replace}\PYG{o}{=}\PYG{k+kc}{False}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{n}{initial\PYGZus{}indices}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{p}{[}\PYG{n}{initial\PYGZus{}indices}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Remaining pool of unlabeled data}
        \PYG{n}{remaining\PYGZus{}indices} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{setdiff1d}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{l+m+mi}{1000}\PYG{p}{)}\PYG{p}{,} \PYG{n}{initial\PYGZus{}indices}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool} \PYG{o}{=} \PYG{n}{X}\PYG{p}{[}\PYG{n}{remaining\PYGZus{}indices}\PYG{p}{]}\PYG{p}{,} \PYG{n}{y}\PYG{p}{[}\PYG{n}{remaining\PYGZus{}indices}\PYG{p}{]}

        \PYG{c+c1}{\PYGZsh{} Initialize data for random sampling}
        \PYG{n}{X\PYGZus{}initial\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial\PYGZus{}random} \PYG{o}{=} \PYG{n}{X\PYGZus{}initial}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}
        \PYG{n}{X\PYGZus{}pool\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool\PYGZus{}random} \PYG{o}{=} \PYG{n}{X\PYGZus{}pool}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{o}{.}\PYG{n}{copy}\PYG{p}{(}\PYG{p}{)}

        \PYG{n}{variances\PYGZus{}active} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}
        \PYG{n}{variances\PYGZus{}random} \PYG{o}{=} \PYG{p}{[}\PYG{p}{]}

        \PYG{k}{for} \PYG{n}{iteration} \PYG{o+ow}{in} \PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{:}
            \PYG{c+c1}{\PYGZsh{} Active learning iteration}
            \PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{,} \PYG{n}{avg\PYGZus{}variance\PYGZus{}active} \PYG{o}{=} \PYG{n}{active\PYGZus{}learning\PYGZus{}iteration}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool}\PYG{p}{)}
            \PYG{n}{variances\PYGZus{}active}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{avg\PYGZus{}variance\PYGZus{}active}\PYG{p}{)}
    
            \PYG{c+c1}{\PYGZsh{} Random sampling iteration}
            \PYG{n}{X\PYGZus{}initial\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial\PYGZus{}random}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool\PYGZus{}random}\PYG{p}{,} \PYG{n}{avg\PYGZus{}variance\PYGZus{}random} \PYG{o}{=} \PYG{n}{random\PYGZus{}sampling\PYGZus{}iteration}\PYG{p}{(}\PYG{n}{X\PYGZus{}initial\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}initial\PYGZus{}random}\PYG{p}{,} \PYG{n}{X\PYGZus{}pool\PYGZus{}random}\PYG{p}{,} \PYG{n}{y\PYGZus{}pool\PYGZus{}random}\PYG{p}{)}
            \PYG{n}{variances\PYGZus{}random}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{avg\PYGZus{}variance\PYGZus{}random}\PYG{p}{)}

        \PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{variances\PYGZus{}active}\PYG{p}{)}
        \PYG{n}{variances\PYGZus{}random\PYGZus{}runs}\PYG{o}{.}\PYG{n}{append}\PYG{p}{(}\PYG{n}{variances\PYGZus{}random}\PYG{p}{)}

    \PYG{k}{return} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{p}{)}\PYG{p}{,} \PYG{n}{np}\PYG{o}{.}\PYG{n}{array}\PYG{p}{(}\PYG{n}{variances\PYGZus{}random\PYGZus{}runs}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
Finally, we run the simulation and plot the learning curves for active learning and random sampling.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
\PYG{c+c1}{\PYGZsh{} Run the simulation}
\PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{p}{,} \PYG{n}{variances\PYGZus{}random\PYGZus{}runs} \PYG{o}{=} \PYG{n}{run\PYGZus{}active\PYGZus{}learning\PYGZus{}simulation\PYGZus{}with\PYGZus{}data\PYGZus{}regeneration}\PYG{p}{(}\PYG{n}{n\PYGZus{}runs}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{,} \PYG{n}{n\PYGZus{}iterations}\PYG{o}{=}\PYG{l+m+mi}{50}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Compute means and confidence intervals}
\PYG{n}{mean\PYGZus{}variances\PYGZus{}active} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{mean\PYGZus{}variances\PYGZus{}random} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{mean}\PYG{p}{(}\PYG{n}{variances\PYGZus{}random\PYGZus{}runs}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{n}{ci\PYGZus{}variances\PYGZus{}active\PYGZus{}5} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{p}{,} \PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}variances\PYGZus{}active\PYGZus{}95} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{variances\PYGZus{}active\PYGZus{}runs}\PYG{p}{,} \PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}variances\PYGZus{}random\PYGZus{}5} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{variances\PYGZus{}random\PYGZus{}runs}\PYG{p}{,}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}
\PYG{n}{ci\PYGZus{}variances\PYGZus{}random\PYGZus{}95} \PYG{o}{=} \PYG{n}{np}\PYG{o}{.}\PYG{n}{percentile}\PYG{p}{(}\PYG{n}{variances\PYGZus{}random\PYGZus{}runs}\PYG{p}{,} \PYG{l+m+mi}{90}\PYG{p}{,} \PYG{n}{axis}\PYG{o}{=}\PYG{l+m+mi}{0}\PYG{p}{)}

\PYG{c+c1}{\PYGZsh{} Plot the learning curves with shaded regions for confidence intervals}
\PYG{n}{n\PYGZus{}iterations} \PYG{o}{=} \PYG{l+m+mi}{50}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{figure}\PYG{p}{(}\PYG{n}{figsize}\PYG{o}{=}\PYG{p}{(}\PYG{l+m+mi}{10}\PYG{p}{,} \PYG{l+m+mi}{6}\PYG{p}{)}\PYG{p}{,} \PYG{n}{dpi}\PYG{o}{=}\PYG{l+m+mi}{300}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}variances\PYGZus{}active}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Active Learning}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{o}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ci\PYGZus{}variances\PYGZus{}active\PYGZus{}5}\PYG{p}{,} \PYG{n}{ci\PYGZus{}variances\PYGZus{}active\PYGZus{}95}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{blue}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{plot}\PYG{p}{(}\PYG{n}{mean\PYGZus{}variances\PYGZus{}random}\PYG{p}{,} \PYG{n}{label}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Random Sampling}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{marker}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{d}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{fill\PYGZus{}between}\PYG{p}{(}\PYG{n+nb}{range}\PYG{p}{(}\PYG{n}{n\PYGZus{}iterations}\PYG{p}{)}\PYG{p}{,} \PYG{n}{ci\PYGZus{}variances\PYGZus{}random\PYGZus{}5}\PYG{p}{,} \PYG{n}{ci\PYGZus{}variances\PYGZus{}random\PYGZus{}95}\PYG{p}{,} \PYG{n}{color}\PYG{o}{=}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{orange}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{,} \PYG{n}{alpha}\PYG{o}{=}\PYG{l+m+mf}{0.2}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{xlabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Iteration}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{ylabel}\PYG{p}{(}\PYG{l+s+s1}{\PYGZsq{}}\PYG{l+s+s1}{Average Variance of Coefficients}\PYG{l+s+s1}{\PYGZsq{}}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{legend}\PYG{p}{(}\PYG{p}{)}
\PYG{n}{plt}\PYG{o}{.}\PYG{n}{show}\PYG{p}{(}\PYG{p}{)}
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\noindent\sphinxincludegraphics{{9dd93285ea963f696041088fb80d1448a80d1373687e5cbe163acaa2ffb035a3}.png}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\sphinxAtStartPar
We can see how active learning is a powerful technique for efficiently estimating parameters in causal inference, especially when data collection is limited or expensive. By selectively choosing the most informative samples, active learning can significantly improve the estimation accuracy with fewer data points compared to random sampling. This tutorial demonstrated the application of active learning in a linear regression context, highlighting its advantages over random sampling.

\sphinxstepscope


\part{Other}

\sphinxstepscope


\chapter{References}
\label{\detokenize{bibliography:references}}\label{\detokenize{bibliography::doc}}
\begin{sphinxthebibliography}{Hyvarine}
\bibitem[BIM20]{bibliography:id8}
\sphinxAtStartPar
Derek W Bunn, John N Inekwe, and David MacGeehan. Analysis of the fundamental predictability of prices in the british balancing market. \sphinxstyleemphasis{IEEE Transactions on Power Systems}, 36(2):1309–1316, 2020.
\bibitem[CCD+18]{bibliography:id9}
\sphinxAtStartPar
Victor Chernozhukov, Denis Chetverikov, Mert Demirer, Esther Duflo, Christian Hansen, Whitney Newey, and James Robins. Double/debiased machine learning for treatment and structural parameters. \sphinxstyleemphasis{The Econometrics Journal}, 21(1):C1–C68, 2018.
\bibitem[GXBH15]{bibliography:id19}
\sphinxAtStartPar
Huan Gui, Ya Xu, Anmol Bhasin, and Jiawei Han. Network a/b testing: from sampling to estimation. In \sphinxstyleemphasis{Proceedings of the 24th International Conference on World Wide Web}, 399–409. 2015.
\bibitem[HyvarinenSH08]{bibliography:id6}
\sphinxAtStartPar
Aapo Hyvärinen, Shohei Shimizu, and Patrik O Hoyer. Causal modelling combining instantaneous and lagged effects: an identifiable model based on non\sphinxhyphen{}gaussianity. In \sphinxstyleemphasis{Proceedings of the 25th international conference on Machine learning}, 424–431. 2008.
\bibitem[HyvarinenZSH10]{bibliography:id7}
\sphinxAtStartPar
Aapo Hyvärinen, Kun Zhang, Shohei Shimizu, and Patrik O Hoyer. Estimation of a structural vector autoregression model using non\sphinxhyphen{}gaussianity. \sphinxstyleemphasis{Journal of Machine Learning Research}, 2010.
\bibitem[IIZ+23]{bibliography:id3}
\sphinxAtStartPar
Takashi Ikeuchi, Mayumi Ide, Yan Zeng, Takashi Nicholas Maeda, and Shohei Shimizu. Python package for causal discovery based on lingam. \sphinxstyleemphasis{Journal of Machine Learning Research}, 24(14):1–8, 2023.
\bibitem[JonssonPM10]{bibliography:id11}
\sphinxAtStartPar
Tryggvi Jónsson, Pierre Pinson, and Henrik Madsen. On the market impact of wind energy forecasts. \sphinxstyleemphasis{Energy Economics}, 32(2):313–320, 2010.
\bibitem[Mon17]{bibliography:id16}
\sphinxAtStartPar
Douglas C Montgomery. \sphinxstyleemphasis{Design and analysis of experiments}. John wiley \& sons, 2017.
\bibitem[MMAC16]{bibliography:id17}
\sphinxAtStartPar
Raymond H Myers, Douglas C Montgomery, and Christine M Anderson\sphinxhyphen{}Cook. \sphinxstyleemphasis{Response surface methodology: process and product optimization using designed experiments}. John Wiley \& Sons, 2016.
\bibitem[PMJScholkopf14]{bibliography:id5}
\sphinxAtStartPar
Jonas Peters, Joris M Mooij, Dominik Janzing, and Bernhard Schölkopf. Causal discovery with continuous additive noise models. \sphinxstyleemphasis{Journal of Machine Learning Research}, 2014.
\bibitem[RR83]{bibliography:id12}
\sphinxAtStartPar
Paul R Rosenbaum and Donald B Rubin. The central role of the propensity score in observational studies for causal effects. \sphinxstyleemphasis{Biometrika}, 70(1):41–55, 1983.
\bibitem[RNK+19]{bibliography:id15}
\sphinxAtStartPar
Jakob Runge, Peer Nowack, Marlene Kretschmer, Seth Flaxman, and Dino Sejdinovic. Detecting and quantifying causal associations in large nonlinear time series datasets. \sphinxstyleemphasis{Science advances}, 5(11):eaau4996, 2019.
\bibitem[SWNW03]{bibliography:id18}
\sphinxAtStartPar
Thomas J Santner, Brian J Williams, William I Notz, and Brain J Williams. \sphinxstyleemphasis{The design and analysis of computer experiments}. Volume 1. Springer, 2003.
\bibitem[SHHyvarinen+06]{bibliography:id4}
\sphinxAtStartPar
Shohei Shimizu, Patrik O Hoyer, Aapo Hyvärinen, Antti Kerminen, and Michael Jordan. A linear non\sphinxhyphen{}gaussian acyclic model for causal discovery. \sphinxstyleemphasis{Journal of Machine Learning Research}, 2006.
\bibitem[Spi01]{bibliography:id14}
\sphinxAtStartPar
Peter Spirtes. An anytime algorithm for causal inference. In \sphinxstyleemphasis{International Workshop on Artificial Intelligence and Statistics}, 278–285. PMLR, 2001.
\bibitem[SGS01]{bibliography:id13}
\sphinxAtStartPar
Peter Spirtes, Clark Glymour, and Richard Scheines. \sphinxstyleemphasis{Causation, prediction, and search}. MIT press, 2001.
\end{sphinxthebibliography}







\renewcommand{\indexname}{Index}
\printindex
\end{document}