
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>2. Linear Regression &#8212; Applied Causal Inference with Examples from Electricity Markets</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/review_linear_models';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="3. Machine Learning" href="review_ML.html" />
    <link rel="prev" title="1. Probability Theory and Statistics" href="review_stats.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo_applied_causal_inference.png" class="logo__image only-light" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>
    <script>document.write(`<img src="../_static/logo_applied_causal_inference.png" class="logo__image only-dark" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Applied Causal Inference for Electricity Markets
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Crash course on Stats and ML</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="review_stats.html">1. Probability Theory and Statistics</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_ML.html">3. Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="motivation.html">4. Motivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide.html">5. What to expect from each chapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Basic Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="correlation_vs_causation.html">6. Correlation vs. Causation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAG.html">7. Causal Representations</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_dag_structures.html">8. Basic Causal Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">9. Definitions and Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II. Causal Discovery</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_discovery.html">10. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_direct_lingam.html">11. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_resit.html">12. Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_varlingam.html">13. Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="structural_breaks_example.html">14. Structural Breaks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III. Causal Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_inference.html">15. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="instrumental_variables.html">16. Instrumental Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="propensity_scores.html">17. Propensity Score Matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="double_machine_learning.html">18. Double Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="diff_in_diff.html">19. Difference-in-Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="interrupted_time_series.html">20. Interrupted Time Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IV. Interpretability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_interpretability.html">21. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="partial_dependency_plots.html">22. Partial Dependence Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="accumulated_local_effects.html">23. Accumulated Local Effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="impulse_response_functions.html">24. Impulse Response Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="shapley.html">25. Shapley Additive Explanations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">V. Experiments and Data Collection</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_designs.html">26. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="AB_testing.html">27. A/B Testing</a></li>
<li class="toctree-l1"><a class="reference internal" href="bandits.html">28. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html">29. Design of Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="active_learning.html">30. Active Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnotebooks/review_linear_models.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/review_linear_models.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Linear Regression</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-preamble">2.1. A preamble</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#now-to-linear-regression">2.2. Now to linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">2.2.1. Ordinary least squares</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-and-variance-of-the-least-square-estimators">2.2.1.1. Expected value and variance of the least square estimators</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-sigma-2">2.2.1.2. Estimation of <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing-in-multiple-linear-regression">2.3. Hypothesis testing in multiple linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-for-significance-of-regression-anova">2.3.1. Tests for significance of regression (ANOVA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-sum-of-squares-method-tests-on-groups-of-coefficients">2.3.2. Extra sum of squares method (tests on groups of coefficients)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-on-individual-regression-coefficients">2.3.3. Tests on individual regression coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">2.3.4. Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-on-individual-regression-coefficients">2.3.5. Confidence intervals on individual regression coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-confidence-region-on-the-regression-coefficients-confidence-ellipsoid">2.3.5.1. Joint confidence region on the regression coefficients (confidence ellipsoid)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#mahalanobis-distance">2.3.5.1.1. Mahalanobis distance</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-ellipsoid-we-can-define-an-ellipsoid-for-the-regression-coefficients-boldsymbol-beta-as">2.3.5.2. Confidence ellipsoid We can define an ellipsoid for the regression coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> as</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-expectations-causality-observational-data-vs-designed-experiments">2.4. Conditional expectations/causality: observational data vs. designed experiments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-criteria-aic-and-bic">2.5. Model selection criteria: AIC and BIC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-methods">2.6. Regularization methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l1-penalty">2.6.1. Lasso Regression (L1 Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty">2.6.2. Ridge Regression (L2 Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-net-combination-of-l1-and-l2-penalties">2.6.3. Elastic Net (Combination of L1 and L2 Penalties)</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="linear-regression">
<h1><span class="section-number">2. </span>Linear Regression<a class="headerlink" href="#linear-regression" title="Link to this heading">#</a></h1>
<section id="a-preamble">
<h2><span class="section-number">2.1. </span>A preamble<a class="headerlink" href="#a-preamble" title="Link to this heading">#</a></h2>
<p>Linear models are extremely useful becase they can be used to approximate complex relationships with easy and interpretable models. When employing linear models to learn a function <span class="math notranslate nohighlight">\(f\)</span> relating <span class="math notranslate nohighlight">\(Y\)</span> to <span class="math notranslate nohighlight">\(X\)</span>, we do not need to necessarily assume that the relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span> is linear. We are just looking for the best linear approximation to the true relationship, whatever that might be. Taylor’s theorem gives reasons to believe that a linear model is a sensible approximation even for more complex functions, at least locally. Indeed, if the true regression function <span class="math notranslate nohighlight">\(f(x)\)</span> is a smooth function, given a specific value <span class="math notranslate nohighlight">\(x_0\)</span>, we can expand the function as</p>
<div class="amsmath math notranslate nohighlight" id="equation-378d3654-700c-4362-bd7a-1ec3196f7516">
<span class="eqno">(2.1)<a class="headerlink" href="#equation-378d3654-700c-4362-bd7a-1ec3196f7516" title="Permalink to this equation">#</a></span>\[\begin{equation}
    f(x) = f(x_0) + \frac{f'(x_0)}{1!}(x-x_0) + \frac{f''(x_0)}{2!}(x-x_0)^2 + \cdots
\end{equation}\]</div>
<p>This expansion breaks down the function into an infinite sum of terms based on the function’s derivatives at <span class="math notranslate nohighlight">\(x_0\)</span>. The approximation starts with the function’s value at <span class="math notranslate nohighlight">\(x_0\)</span>, then adds adjustments based on how the function changes (its derivatives) as you move away from <span class="math notranslate nohighlight">\(x_0\)</span>. For <span class="math notranslate nohighlight">\(x\)</span> close enough to <span class="math notranslate nohighlight">\(x_0\)</span>, we can get away with truncating the series at first order, as in</p>
<div class="amsmath math notranslate nohighlight" id="equation-66853874-93c8-4687-989e-ea9a913142d3">
<span class="eqno">(2.2)<a class="headerlink" href="#equation-66853874-93c8-4687-989e-ea9a913142d3" title="Permalink to this equation">#</a></span>\[\begin{equation}
    f(x) \approx f(x_0) + f'(x_0)(x-x_0)
\end{equation}\]</div>
<p>When you truncate the Taylor series after the first derivative term, we are essentially creating a linear model. This model approximates the function <span class="math notranslate nohighlight">\(f(x)\)</span> using a straight line that tangentially matches the function’s slope at <span class="math notranslate nohighlight">\(x_0\)</span>. This approach is valid as long as the higher-order terms (like the quadratic term and beyond) are negligible, which usually means <span class="math notranslate nohighlight">\(x\)</span> is close enough to <span class="math notranslate nohighlight">\(x_0\)</span>. Thus, while a  linear approximation may work well locally (near <span class="math notranslate nohighlight">\(x_0\)</span>), extending this approximation globally (over a wide range of <span class="math notranslate nohighlight">\(x\)</span> values) may not always be accurate unless the function is nearly linear over that range. The key to a successful linear approximation lies in determining how “close” <span class="math notranslate nohighlight">\(x\)</span> must be to <span class="math notranslate nohighlight">\(x_0\)</span> for the higher-order terms to be negligible. For a linear approximation to be valid, we want the influence of this term (and all higher-order terms) to be small compared to the first derivative term. We formalize this by imposing that the linear term dominates over the quadratic</p>
<div class="amsmath math notranslate nohighlight" id="equation-1df98d31-8c68-48e4-83fc-8be27b07f910">
<span class="eqno">(2.3)<a class="headerlink" href="#equation-1df98d31-8c68-48e4-83fc-8be27b07f910" title="Permalink to this equation">#</a></span>\[\begin{equation}
    |x-x_0|f'(x_0) \gg \frac{f''(x_0)}{2}
\end{equation}\]</div>
<p>which is true if</p>
<div class="amsmath math notranslate nohighlight" id="equation-ad541963-19f3-453c-9f24-26fd388ca7cb">
<span class="eqno">(2.4)<a class="headerlink" href="#equation-ad541963-19f3-453c-9f24-26fd388ca7cb" title="Permalink to this equation">#</a></span>\[\begin{equation}
    2\frac{f'(x_0)}{f''(x_0)} \gg |x-x_0|
\end{equation}\]</div>
<p>This tells us that the distance between <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(x_0\)</span> must be smaller than twice the ratio of the magnitude of the first derivative to the magnitude of the second derivative for the linear approximation to hold effectively. The exact bounds of “close enough” depend on the relative sizes of the first and second derivatives of the function at <span class="math notranslate nohighlight">\(x_0\)</span>, providing a rule of thumb for when a linear model is likely to be a good approximation.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define a nonlinear function and its linear approximation</span>
<span class="k">def</span> <span class="nf">nonlinear_function</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">linear_approximation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x0</span><span class="p">):</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sin</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">cos</span><span class="p">(</span><span class="n">x0</span><span class="p">)</span> <span class="o">*</span> <span class="p">(</span><span class="n">x</span> <span class="o">-</span> <span class="n">x0</span><span class="p">)</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span><span class="p">,</span> <span class="mi">400</span><span class="p">)</span>
<span class="n">x0</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">pi</span> <span class="o">/</span> <span class="mi">2</span>  <span class="c1"># point of approximation</span>

<span class="n">y</span> <span class="o">=</span> <span class="n">nonlinear_function</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">y_approx</span> <span class="o">=</span> <span class="n">linear_approximation</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">x0</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Nonlinear Function&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y_approx</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear Approximation (Taylor)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">axvline</span><span class="p">(</span><span class="n">x0</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;:&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Approximation Point&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Nonlinear Function and Its Linear Approximation&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;x&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;f(x)&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/d70570101230f888379ca68e1fac00a9c565edd73372783006dacf7082c4fc91.png" src="../_images/d70570101230f888379ca68e1fac00a9c565edd73372783006dacf7082c4fc91.png" />
</div>
</div>
</section>
<section id="now-to-linear-regression">
<h2><span class="section-number">2.2. </span>Now to linear regression<a class="headerlink" href="#now-to-linear-regression" title="Link to this heading">#</a></h2>
<p>A linear regression model is a model used to analyze the relationship between a dependent variable (response) and one or more independent variables (predictors or covariates). In the case of one predictor, the linear regression model is referred to as a simple linear regression model, and it is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-255d70fe-aa0a-43eb-bdf7-60f5af714351">
<span class="eqno">(2.5)<a class="headerlink" href="#equation-255d70fe-aa0a-43eb-bdf7-60f5af714351" title="Permalink to this equation">#</a></span>\[\begin{equation}
    y_i = \beta_0 + \beta_1 x_i + \epsilon_i
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(y_i\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th observation of the response variable, <span class="math notranslate nohighlight">\(x\)</span> is the <span class="math notranslate nohighlight">\(i\)</span>th observation of the predictor, <span class="math notranslate nohighlight">\(\beta_0\)</span> is the intercept (the value of <span class="math notranslate nohighlight">\(y\)</span> when <span class="math notranslate nohighlight">\(x\)</span> is zero), <span class="math notranslate nohighlight">\(\beta_1\)</span> is the regression coefficient (the expected change in <span class="math notranslate nohighlight">\(y\)</span> per unit change in <span class="math notranslate nohighlight">\(x\)</span>), and <span class="math notranslate nohighlight">\(\epsilon\)</span> represents the error term, accounting for the variability in <span class="math notranslate nohighlight">\(y\)</span> that cannot be explained by the linear relationship with <span class="math notranslate nohighlight">\(x\)</span>. In most of the cases, we will assume <span class="math notranslate nohighlight">\(\epsilon\)</span> to be normally distributed around zero with finite variance, <span class="math notranslate nohighlight">\(\epsilon \sim \mathcal{N}(0, \sigma^2)\)</span>. In the case of <span class="math notranslate nohighlight">\(p\)</span> predictors and <span class="math notranslate nohighlight">\(n\)</span> observations, the model is referred to as a multiple linear regression model, and we can express it in matrix notation, as in</p>
<div class="amsmath math notranslate nohighlight" id="equation-78110cc4-a25b-4858-a5b9-617293688d4c">
<span class="eqno">(2.6)<a class="headerlink" href="#equation-78110cc4-a25b-4858-a5b9-617293688d4c" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}
\end{equation}\]</div>
<p>where</p>
<div class="amsmath math notranslate nohighlight">
\[\begin{equation*}
    \mathbf{y}=\left[\begin{array}{c}
y_1 \\
y_2 \\
\vdots \\
y_n
\end{array}\right], \quad \mathbf{X}=\left[\begin{array}{ccccc}
1 &amp; x_{11} &amp; x_{12} &amp; \cdots &amp; x_{1 p} \\
1 &amp; x_{21} &amp; x_{22} &amp; \cdots &amp; x_{2 p} \\
\vdots &amp; \vdots &amp; &amp; \vdots \\
1 &amp; x_{n 1} &amp; x_{n 2} &amp; \cdots &amp; x_{n p}
\end{array}\right], \quad \boldsymbol{\beta}=\left[\begin{array}{c}
\beta_0 \\
\beta_1 \\
\vdots \\
\beta_p
\end{array}\right], \quad \text { and } \quad \boldsymbol{\epsilon}=\left[\begin{array}{c}
\epsilon_1 \\
\epsilon_2 \\
\vdots \\
\epsilon_n
\end{array}\right]
\end{equation*}\]</div>
<p><span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is a <span class="math notranslate nohighlight">\(n \times 1\)</span> vector of response variables, <span class="math notranslate nohighlight">\(\mathbf{X}\)</span> is a <span class="math notranslate nohighlight">\(n \times (p + 1)\)</span> model matrix, <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is a <span class="math notranslate nohighlight">\((p + 1) \times 1\)</span> vector of regression coefficients, and <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is a <span class="math notranslate nohighlight">\(n \times 1\)</span> vector representing the noise, with covariance matrix <span class="math notranslate nohighlight">\(\sigma^2 \mathbf{I}\)</span>. If the predictors and the response are centered (for example by subtracting the mean), the intercept term can be removed from the model. In that case, the size of the model matrix becomes <span class="math notranslate nohighlight">\(n \times p\)</span>, and <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> a <span class="math notranslate nohighlight">\(p \times 1\)</span> vector. We will assume this is the case in the following sections.</p>
<p>Linear regression models are based on several <strong>key assumptions</strong>:</p>
<ul class="simple">
<li><p><strong>Linearity</strong>: the relationship between the predictors and the response is linear. This implies that a change in a predictor leads to a proportional change in the response. While real-life processes are rarely purely linear, we can often assume local linearity. This means that the linearity assumption holds within a reasonably limited range of the design space, recognizing that the relationship may not be linear over a wider range.</p></li>
<li><p><strong>Independence</strong>: observations are independent of each other. In other words, the observations do not influence each other. This assumption can be violated in many situations, particularly in time series data or spatial data where there might be autocorrelation (i.e., the value of a variable at one point in time or space is correlated with its values at other points).</p></li>
<li><p><strong>Homoscedasticity</strong>: the variance of the error terms (residuals) is constant across all levels of the independent variables. This condition, known as homoscedasticity, implies that the spread of the residuals should be roughly the same for all values of the predictors. If the variance of the residuals changes with the level of the predictors, the condition is known as heteroscedasticity, which can lead to inefficiencies and bias in the estimation of parameters.</p></li>
<li><p><strong>Normality of the error terms</strong>: this assumption is particularly important for hypothesis testing and creating confidence intervals. It’s crucial to note that this assumption pertains to the errors, not necessarily to the distributions of the predictors or the response variable. While linear regression can be robust to mild violations of this assumption, severe departures can affect the reliability of inference procedures</p></li>
</ul>
<section id="ordinary-least-squares">
<h3><span class="section-number">2.2.1. </span>Ordinary least squares<a class="headerlink" href="#ordinary-least-squares" title="Link to this heading">#</a></h3>
<p>In regression analysis, the most common method to estimate the unknown model parameters <span class="math notranslate nohighlight">\(\mathbf{\beta}\)</span> is ordinary least squares (OLS). The OLS method seeks to find the coefficients that minimize the sum of squares of the errors, <span class="math notranslate nohighlight">\(\epsilon_i\)</span>. Recall that a key assumption in linear regression models is that <span class="math notranslate nohighlight">\(\{\epsilon_i\}\)</span> are uncorrelated random variables. We aim to find the vector of least squares estimators <span class="math notranslate nohighlight">\(\boldsymbol{{beta}}\)</span> that minimizes</p>
<div class="amsmath math notranslate nohighlight" id="equation-c270ed3d-d299-412f-aec8-9ab64ba2466b">
<span class="eqno">(2.7)<a class="headerlink" href="#equation-c270ed3d-d299-412f-aec8-9ab64ba2466b" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \mathcal{L} = \sum_{i=1}^{n} \epsilon_i^2 = \boldsymbol{\epsilon}^\top\boldsymbol{\epsilon} 
\end{equation}\]</div>
<p>Because <span class="math notranslate nohighlight">\(\mathbf{y}=\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon}\)</span>, we can express <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} = \mathbf{y} - \mathbf{X} \boldsymbol{\beta}\)</span>. So, we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-f52d4dae-f806-4db8-b0d9-a35e88841130">
<span class="eqno">(2.8)<a class="headerlink" href="#equation-f52d4dae-f806-4db8-b0d9-a35e88841130" title="Permalink to this equation">#</a></span>\[\begin{align}
    \mathcal{L} 
    &amp;= (\mathbf{y} - \mathbf{X} \boldsymbol{\beta})^\top (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
    &amp;= (\mathbf{y}^\top - \boldsymbol{\beta}^\top\mathbf{X}^\top) (\mathbf{y} - \mathbf{X} \boldsymbol{\beta}) \\
    &amp;= \mathbf{y}^\top \mathbf{y} - \mathbf{y}^\top \mathbf{X} \boldsymbol{\beta} - \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta} \\
    &amp;= \mathbf{y}^\top \mathbf{y} - 2 \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y} + \boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{X}\boldsymbol{\beta}
\end{align}\]</div>
<p>Note that <span class="math notranslate nohighlight">\(\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta}\)</span> is a scalar, because we have <span class="math notranslate nohighlight">\((1 \times n) \times (n \times p) \times (p \times 1)\)</span>, resulting in a <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrix, which is a scalar. Similarly, <span class="math notranslate nohighlight">\(\boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y}\)</span>, having dimensions <span class="math notranslate nohighlight">\((1 \times p) \times (p \times n) \times (n \times 1)\)</span>, also results in a <span class="math notranslate nohighlight">\(1 \times 1\)</span> matrix, or scalar. Moreover, due to the properties of transposition and the commutative property of scalar multiplication, these two expressions are not only scalars but also represent the same scalar value. Transposing a scalar does not affect its value, thus we have <span class="math notranslate nohighlight">\((\mathbf{y}^\top \mathbf{X} \boldsymbol{\beta})^\top=\boldsymbol{\beta}^\top\mathbf{X}^\top \mathbf{y}\)</span>. Remind that, in matrix multiplication, if we transpose the product of two matrices, we reverse the order of multiplication and transpose each matrix: <span class="math notranslate nohighlight">\((\mathbf{A}\mathbf{B})^\top=\mathbf{B}^\top \mathbf{A}^\top\)</span>.</p>
<p>Now, we need to get the derivative of <span class="math notranslate nohighlight">\(\mathcal{L}\)</span> with respect to the parameter vector <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> and set it to zero. This way we will find the estimated coefficients <span class="math notranslate nohighlight">\(\mathbf{b}\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-9de47cf0-6927-42de-ae6a-11c51ce9e684">
<span class="eqno">(2.9)<a class="headerlink" href="#equation-9de47cf0-6927-42de-ae6a-11c51ce9e684" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{\partial \mathcal{L}}{\partial \boldsymbol{\beta}} \Bigr|_{\widehat{\boldsymbol{\beta}}} = - 2 \mathbf{X}^\top \mathbf{y} + 2 \mathbf{X}^\top \mathbf{X}\widehat{\boldsymbol{\beta}} = \mathbf{0}
\end{equation}\]</div>
<p>which simplifies to</p>
<div class="amsmath math notranslate nohighlight" id="equation-726344a6-d78f-48fd-b5f5-40010cc477bd">
<span class="eqno">(2.10)<a class="headerlink" href="#equation-726344a6-d78f-48fd-b5f5-40010cc477bd" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \mathbf{X}^\top \mathbf{X}\widehat{\boldsymbol{\beta}} = \mathbf{X}^\top \mathbf{y}
\end{equation}\]</div>
<p>Multiplying both sides by the inverse of <span class="math notranslate nohighlight">\(\mathbf{X}^\top \mathbf{X}\)</span> we get the OLS estimate</p>
<div class="amsmath math notranslate nohighlight" id="equation-0b60d953-c397-4d79-8488-d43523461772">
<span class="eqno">(2.11)<a class="headerlink" href="#equation-0b60d953-c397-4d79-8488-d43523461772" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\boldsymbol{\beta}} = (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}
\end{equation}\]</div>
<p>The matrix <span class="math notranslate nohighlight">\(\mathbf{X}^\top \mathbf{X}\)</span> is sometimes referred to as the Gram matrix or the moment matrix, because it contains the ``second moments’’ (i.e., variances and covariances) of the independent variables. The diagonal elements are the sums of squares of each predictor, and the off-diagonal elements represent the sums of cross-products (or covariances) between different predictors. The fitted regression model is then given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-67a6a21e-4cf8-4f87-9ee5-5307cf7c4018">
<span class="eqno">(2.12)<a class="headerlink" href="#equation-67a6a21e-4cf8-4f87-9ee5-5307cf7c4018" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\mathbf{y}} = \mathbf{X} \widehat{\boldsymbol{\beta}} = \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y} = \mathbf{H}\mathbf{y}
\end{equation}\]</div>
<p>where the matrix <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> is referred to as the hat matrix or influence matrix. We can see how the fitted values at the data points used to estimate the model are linear combinations of the observed responses, with weights given by the hat matrix. Geometrically, this means that we find the fitted values by taking the vector of observed responses <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> and projecting it onto a certain plane, which is entirely defined by the values in <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>. If we repeat our experiment (e.g., survey, observation) many times at the same locations <span class="math notranslate nohighlight">\(\mathbf{X}\)</span>, we get different responses <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> every time. But <span class="math notranslate nohighlight">\(\mathbf{H}\)</span> does not change. The properties of the fitted values are thus largely determined by the properties of <span class="math notranslate nohighlight">\(\mathbf{H}\)</span>.</p>
<section id="expected-value-and-variance-of-the-least-square-estimators">
<h4><span class="section-number">2.2.1.1. </span>Expected value and variance of the least square estimators<a class="headerlink" href="#expected-value-and-variance-of-the-least-square-estimators" title="Link to this heading">#</a></h4>
<p>Because we know that <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon} \sim \mathcal{N}(\mathbf{0}, \sigma^2\mathbf{I}_n)\)</span>, we can say that <span class="math notranslate nohighlight">\(\mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2\mathbf{I}_n)\)</span>. Since <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> follows a multivariate normal distribution, any linear combination of <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> is also multivariate normally distributed, including <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>. The expectation of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> is <span class="math notranslate nohighlight">\(\mathbf{X} \boldsymbol{\beta}\)</span> because <span class="math notranslate nohighlight">\(\boldsymbol{\epsilon}\)</span> has expectation zero. Similarly, since <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> is a linear transformation of <span class="math notranslate nohighlight">\(\mathbf{y}\)</span>, it is also normally distributed.</p>
<p>Assuming the model is correct, we can first evaluate the bias of the OLS estimator by looking at the expected value of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span>, which is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-1b1677ad-1679-406a-82bb-005c77f6afef">
<span class="eqno">(2.13)<a class="headerlink" href="#equation-1b1677ad-1679-406a-82bb-005c77f6afef" title="Permalink to this equation">#</a></span>\[\begin{align}
    \mathbb{E}[\widehat{\boldsymbol{\beta}}] &amp;= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}] \\
    &amp;= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top (\mathbf{X} \boldsymbol{\beta}+\boldsymbol{\epsilon})] \\
    &amp;= \mathbb{E}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X} \boldsymbol{\beta}+(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top\boldsymbol{\epsilon})] \\
    &amp;= \mathbb{E}[\boldsymbol{\beta} + (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \boldsymbol{\epsilon}] \\
    &amp;= \boldsymbol{\beta}
\end{align}\]</div>
<p>Thus, <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> if the model is correct. In the derivation of the expected value we used that <span class="math notranslate nohighlight">\(\mathbb{E}[\boldsymbol{\epsilon}]=\mathbf{0}\)</span> and that <span class="math notranslate nohighlight">\((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}=\mathbf{I}\)</span>, and of course the expected value of the constant <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> itself.</p>
<p>The variance property of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> is expressed by the covariance matrix</p>
<div class="amsmath math notranslate nohighlight" id="equation-f3840001-cef0-4db9-be5c-b674c0dc25cc">
<span class="eqno">(2.14)<a class="headerlink" href="#equation-f3840001-cef0-4db9-be5c-b674c0dc25cc" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Cov}[\widehat{\boldsymbol{\beta}}] = \mathbb{E}[(\widehat{\boldsymbol{\beta}}-\mathbb{E}[\widehat{\boldsymbol{\beta}}])(\widehat{\boldsymbol{\beta}}-\mathbb{E}[\widehat{\boldsymbol{\beta}}])^\top]
\end{equation}\]</div>
<p>which is a <span class="math notranslate nohighlight">\(p \times p\)</span> symmetric matrix whose <span class="math notranslate nohighlight">\(j\)</span>th diagonal element is the variance of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}_j\)</span> and whose (<span class="math notranslate nohighlight">\(ij\)</span>)th off-diagonal element is the covariance between <span class="math notranslate nohighlight">\(\widehat{\beta}_i\)</span> and <span class="math notranslate nohighlight">\(\widehat{\beta}_j\)</span>. The
covariance matrix of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> is found by applying a variance operator to <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-391f6355-44e5-4070-bbe7-bb292a62be98">
<span class="eqno">(2.15)<a class="headerlink" href="#equation-391f6355-44e5-4070-bbe7-bb292a62be98" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Cov}[\widehat{\boldsymbol{\beta}}] = \operatorname{Var}[\widehat{\boldsymbol{\beta}}] = \operatorname{Var}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}]
\end{equation}\]</div>
<p>Now let us remind that, in general, if a vector <span class="math notranslate nohighlight">\(\mathbf{v}\)</span> has covariance matrix <span class="math notranslate nohighlight">\(\mathbf{C}\)</span> we have that</p>
<div class="amsmath math notranslate nohighlight" id="equation-3a7ef63b-5189-470a-bf27-b20a9fdcb2f5">
<span class="eqno">(2.16)<a class="headerlink" href="#equation-3a7ef63b-5189-470a-bf27-b20a9fdcb2f5" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Cov}[\mathbf{A}\mathbf{v}] = \mathbf{A} \mathbf{C} \mathbf{A}^\top
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\mathbf{A}\)</span> is a linear transformation. Thus, knowing that <span class="math notranslate nohighlight">\(\mathbf{y}\)</span> has a covariance equal to <span class="math notranslate nohighlight">\(\sigma^2\mathbf{I}_n\)</span>, we get that</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d5b028d-80c0-4a2a-a583-78fda15ab1a4">
<span class="eqno">(2.17)<a class="headerlink" href="#equation-4d5b028d-80c0-4a2a-a583-78fda15ab1a4" title="Permalink to this equation">#</a></span>\[\begin{align}
    \operatorname{Var}[\widehat{\boldsymbol{\beta}}] &amp;= \operatorname{Var}[(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{y}]\\ &amp;= (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top(\sigma^2\mathbf{I}_n)((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top)^\top \\
    &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{I}_n((\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top)^\top \\
    &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{X}^\top \mathbf{X}(\mathbf{X}^\top \mathbf{X})^{-1} \\
    &amp;= \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}
\end{align}\]</div>
<p>Thus, we have that our estimator is normally distributed around the true parameter vector as</p>
<div class="amsmath math notranslate nohighlight" id="equation-71b86591-c7b4-489f-b8c5-1b00b1d85354">
<span class="eqno">(2.18)<a class="headerlink" href="#equation-71b86591-c7b4-489f-b8c5-1b00b1d85354" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\boldsymbol{\beta}}|\mathbf{X} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})
\end{equation}\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>

<span class="c1"># Fit linear regression model</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="n">y_pred</span> <span class="o">=</span> <span class="n">lin_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">scatter</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Data&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y_pred</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;red&#39;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Linear Fit&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Simple Linear Regression&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;X&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/7b9907b1a2f0b860cd46104abb1c5fb972e6eea5680800216949c82a1f6fafdc.png" src="../_images/7b9907b1a2f0b860cd46104abb1c5fb972e6eea5680800216949c82a1f6fafdc.png" />
</div>
</div>
</section>
<section id="estimation-of-sigma-2">
<h4><span class="section-number">2.2.1.2. </span>Estimation of <span class="math notranslate nohighlight">\(\sigma^2\)</span><a class="headerlink" href="#estimation-of-sigma-2" title="Link to this heading">#</a></h4>
<p>We can develop an estimate of <span class="math notranslate nohighlight">\(\sigma^2\)</span> from the residual sum of squares, <span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span>), given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-f25a856f-d908-438f-abb4-c90ef1c06b0e">
<span class="eqno">(2.19)<a class="headerlink" href="#equation-f25a856f-d908-438f-abb4-c90ef1c06b0e" title="Permalink to this equation">#</a></span>\[\begin{align}
    \operatorname{SS}_E &amp;= \sum_{i=1}^{n}(y_i - \widehat{y}_i)^2 = \\
    &amp;= \sum_{i=1}^{n}e_i^2 = \mathbf{e}^\top\mathbf{e} \\
    &amp; = (\mathbf{y}-\widehat{\mathbf{y}})^\top(\mathbf{y}-\widehat{\mathbf{y}}) \\
    &amp; = (\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}})^\top(\mathbf{y} - \mathbf{X} \widehat{\boldsymbol{\beta}}) \\  
    &amp;= \mathbf{y}^\top\mathbf{y} - \widehat{\boldsymbol{\beta}}^\top\mathbf{X}^\top\mathbf{y}
\end{align}\]</div>
<p>then, the residual mean squares, <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-cb1feee0-28da-40f7-842c-c7974997ffd2">
<span class="eqno">(2.20)<a class="headerlink" href="#equation-cb1feee0-28da-40f7-842c-c7974997ffd2" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\sigma}^2 = \operatorname{MS}_E = \frac{\operatorname{SS}_E}{n-p}
\end{equation}\]</div>
<p>which has an expected value of <span class="math notranslate nohighlight">\(\sigma^2(n-p)\)</span>. So, <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span> is an unbiased estimator for <span class="math notranslate nohighlight">\(\sigma^2\)</span>. We can also see how the variance of the estimator goes to 0 as <span class="math notranslate nohighlight">\(n \rightarrow \infty\)</span>. Since the residuals are normally distributed, the estimated error <span class="math notranslate nohighlight">\(\widehat{\sigma}^2 \)</span> has a <span class="math notranslate nohighlight">\(\chi^2_{n-p}\)</span> distribution, resulting from the sum of squared normal variables.</p>
</section>
</section>
</section>
<section id="hypothesis-testing-in-multiple-linear-regression">
<h2><span class="section-number">2.3. </span>Hypothesis testing in multiple linear regression<a class="headerlink" href="#hypothesis-testing-in-multiple-linear-regression" title="Link to this heading">#</a></h2>
<section id="tests-for-significance-of-regression-anova">
<h3><span class="section-number">2.3.1. </span>Tests for significance of regression (ANOVA)<a class="headerlink" href="#tests-for-significance-of-regression-anova" title="Link to this heading">#</a></h3>
<p>The test for significance of regression is a test to determine if there is a linear relationship between the response variable <span class="math notranslate nohighlight">\(y\)</span> and a subset of the regressor variables <span class="math notranslate nohighlight">\(x_1, x_2,\ldots, x_p\)</span>. The appropriate hypotheses are</p>
<div class="amsmath math notranslate nohighlight" id="equation-fe3b90db-3d44-410d-b8b0-f40d9de42e2f">
<span class="eqno">(2.21)<a class="headerlink" href="#equation-fe3b90db-3d44-410d-b8b0-f40d9de42e2f" title="Permalink to this equation">#</a></span>\[\begin{align}
    H_0 &amp;: \beta_1 = \beta_2 = \ldots = \beta_p = 0 \\
    H_1 &amp;: \beta_j \neq 0 \quad \text{for at least one } j
\end{align}\]</div>
<p>This test procedure is called an analysis of variance (ANOVA) because it is based on a decomposition of the total variability in the response variable <span class="math notranslate nohighlight">\(y\)</span>. The total variability in the response variable is measured by the total sum of squares (<span class="math notranslate nohighlight">\(\operatorname{SS}_T\)</span>), calculated as</p>
<div class="amsmath math notranslate nohighlight" id="equation-a55cbbbb-e42e-48eb-90bc-358afc383d2e">
<span class="eqno">(2.22)<a class="headerlink" href="#equation-a55cbbbb-e42e-48eb-90bc-358afc383d2e" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{SS}_T = \sum_{i=1}^{n} (y_i - \bar{y})^2
\end{equation}\]</div>
<p>which is used as a measure of overall variability in the data. Intuitively, this is reasonable because if we were to divide <span class="math notranslate nohighlight">\(\operatorname{SS}_T\)</span> by the appropriate number of degrees of freedom, we would have the sample variance of the <span class="math notranslate nohighlight">\(y\)</span>s. This <span class="math notranslate nohighlight">\(\operatorname{SS}_T\)</span>  is partitioned into</p>
<div class="amsmath math notranslate nohighlight" id="equation-4d02716a-03f6-4c53-a5a7-638da42bc273">
<span class="eqno">(2.23)<a class="headerlink" href="#equation-4d02716a-03f6-4c53-a5a7-638da42bc273" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{SS}_T = \operatorname{SS}_M + \operatorname{SS}_E
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span> is the sum of squares due to the regression model(<span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span>), measuring the explained variation in <span class="math notranslate nohighlight">\( y \)</span>, calculated as <span class="math notranslate nohighlight">\( \sum_{i=1}^{n} (\hat{y}_i - \bar{y})^2 \)</span>; and <span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span> is the sum of squares due to error (<span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span>), measuring the unexplained variation in <span class="math notranslate nohighlight">\( y \)</span>, calculated as <span class="math notranslate nohighlight">\( \sum_{i=1}^{n} (y_i - \hat{y}_i)^2 \)</span>. <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span> measures the explained variability in the dependent variable due to the independent variables in the regression model. Under the null hypothesis (which typically states that all regression coefficients except the intercept are zero), <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span> captures variability that is purely due to chance. To reject the null hypotheses and say that a significant part of the variability is explained by the model (thus justifying the existence of at least one coefficient), we would like <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span> to be significantly larger than <span class="math notranslate nohighlight">\(\operatorname{SS}_R\)</span>. More formally, we use the test statistic is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-f54a8ad6-1268-40d4-a3b7-7342a1120450">
<span class="eqno">(2.24)<a class="headerlink" href="#equation-f54a8ad6-1268-40d4-a3b7-7342a1120450" title="Permalink to this equation">#</a></span>\[\begin{equation}
    F_0 = \frac{\operatorname{SS}_M/p}{\operatorname{SS}_E/(n-p-1)} = \frac{\operatorname{MS}_M}{\operatorname{MS}_E}
\end{equation}\]</div>
<p>Since the residuals are assumed to be normally distributed, <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span> and <span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span> follow a chi-squared distribution, and their ratio follows an F-distribution. This is why the critical value for the test is given <span class="math notranslate nohighlight">\(F_{\alpha, p, n-p-1}\)</span>, where <span class="math notranslate nohighlight">\(\alpha\)</span> is the confidence level. We reject the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> if <span class="math notranslate nohighlight">\(F_0 &gt; F_{\alpha, p, n-p-1}\)</span>. \textbf{ANOVA one factor at the time:} If we have five factors (A, B, C, D, E) and we want to find which factors are significant for a given significance level <span class="math notranslate nohighlight">\(\alpha\)</span>, we compare <span class="math notranslate nohighlight">\(MS_i/MS_E\)</span> for <span class="math notranslate nohighlight">\(i=A, \ldots, E\)</span> where the <span class="math notranslate nohighlight">\(MS_E\)</span> is usually the pure error that comes from replications (or the whole error?).</p>
<p><strong>\texttt{f_regression} (sk-learn function)</strong>
It is quite similar to doing ANOVA and only considering one factor in the model at each time. So we are just saying that the model consists only of one predictor, and everything else goes in the residual. In the \texttt{python} function, this is done in 2 steps. It first computes the correlation between each regressor and the target is computed as in <span class="math notranslate nohighlight">\(((X[:, i] - mean(X[:, i])) * (y - mean_y)) / (std(X[:, i]) * std(y))\)</span>. Then, it converts it into an F score (and then to a p-value). The key idea is that the squared correlation coefficient corresponds to the coefficient of determination <span class="math notranslate nohighlight">\(R^2\)</span>, which is the explained variance over total variance. From there, we can work out the F.</p>
<p>In practice, the first step computes the Pearson correlation coefficient (<span class="math notranslate nohighlight">\(r\)</span>) between each predictor (<span class="math notranslate nohighlight">\(x\)</span>) and the response (<span class="math notranslate nohighlight">\(y\)</span>). For a single predictor <span class="math notranslate nohighlight">\(X_i\)</span> and response <span class="math notranslate nohighlight">\(y\)</span>, the correlation coefficient is calculated as</p>
<div class="math notranslate nohighlight">
\[
r = \frac{\sum_{i=1}^{n} (x_i - \bar{x})(y_i - \bar{y})}{\sqrt{\sum_{i=1}^{n} (x_i - \bar{x})^2 \sum_{i=1}^{n} (y_i - \bar{y})^2}}
\]</div>
<p>where <span class="math notranslate nohighlight">\(\bar{x}\)</span> and <span class="math notranslate nohighlight">\(\bar{y}\)</span> are the means of <span class="math notranslate nohighlight">\(x\)</span> and <span class="math notranslate nohighlight">\(y\)</span>, respectively. Next, the correlation coefficient is used to compute the F-statistic, which assesses the overall significance of the model. The F-statistic for a model with one predictor is calculated as:</p>
<div class="math notranslate nohighlight">
\[
F = \frac{r^2 / 1}{(1 - r^2) / (n - 2)}
\]</div>
<p>This formula derives from the ratio of explained variance (proportional to <span class="math notranslate nohighlight">\(r^2\)</span>) to unexplained variance, adjusted for their respective degrees of freedom. The denominator, <span class="math notranslate nohighlight">\((1 - r^2) / (n - 2)\)</span>, represents the unexplained variance per degree of freedom in the residuals, with <span class="math notranslate nohighlight">\(n - 2\)</span> being the degrees of freedom for residuals in simple regression, the degrees of freedom can also be <span class="math notranslate nohighlight">\(n - 2\)</span> based on whether the intercept is included or not in the model (?). The F-test in regression is designed to test whether the model provides a better fit to the data than a model with no predictors. By converting the correlation coefficient to an F-statistic, \texttt{f_regression} tests the null hypothesis that a given predictor has no linear relationship with the response variable. Rejecting this null hypothesis (i.e., obtaining a small p-value) indicates the predictor is statistically significant.</p>
</section>
<section id="extra-sum-of-squares-method-tests-on-groups-of-coefficients">
<h3><span class="section-number">2.3.2. </span>Extra sum of squares method (tests on groups of coefficients)<a class="headerlink" href="#extra-sum-of-squares-method-tests-on-groups-of-coefficients" title="Link to this heading">#</a></h3>
<p>We may also directly examine the contribution to the regression sum of squares for a particular variable, say <span class="math notranslate nohighlight">\(x_j\)</span>, given that other variables <span class="math notranslate nohighlight">\(x_i\)</span> (<span class="math notranslate nohighlight">\(i \neq j\)</span>) are included in the model. The procedure used to do this is called the extra sum of squares method. This procedure can also be used to investigate the contribution of a subset of the regressor variables to the model. Suppose interest lies in testing the significance of a subset of <span class="math notranslate nohighlight">\(r\)</span> regressor variables (<span class="math notranslate nohighlight">\(r &lt; p\)</span>). The vector of regression coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> can be partitioned as:
$$
\boldsymbol{\beta} =</p>
<div class="amsmath math notranslate nohighlight" id="equation-ef5eeb5b-ef12-4a86-83d4-89760130f5c6">
<span class="eqno">(2.25)<a class="headerlink" href="#equation-ef5eeb5b-ef12-4a86-83d4-89760130f5c6" title="Permalink to this equation">#</a></span>\[\begin{bmatrix}
\boldsymbol{\beta}_1 \\
\boldsymbol{\beta}_2
\end{bmatrix}\]</div>
<p>$<span class="math notranslate nohighlight">\(
with \)</span>\boldsymbol{\beta}_1<span class="math notranslate nohighlight">\( corresponding to the coefficients of the \)</span>r<span class="math notranslate nohighlight">\( variables under consideration, and \)</span>\boldsymbol{\beta}_2<span class="math notranslate nohighlight">\( to the coefficients of the remaining \)</span>p-r<span class="math notranslate nohighlight">\( variables. The hypotheses for assessing the significance of these \)</span>r$ variables are structured as follows:</p>
<ul class="simple">
<li><p><span class="math notranslate nohighlight">\(H_0: \boldsymbol{\beta}_1 = \mathbf{0}\)</span> suggesting the <span class="math notranslate nohighlight">\(r\)</span> variables do not significantly contribute to the model.</p></li>
<li><p><span class="math notranslate nohighlight">\(H_1: \boldsymbol{\beta}_1 \neq \mathbf{0}\)</span> indicating at least one of the <span class="math notranslate nohighlight">\(r\)</span> variables significantly contributes.</p></li>
</ul>
<p>To evaluate these hypotheses, two models are considered. The full model, inclusive of all <span class="math notranslate nohighlight">\(p\)</span> regressor variables, and the reduced model, excluding the <span class="math notranslate nohighlight">\(r\)</span> variables of interest, thus only comprising <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_2\)</span>. Denote the sum of squares for the full model by <span class="math notranslate nohighlight">\(\operatorname{SS}_M(\boldsymbol{\beta})\)</span>, and for the reduced model by <span class="math notranslate nohighlight">\(\operatorname{SS}_M(\boldsymbol{\beta}_2)\)</span>. Then, the increase in the regression sum of squares due to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_1\)</span> given that <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_2\)</span> is already in the model is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-69b16189-3401-4683-b6f1-21d71f981d9f">
<span class="eqno">(2.26)<a class="headerlink" href="#equation-69b16189-3401-4683-b6f1-21d71f981d9f" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{SS}_M(\boldsymbol{\beta}_1|\boldsymbol{\beta}_2) = \operatorname{SS}_M(\boldsymbol{\beta}) - \operatorname{SS}_M(\boldsymbol{\beta}_2)
\end{equation}\]</div>
<p>which is also referred to as extra sum of squares due to <span class="math notranslate nohighlight">\(\boldsymbol{\beta}_1\)</span>, and has <span class="math notranslate nohighlight">\(r\)</span> degrees of freedom. It represents the increase in the regression model sum of squares due to including the new variables (hence the reduction in the sum of squares of the error). Since <span class="math notranslate nohighlight">\(\operatorname{SS}_M(\boldsymbol{\beta}_1|\boldsymbol{\beta}_2)\)</span> is independent of <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span>, the hypothesis <span class="math notranslate nohighlight">\(H_0: \boldsymbol{\beta}_1 = \mathbf{0}\)</span> can be tested using the statistic</p>
<div class="amsmath math notranslate nohighlight" id="equation-dd844264-e9fd-4cd5-9305-3126f9976c99">
<span class="eqno">(2.27)<a class="headerlink" href="#equation-dd844264-e9fd-4cd5-9305-3126f9976c99" title="Permalink to this equation">#</a></span>\[\begin{equation}
    F_0 = \frac{\operatorname{SS}_M(\boldsymbol{\beta}_1|\boldsymbol{\beta}_2)/r}{\operatorname{MS}_E}
\end{equation}\]</div>
<p>Should <span class="math notranslate nohighlight">\(F_0\)</span> surpass the critical value from the F-distribution with <span class="math notranslate nohighlight">\(r\)</span> and <span class="math notranslate nohighlight">\(n-p\)</span> degrees of freedom (<span class="math notranslate nohighlight">\(F_{\alpha,r,n-p}\)</span>), the null hypothesis <span class="math notranslate nohighlight">\(H_0\)</span> is rejected, affirming that at least one of the <span class="math notranslate nohighlight">\(r\)</span> variables significantly impacts the model. This methodology is particularly valuable for its capacity to assess the collective influence of a variable set, rather than evaluating the effect of each variable in isolation. It finds extensive application in model refinement and in the response surface methodology, aiding in the simplification of complex models through the identification and elimination of non-significant variables.</p>
<p>Backward and forward feature selection are iterative methods for model selection where the choice of predictive variables is carried out automatically. These methods are particularly useful in scenarios with many predictors, and they aim to identify the most significant subset that provides the best predictive performance for the model.</p>
<p>\paragraph{Forward selection} It starts with an empty model and adds predictors to the model one at a time. In each step, the variable that provides the most significant improvement to the model’s fit is included, until no additional variables improve the model significantly. This can be formally represented as adding the variable that maximizes the decrease in the sum of squares due to error (<span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span>) or equivalently, maximizes the increase in the regression sum of squares (<span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span>) given the current set of variables. At each step, the significance of adding a new variable is assessed using the extra sum of squares method, where the null hypothesis tests if the coefficient of the variable to be added equals zero.</p>
<p>\paragraph{Backward elimination} It begins with a full model that includes all potential predictors. Variables are removed one at a time, where at each step, the least significant variable, whose removal causes the least decrease in the model fit, is eliminated. The removal of variables is based on the increase in <span class="math notranslate nohighlight">\(\operatorname{SS}_E\)</span> or the decrease in <span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span>, and the significance of each variable is assessed through the extra sum of squares method. The process continues until all remaining variables in the model are statistically significant.</p>
</section>
<section id="tests-on-individual-regression-coefficients">
<h3><span class="section-number">2.3.3. </span>Tests on individual regression coefficients<a class="headerlink" href="#tests-on-individual-regression-coefficients" title="Link to this heading">#</a></h3>
<p>Adding a variable to the regression model always causes the sum of squares for the regression model (<span class="math notranslate nohighlight">\(\operatorname{SS}_M\)</span>) to increase and the error sum of squares to decrease. The hypotheses for testing the significance of any individual regression coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span> are</p>
<div class="amsmath math notranslate nohighlight" id="equation-ab3cf1d0-ddad-4100-9cb6-e9e076024321">
<span class="eqno">(2.28)<a class="headerlink" href="#equation-ab3cf1d0-ddad-4100-9cb6-e9e076024321" title="Permalink to this equation">#</a></span>\[\begin{align}
    H_0 &amp;: \beta_j = 0 \\
    H_1 &amp;: \beta_j \neq 0
\end{align}\]</div>
<p>If <span class="math notranslate nohighlight">\(H_0\)</span> is not rejected, then this indicates that <span class="math notranslate nohighlight">\(x_j\)</span> can be deleted from the model. However, note that this is truly a partial or marginal test, because the regression coefficients depend on all the other regressor variables <span class="math notranslate nohighlight">\(x_i\)</span>, with <span class="math notranslate nohighlight">\(i \neq j\)</span>, that are in the model. Because we know that</p>
<div class="amsmath math notranslate nohighlight" id="equation-e286e5f9-431f-4e71-9541-c5ce9511bea6">
<span class="eqno">(2.29)<a class="headerlink" href="#equation-e286e5f9-431f-4e71-9541-c5ce9511bea6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})
\end{equation}\]</div>
<p>For the <span class="math notranslate nohighlight">\(i\)</span>th coefficient, we do have that</p>
<div class="amsmath math notranslate nohighlight" id="equation-b57255ce-3a09-4821-98fc-b0ae1442a16b">
<span class="eqno">(2.30)<a class="headerlink" href="#equation-b57255ce-3a09-4821-98fc-b0ae1442a16b" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\beta}_i \sim \mathcal{N}(\beta_i, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii})
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}\)</span> is the diagonal element of the covariance matrix <span class="math notranslate nohighlight">\(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\)</span> corresponding to <span class="math notranslate nohighlight">\(b_j\)</span>. The test statistic is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-0f41a2ca-e58a-4aa5-8a49-b619ecf05766">
<span class="eqno">(2.31)<a class="headerlink" href="#equation-0f41a2ca-e58a-4aa5-8a49-b619ecf05766" title="Permalink to this equation">#</a></span>\[\begin{equation}
    t_0 = \frac{\widehat{\beta}_i}{\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}}}
\end{equation}\]</div>
<p>The denominator <span class="math notranslate nohighlight">\(\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{ii}}\)</span> is also called standard error of the regression coefficients <span class="math notranslate nohighlight">\(\widehat{\beta}_i\)</span>, so we can also write it as</p>
<div class="amsmath math notranslate nohighlight" id="equation-c16af0ca-5d93-45a4-ae31-a84a120eb1a6">
<span class="eqno">(2.32)<a class="headerlink" href="#equation-c16af0ca-5d93-45a4-ae31-a84a120eb1a6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    t_0 = \frac{\widehat{\beta}_i}{\text{se}(\widehat{\beta}_i)}
\end{equation}\]</div>
<p>The distribution is a Student’s t distribution because the coefficients itself is normally distributed, while the estimated variance has a Chi-square distribution. So the ratio between a normal and the squared root of a Chi-square follows a t distribution.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.datasets</span> <span class="kn">import</span> <span class="n">make_regression</span>
<span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.feature_selection</span> <span class="kn">import</span> <span class="n">f_regression</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Fit the model</span>
<span class="n">lin_reg</span> <span class="o">=</span> <span class="n">LinearRegression</span><span class="p">()</span>
<span class="n">lin_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="c1"># Perform ANOVA</span>
<span class="n">F</span><span class="p">,</span> <span class="n">p_values</span> <span class="o">=</span> <span class="n">f_regression</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;F-values: </span><span class="si">{</span><span class="n">F</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;P-values: </span><span class="si">{</span><span class="n">p_values</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="n">F</span><span class="p">,</span> <span class="n">tick_label</span><span class="o">=</span><span class="p">[</span><span class="sa">f</span><span class="s1">&#39;Feature </span><span class="si">{</span><span class="n">i</span><span class="si">}</span><span class="s1">&#39;</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="n">F</span><span class="p">)</span><span class="o">+</span><span class="mi">1</span><span class="p">)])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;ANOVA F-values for Regression Features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Features&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;F-value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>F-values: [45.07044471 33.65775767 76.91185757]
P-values: [1.24506616e-09 8.06771918e-08 5.62594307e-14]
</pre></div>
</div>
<img alt="../_images/f5a65df7a8848a2bf68db4adced8f2aa89226e47d0e02c9d6841c0135d28d1c7.png" src="../_images/f5a65df7a8848a2bf68db4adced8f2aa89226e47d0e02c9d6841c0135d28d1c7.png" />
</div>
</div>
</section>
<section id="confidence-intervals">
<h3><span class="section-number">2.3.4. </span>Confidence intervals<a class="headerlink" href="#confidence-intervals" title="Link to this heading">#</a></h3>
<p>A confidence interval, from the frequentist standpoint, is an interval estimate of a parameter <span class="math notranslate nohighlight">\(\beta_j\)</span> that, if the same data collection and analysis procedure were repeated many times, would contain the true parameter value a certain percentage (e.g., 95%) of the time <span class="math notranslate nohighlight">\(\beta_j\)</span>. This is because in frequentist statistics, parameters are considered fixed but unknown quantities. The data are random, which means the calculated confidence interval varies from sample to sample. Saying a 95% confidence interval for a parameter is <span class="math notranslate nohighlight">\([a, b]\)</span> means that if we were to repeat the study many times, 95% of such calculated intervals would contain the true parameter value. It does not mean there is a 95% probability that the true parameter lies within that specific interval for the observed data. Conversely, Bayesian methods treat parameters as random variables and provide a probability distribution (the posterior distribution) that quantifies uncertainty about parameter values given the observed data. The uncertainty about a parameter is directly quantified by its posterior distribution. A 95% credible interval from Bayesian analysis means there is a 95% probability that the parameter lies within this interval, given the data and the prior information. Frequentist confidence intervals do not allow for probabilistic statements about the parameter being within the interval for a given dataset. In contrast, Bayesian credible intervals provide a probability that the parameter lies within the interval, given the data and prior.</p>
</section>
<section id="confidence-intervals-on-individual-regression-coefficients">
<h3><span class="section-number">2.3.5. </span>Confidence intervals on individual regression coefficients<a class="headerlink" href="#confidence-intervals-on-individual-regression-coefficients" title="Link to this heading">#</a></h3>
<p>So far, we have demonstrated that <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}} \sim \mathcal{N}(\boldsymbol{\beta}, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})\)</span>. This also implied that the marginal distribution for any regression coefficient is <span class="math notranslate nohighlight">\(\widehat{\beta}_j \sim \mathcal{N}(\beta_j, \sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj})\)</span>. Then, we can say that</p>
<div class="amsmath math notranslate nohighlight" id="equation-99dee1d5-bff1-42eb-9a25-d65ceb0a0343">
<span class="eqno">(2.33)<a class="headerlink" href="#equation-99dee1d5-bff1-42eb-9a25-d65ceb0a0343" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{\widehat{\beta}_j - \beta_j}{\sqrt{\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}} \sim \mathcal{N}(0, 1) 
\end{equation}\]</div>
<p>Since we do not know <span class="math notranslate nohighlight">\(\sigma^2\)</span>, we used its estimate <span class="math notranslate nohighlight">\(\widehat{\sigma}^2\)</span> obtained through the <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span>, which has a <span class="math notranslate nohighlight">\(\chi^2\)</span> distribution with <span class="math notranslate nohighlight">\(n-p\)</span> degrees of freedom. Thus, we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-dcae8a12-f2e2-4f19-a66f-a78fcd1a25c7">
<span class="eqno">(2.34)<a class="headerlink" href="#equation-dcae8a12-f2e2-4f19-a66f-a78fcd1a25c7" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{\widehat{\beta}_j - \beta_j}{\sqrt{\widehat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}} \sim t_{n-p}
\end{equation}\]</div>
<p>We can then define a 100<span class="math notranslate nohighlight">\((1-\alpha)\%\)</span> confidence interval for the regression coefficient <span class="math notranslate nohighlight">\(\beta_j\)</span>, <span class="math notranslate nohighlight">\(j = 0,1,\ldots, p\)</span>, as</p>
<div class="amsmath math notranslate nohighlight" id="equation-98af7242-46c1-451c-bf9c-ee2dd371a790">
<span class="eqno">(2.35)<a class="headerlink" href="#equation-98af7242-46c1-451c-bf9c-ee2dd371a790" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\beta}_j - t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}} \leq \beta_j \leq \widehat{\beta}_j + t_{\alpha/2, n-p} \sqrt{\hat{\sigma}^2 (\mathbf{X}^\top \mathbf{X})^{-1}_{jj}}
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(t_{\alpha/2, n-p}\)</span> is the critical value from the t-distribution for <span class="math notranslate nohighlight">\(\alpha/2\)</span> and <span class="math notranslate nohighlight">\(n-p\)</span> degrees of freedom. This can also be written in terms of the standard error of the estimated coefficient <span class="math notranslate nohighlight">\(b_j\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-cdb06b87-7f2f-419c-8bd5-1b5651e192c6">
<span class="eqno">(2.36)<a class="headerlink" href="#equation-cdb06b87-7f2f-419c-8bd5-1b5651e192c6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{\beta}_j - t_{\alpha/2, n-p} \text{se}(\widehat{\beta}_j) \leq \beta_j \leq \widehat{\beta}_j + t_{\alpha/2, n-p} \text{se}(\widehat{\beta}_j)
\end{equation}\]</div>
<p>#### Confidence interval on the mean response and prediction variance
We can define a confidence interval on the mean response at a particular point <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span>. The fitted value at this particular point is</p>
<div class="amsmath math notranslate nohighlight" id="equation-3e2f0f6a-1717-4791-8a54-79e9456f9adf">
<span class="eqno">(2.37)<a class="headerlink" href="#equation-3e2f0f6a-1717-4791-8a54-79e9456f9adf" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{y}_0 = \mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}
\end{equation}\]</div>
<p>Because we have <span class="math notranslate nohighlight">\(\mathbf{y} \sim \mathcal{N}(\mathbf{X} \boldsymbol{\beta}, \sigma^2\mathbf{I}_n)\)</span>, suggesting how <span class="math notranslate nohighlight">\(\mathbb{E}[y|\mathbf{X}] = \mathbf{X} \boldsymbol{\beta}\)</span>, we also have that <span class="math notranslate nohighlight">\(\mathbb{E}[y|\mathbf{x}_0] = \mathbf{x}_0^\top \boldsymbol{\beta}\)</span>. Then, we have</p>
<div class="amsmath math notranslate nohighlight" id="equation-cb436638-84b3-4552-ada7-ffe1ff4288a2">
<span class="eqno">(2.38)<a class="headerlink" href="#equation-cb436638-84b3-4552-ada7-ffe1ff4288a2" title="Permalink to this equation">#</a></span>\[\begin{equation}
     \mathbb{E}[\widehat{y}_0] = \mathbb{E}[\mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}] = \mathbf{x}_0^\top \boldsymbol{\beta}
\end{equation}\]</div>
<p>because <span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\boldsymbol{\beta}}] = \boldsymbol{\beta}\)</span>. Thus, <span class="math notranslate nohighlight">\(\widehat{y}_0\)</span> is an unbiased estimator of <span class="math notranslate nohighlight">\(\mathbb{E}[y|\mathbf{x}_0]\)</span>. Since <span class="math notranslate nohighlight">\(\widehat{y}_0\)</span> is a linear combination of <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span>, which has a variance of <span class="math notranslate nohighlight">\(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1}\)</span>, the variance of <span class="math notranslate nohighlight">\(\widehat{y}_0\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-95a1132c-5545-4f9d-9179-02e606e51d30">
<span class="eqno">(2.39)<a class="headerlink" href="#equation-95a1132c-5545-4f9d-9179-02e606e51d30" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Var}[\widehat{y}_0] = \operatorname{Var}\left[\mathbf{x}_0^\top \widehat{\boldsymbol{\beta}}\right] = \mathbf{x}_0^\top(\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})\mathbf{x}_0 = \sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}\]</div>
<p>Then, we can define the 100(1-<span class="math notranslate nohighlight">\(\alpha\)</span>)% confidence interval on the mean response at point <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-473676d6-9cdc-4e53-866e-03d1a65697db">
<span class="eqno">(2.40)<a class="headerlink" href="#equation-473676d6-9cdc-4e53-866e-03d1a65697db" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \widehat{y}_0 - t_{\alpha/2, n-p} \sqrt{\sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0} \leq \mathbb{E}[y|\mathbf{x}_0] \leq \widehat{y}_0 + t_{\alpha/2, n-p} \sqrt{\sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0}
\end{equation}\]</div>
<p>In general, we can define the <strong>prediction variance (PV)</strong> of the fitted value at location <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d0b008d-8609-4c93-93a3-d98f182709b4">
<span class="eqno">(2.41)<a class="headerlink" href="#equation-8d0b008d-8609-4c93-93a3-d98f182709b4" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{PV} = \operatorname{Var}[\widehat{y}_0] = \sigma^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}\]</div>
<p>Then, the <strong>unscaled prediction variance (UPV)</strong> is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-1b868a61-ad24-4add-b9e7-5ef16ef1ceb6">
<span class="eqno">(2.42)<a class="headerlink" href="#equation-1b868a61-ad24-4add-b9e7-5ef16ef1ceb6" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{UPV} = \frac{\operatorname{Var}[\widehat{y}_0]}{\sigma^2} = \mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}\]</div>
<p>The main benefit of the UPV is that it can be used during the design selection phase. The division
by <span class="math notranslate nohighlight">\(\sigma^2\)</span> means that \underline{UPV is a function of only the design matrix} and does not require data to have been collected. UPV provides a way to quantify the precision of the predictions made by a regression model at specific points. It tells us how much the predicted value at a certain point <span class="math notranslate nohighlight">\(\mathbf{x}_0\)</span> is expected to vary, due to the variability in the estimated regression coefficients, but without considering the inherent variability of the response itself. The key intuition is that since <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> carries variance, any prediction made using these estimates also inherits variance (i.e., uncertainty propagation?). UPV connects parameter estimation to prediction variance by translating the uncertainty in the model’s coefficients into the uncertainty of the model’s predictions. If we have already collected data, we can try to estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> using the <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span>. Then, we can define the <strong>estimated prediction variance (EPV)</strong> as</p>
<div class="amsmath math notranslate nohighlight" id="equation-37a0cf91-efe3-410d-82c5-0bd4d355db9b">
<span class="eqno">(2.43)<a class="headerlink" href="#equation-37a0cf91-efe3-410d-82c5-0bd4d355db9b" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{EPV} = \widehat{\sigma}^2\mathbf{x}_0^\top(\mathbf{X}^\top \mathbf{X})^{-1}\mathbf{x}_0
\end{equation}\]</div>
<section id="joint-confidence-region-on-the-regression-coefficients-confidence-ellipsoid">
<h4><span class="section-number">2.3.5.1. </span>Joint confidence region on the regression coefficients (confidence ellipsoid)<a class="headerlink" href="#joint-confidence-region-on-the-regression-coefficients-confidence-ellipsoid" title="Link to this heading">#</a></h4>
<p>The confidence intervals in the previous subsection should be thought of as one-at-a-time intervals. We can also define a <strong>confidence ellipsoid</strong> representing the region in which the true coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> are likely to be found, given the estimated coefficients <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}\)</span> and a certain level of confidence <span class="math notranslate nohighlight">\(\alpha\)</span>. The ellipsoid provides a geometrical assessment of the uncertainty associated with the estimated coefficients. It shows how these estimates might vary due to the variability in the data used to calculate them. The volume and shape of the ellipsoid depend on the precision of the estimates: a smaller ellipsoid indicates more precise estimates.</p>
<section id="mahalanobis-distance">
<h5><span class="section-number">2.3.5.1.1. </span>Mahalanobis distance<a class="headerlink" href="#mahalanobis-distance" title="Link to this heading">#</a></h5>
<p>Before introducing the confidence ellipsoid, let us provide a quick intuition and recap regarding Mahalanobis distance, which is particularly useful in statistics for understanding the distribution of multivariate data, creating confidence regions around means or model parameters, and identifying outliers. Let us start from the PDF of a multivariate normal distribution with mean vector <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span></p>
<div class="amsmath math notranslate nohighlight" id="equation-17f45dc2-cf1e-471c-a761-d694cb9a659b">
<span class="eqno">(2.44)<a class="headerlink" href="#equation-17f45dc2-cf1e-471c-a761-d694cb9a659b" title="Permalink to this equation">#</a></span>\[\begin{equation}
    f(x) = \frac{1}{\sqrt{2\pi^p \operatorname{det}(\boldsymbol{\Sigma})}}e^{-\frac{1}{2}(\mathbf{x}- \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}- \boldsymbol{\mu})}
\end{equation}\]</div>
<p>The term inside the exponential function, <span class="math notranslate nohighlight">\((\mathbf{x}- \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}- \boldsymbol{\mu})\)</span>, is the squared Mahalanobis distance between the point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> and the mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span>. This term essentially measures how far <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> is from the mean, adjusted for the spread and correlation structure of the variables as given by the covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>. More generally, given a probability distribution <span class="math notranslate nohighlight">\(Q\)</span> with mean <span class="math notranslate nohighlight">\(\boldsymbol{\mu}\)</span> and positive-definite covariance matrix <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}\)</span>, the Mahalanobis distance of a point <span class="math notranslate nohighlight">\(\mathbf{x}\)</span> from <span class="math notranslate nohighlight">\(Q\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-8d196eb6-b395-4e1b-a36e-7a3a07862449">
<span class="eqno">(2.45)<a class="headerlink" href="#equation-8d196eb6-b395-4e1b-a36e-7a3a07862449" title="Permalink to this equation">#</a></span>\[\begin{equation}
    d_M = \sqrt{(\mathbf{x}- \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}- \boldsymbol{\mu})}
\end{equation}\]</div>
<p>Data points with the same Mahalanobis distance form an ellipsoid in the multidimensional space, centered at the mean, where the covariance matrix determines the size, shape, and orientation of the ellipsoid. Given the ellipsoid <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> defined by</p>
<div class="amsmath math notranslate nohighlight" id="equation-c4077e98-a170-4a0c-bcc3-434f39dc392a">
<span class="eqno">(2.46)<a class="headerlink" href="#equation-c4077e98-a170-4a0c-bcc3-434f39dc392a" title="Permalink to this equation">#</a></span>\[\begin{equation}
    (\mathbf{x}- \boldsymbol{\mu})^\top \boldsymbol{\Sigma}^{-1}(\mathbf{x}- \boldsymbol{\mu}) \leq k
\end{equation}\]</div>
<p>we know that</p>
<div class="amsmath math notranslate nohighlight" id="equation-b8108a12-6f75-412d-bda4-da0ec40fac4a">
<span class="eqno">(2.47)<a class="headerlink" href="#equation-b8108a12-6f75-412d-bda4-da0ec40fac4a" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Vol}(\mathcal{E}) \propto \sqrt{\operatorname{det}(\boldsymbol{\Sigma})}
\end{equation}\]</div>
<p>which means that the volume of the ellipsoid is inversely proportional to the square root of the determinant of <span class="math notranslate nohighlight">\(\boldsymbol{\Sigma}^{-1}\)</span>.</p>
</section>
</section>
<section id="confidence-ellipsoid-we-can-define-an-ellipsoid-for-the-regression-coefficients-boldsymbol-beta-as">
<h4><span class="section-number">2.3.5.2. </span>Confidence ellipsoid We can define an ellipsoid for the regression coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> as<a class="headerlink" href="#confidence-ellipsoid-we-can-define-an-ellipsoid-for-the-regression-coefficients-boldsymbol-beta-as" title="Link to this heading">#</a></h4>
<div class="amsmath math notranslate nohighlight" id="equation-a9be208a-386a-48cc-a750-9f4c409253e0">
<span class="eqno">(2.48)<a class="headerlink" href="#equation-a9be208a-386a-48cc-a750-9f4c409253e0" title="Permalink to this equation">#</a></span>\[\begin{equation}
    (\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top (\sigma^2 (\mathbf{X}^\top \mathbf{X})^{-1})^{-1}(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}) = \frac{(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top \mathbf{X}^\top \mathbf{X}(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})}{\sigma^2}
\end{equation}\]</div>
<p>Since we estimate <span class="math notranslate nohighlight">\(\sigma^2\)</span> with <span class="math notranslate nohighlight">\(\operatorname{MS}_E\)</span>, we get</p>
<div class="amsmath math notranslate nohighlight" id="equation-33b6eb05-48bb-4049-91ad-817386542c95">
<span class="eqno">(2.49)<a class="headerlink" href="#equation-33b6eb05-48bb-4049-91ad-817386542c95" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top \mathbf{X}^\top \mathbf{X}(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})}{p\widehat{\sigma}^2} \sim F_{p, n-p}
\end{equation}\]</div>
<p>This holds because the quadratic form <span class="math notranslate nohighlight">\((\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top \mathbf{X}^\top \mathbf{X}(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})\)</span> is a sum of squared normal variables (since <span class="math notranslate nohighlight">\(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta}\)</span> is a vector of normally distributed errors) and thus follows a <span class="math notranslate nohighlight">\(\chi^2_p\)</span> distribution (that is why we divide it by <span class="math notranslate nohighlight">\(p\)</span> too). Since <span class="math notranslate nohighlight">\(\widehat{\sigma}^2\)</span> follows a <span class="math notranslate nohighlight">\(\chi^2_{n-p}\)</span> distribution, then the ratio of the two Chi-squares is distributed as an F-distribution. Consequently,  a <span class="math notranslate nohighlight">\(100(1-\alpha)\)</span>% joint confidence region <span class="math notranslate nohighlight">\(\mathcal{E}\)</span> for all the parameters in <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> is given by</p>
<div class="amsmath math notranslate nohighlight" id="equation-f8686a75-179c-4084-a769-0de0176f77bb">
<span class="eqno">(2.50)<a class="headerlink" href="#equation-f8686a75-179c-4084-a769-0de0176f77bb" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \frac{(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})^\top \mathbf{X}^\top \mathbf{X}(\widehat{\boldsymbol{\beta}}-\boldsymbol{\beta})}{p\widehat{\sigma}^2} \leq F_{\alpha, p, n-p}
\end{equation}\]</div>
<p>From the properties of the ellipsoids we know that the volume of this region is inversely proportional to the square root of the determinant of <span class="math notranslate nohighlight">\(\mathbf{X}^\top \mathbf{X}\)</span>.</p>
<div class="amsmath math notranslate nohighlight" id="equation-8e7e080b-bf4b-49e9-bf6a-bb67e960680c">
<span class="eqno">(2.51)<a class="headerlink" href="#equation-8e7e080b-bf4b-49e9-bf6a-bb67e960680c" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \operatorname{Vol}(\mathcal{E}) \propto \sqrt{\operatorname{det}(\mathbf{X}^\top \mathbf{X})^{-1}}
\end{equation}\]</div>
<p>Thus, maximizing <span class="math notranslate nohighlight">\(\operatorname{det}(\mathbf{X}^\top \mathbf{X})\)</span> minimizes the volume of the ellipsoids (<strong>D-optimality</strong>). A smaller confidence ellipsoid implies that the range of values that the regression coefficients could plausibly take (given the data and the model) is more tightly constrained. This means that we can be more confident about where these parameters lie. In practical terms, it leads to narrower confidence intervals for each coefficient, indicating increased precision in our estimates.</p>
</section>
</section>
</section>
<section id="conditional-expectations-causality-observational-data-vs-designed-experiments">
<h2><span class="section-number">2.4. </span>Conditional expectations/causality: observational data vs. designed experiments<a class="headerlink" href="#conditional-expectations-causality-observational-data-vs-designed-experiments" title="Link to this heading">#</a></h2>
<p>When the coefficients <span class="math notranslate nohighlight">\(\widehat{\beta}_0\)</span> and <span class="math notranslate nohighlight">\(\widehat{\beta}_1\)</span> are estimated from a sample of <span class="math notranslate nohighlight">\(n\)</span> observations, they will be the result of the specific values <span class="math notranslate nohighlight">\(\{x_i, y_i\}\)</span> observed in that sample. This means they are subject to the variability and characteristics of that particular sample. For this reason, when we try to derive the expectation or variance of these quantities, they will be conditional on the observed values: <span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\beta}_1|x_1, \ldots, x_n]\)</span>, <span class="math notranslate nohighlight">\(\operatorname{Var}[\widehat{\beta}_1|x_1, \ldots, x_n]\)</span>. In experiments where the researcher controls or sets the values of <span class="math notranslate nohighlight">\(x_i\)</span> (designed or controlled experiments), these <span class="math notranslate nohighlight">\(x_i\)</span> values are treated as non-random and fixed. In such scenarios, the conditioning on <span class="math notranslate nohighlight">\(x_i\)</span> values can be ignored for the purposes of calculating the expectation and variance of the estimated coefficients. This is because, in these cases, the variability in the estimates comes only from the <span class="math notranslate nohighlight">\(y_i\)</span> values, as the <span class="math notranslate nohighlight">\(x_i\)</span> values are constant and known. In observational studies or experiments where <span class="math notranslate nohighlight">\(x_i\)</span> values are not controlled and can vary, the expectation and variance of the coefficients would need to account for the randomness in both <span class="math notranslate nohighlight">\(x_i\)</span> and <span class="math notranslate nohighlight">\(y_i\)</span> values. So, if we have only observational data (where <span class="math notranslate nohighlight">\(x\)</span>’s are random), we might be interested in finding <span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\beta}_1]\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Var}[\widehat{\beta}_1]\)</span>, not just <span class="math notranslate nohighlight">\(\mathbb{E}[\widehat{\beta}_1|x_1, \ldots, x_n]\)</span> and <span class="math notranslate nohighlight">\(\operatorname{Var}[\widehat{\beta}_1|x_1, \ldots, x_n]\)</span>.</p>
<p>We can think of the slope as the average change in <span class="math notranslate nohighlight">\(Y\)</span> for a one-unit change in <span class="math notranslate nohighlight">\(X\)</span>. Mathematically, this can be expressed as</p>
<div class="amsmath math notranslate nohighlight" id="equation-5d14b2a9-e883-4281-ae57-e1bfecf3396e">
<span class="eqno">(2.52)<a class="headerlink" href="#equation-5d14b2a9-e883-4281-ae57-e1bfecf3396e" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \beta_1 = \mathbb{E}[Y|X=x] - \mathbb{E}[Y|X=x-1]
\end{equation}\]</div>
<p>where we applied a change of one unit to <span class="math notranslate nohighlight">\(x\)</span>. For any <span class="math notranslate nohighlight">\(x_1, x_2\)</span> we can write</p>
<div class="amsmath math notranslate nohighlight" id="equation-10ca03ea-bc0f-4fb4-8099-42ae398998b0">
<span class="eqno">(2.53)<a class="headerlink" href="#equation-10ca03ea-bc0f-4fb4-8099-42ae398998b0" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \beta_1 = \frac{\mathbb{E}[Y|X=x_2] - \mathbb{E}[Y|X=x_1]}{x_2-x_1}
\end{equation}\]</div>
<p>where we are simply saying that the slope of a regression line is given by the ratio of rise (difference on the <span class="math notranslate nohighlight">\(y\)</span> axis) and run (difference on the <span class="math notranslate nohighlight">\(x\)</span> axis). While the slope <span class="math notranslate nohighlight">\(\beta_1\)</span> quantifies the association between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>, it should not automatically be interpreted as a causal effect. If we only deal with observation data, The appropriate way to understand <span class="math notranslate nohighlight">\(\beta_1\)</span> is as an expected difference, not as a causal effect. It should be interpreted as, ``If we compare two groups from the un-manipulated distribution of <span class="math notranslate nohighlight">\(X\)</span> where <span class="math notranslate nohighlight">\(X\)</span> differs by one unit, we expect <span class="math notranslate nohighlight">\(Y\)</span> to differ, on average, by <span class="math notranslate nohighlight">\(b_1\)</span> units’’. This interpretation holds regardless of whether there’s a causal relationship between <span class="math notranslate nohighlight">\(X\)</span> and <span class="math notranslate nohighlight">\(Y\)</span>.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">pandas</span> <span class="k">as</span> <span class="nn">pd</span>
<span class="kn">import</span> <span class="nn">seaborn</span> <span class="k">as</span> <span class="nn">sns</span>

<span class="c1"># Generate synthetic observational data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">size</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>
<span class="n">Z</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span> <span class="o">*</span> <span class="mf">0.1</span>
<span class="n">Y</span> <span class="o">=</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">Z</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="n">size</span><span class="p">)</span>

<span class="n">data</span> <span class="o">=</span> <span class="n">pd</span><span class="o">.</span><span class="n">DataFrame</span><span class="p">({</span><span class="s1">&#39;X&#39;</span><span class="p">:</span> <span class="n">X</span><span class="p">,</span> <span class="s1">&#39;Z&#39;</span><span class="p">:</span> <span class="n">Z</span><span class="p">,</span> <span class="s1">&#39;Y&#39;</span><span class="p">:</span> <span class="n">Y</span><span class="p">})</span>

<span class="c1"># Plotting</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">14</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;X&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Observational Data: X vs Y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">sns</span><span class="o">.</span><span class="n">scatterplot</span><span class="p">(</span><span class="n">data</span><span class="o">=</span><span class="n">data</span><span class="p">,</span> <span class="n">x</span><span class="o">=</span><span class="s1">&#39;Z&#39;</span><span class="p">,</span> <span class="n">y</span><span class="o">=</span><span class="s1">&#39;Y&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Observational Data: Z vs Y&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e1fb05a98da5c72d0a9e1fd9c34e6292c7ed9dfe2306f1e4cb1751e2bdb7244a.png" src="../_images/e1fb05a98da5c72d0a9e1fd9c34e6292c7ed9dfe2306f1e4cb1751e2bdb7244a.png" />
</div>
</div>
</section>
<section id="model-selection-criteria-aic-and-bic">
<h2><span class="section-number">2.5. </span>Model selection criteria: AIC and BIC<a class="headerlink" href="#model-selection-criteria-aic-and-bic" title="Link to this heading">#</a></h2>
<p>Model selection is a critical step in statistical modeling, aiming to identify the most appropriate model that explains the data without overfitting. Two widely used criteria for model selection are the Akaike Information Criterion (AIC) and the Bayesian Information Criterion (BIC). Both criteria are grounded in information theory and aim to balance the goodness of fit of the model with its complexity. The AIC is a measure of the relative quality of a statistical model for a given set of data. It assesses the trade-off between the goodness of fit of the model and its complexity (measured by the number of parameters in the model). The AIC is defined as:</p>
<div class="amsmath math notranslate nohighlight" id="equation-a12ce872-8007-43fe-81a0-d0e5be590b5e">
<span class="eqno">(2.54)<a class="headerlink" href="#equation-a12ce872-8007-43fe-81a0-d0e5be590b5e" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \text{AIC} = 2k - 2\ln(L)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(k\)</span> is the number of parameters in the model, and <span class="math notranslate nohighlight">\(L\)</span> is the maximum value of the likelihood function for the model. A lower AIC value indicates a better model. The AIC encourages model parsimony, so it penalizes the addition of unnecessary parameters.</p>
<p>The BIC, is similar to the AIC but introduces a stronger penalty for models with more parameters. The BIC is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b4a101d1-3fc9-4974-aea0-751a5f422993">
<span class="eqno">(2.55)<a class="headerlink" href="#equation-b4a101d1-3fc9-4974-aea0-751a5f422993" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \text{BIC} = \ln(n)k - 2\ln(L)
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(n\)</span> is the number of observations, <span class="math notranslate nohighlight">\(k\)</span> is the number of parameters, and <span class="math notranslate nohighlight">\(L\)</span> is the likelihood of the model. Like the AIC, the model with the lowest BIC is generally preferred. The BIC’s penalty term is larger than that of the AIC when the sample size <span class="math notranslate nohighlight">\(n\)</span> is greater than 7, making it more stringent against complex models.</p>
<p>Both AIC and BIC aim to select models that are parsimonious and well-fitting. The main differences between them lie in their penalty terms; the BIC penalizes model complexity more heavily than the AIC, especially as the sample size increases. This can lead to different model choices, particularly in situations with large datasets or many candidate models. In practice, AIC is often used when the primary goal is predictive accuracy, as it tends to favor more complex models that might capture the data better. BIC is used when the goal is to find the true model among the set of candidates, as it leans towards simpler models due to its higher penalty on the number of parameters. Both criteria are essential tools in model selection, providing a balance between simplicity and the ability to fit the data. However, they should not be used blindly. It’s important to consider the context of the modeling problem, including the goals of the analysis and the nature of the data, when deciding which criterion to use or whether to use them in conjunction with other model selection methods.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">LinearRegression</span>
<span class="kn">from</span> <span class="nn">sklearn.metrics</span> <span class="kn">import</span> <span class="n">mean_squared_error</span>
<span class="kn">from</span> <span class="nn">statsmodels.regression.linear_model</span> <span class="kn">import</span> <span class="n">OLS</span>
<span class="kn">import</span> <span class="nn">statsmodels.api</span> <span class="k">as</span> <span class="nn">sm</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="mi">100</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y</span> <span class="o">=</span> <span class="mi">4</span> <span class="o">+</span> <span class="mi">3</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">0</span><span class="p">]</span> <span class="o">+</span> <span class="mi">2</span> <span class="o">*</span> <span class="n">X</span><span class="p">[:,</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">100</span><span class="p">)</span>

<span class="c1"># Fit multiple models</span>
<span class="n">model1</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model2</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">[:,</span> <span class="p">:</span><span class="mi">2</span><span class="p">]))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>
<span class="n">model3</span> <span class="o">=</span> <span class="n">OLS</span><span class="p">(</span><span class="n">y</span><span class="p">,</span> <span class="n">sm</span><span class="o">.</span><span class="n">add_constant</span><span class="p">(</span><span class="n">X</span><span class="p">))</span><span class="o">.</span><span class="n">fit</span><span class="p">()</span>

<span class="c1"># Print AIC and BIC for each model</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model 1: AIC=</span><span class="si">{</span><span class="n">model1</span><span class="o">.</span><span class="n">aic</span><span class="si">}</span><span class="s2">, BIC=</span><span class="si">{</span><span class="n">model1</span><span class="o">.</span><span class="n">bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model 2: AIC=</span><span class="si">{</span><span class="n">model2</span><span class="o">.</span><span class="n">aic</span><span class="si">}</span><span class="s2">, BIC=</span><span class="si">{</span><span class="n">model2</span><span class="o">.</span><span class="n">bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Model 3: AIC=</span><span class="si">{</span><span class="n">model3</span><span class="o">.</span><span class="n">aic</span><span class="si">}</span><span class="s2">, BIC=</span><span class="si">{</span><span class="n">model3</span><span class="o">.</span><span class="n">bic</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Model 1: AIC=366.89403845207, BIC=372.1043788240462
Model 2: AIC=275.8424794159408, BIC=283.65798997390505
Model 3: AIC=277.27997777109255, BIC=287.7006585150449
</pre></div>
</div>
</div>
</div>
</section>
<section id="regularization-methods">
<h2><span class="section-number">2.6. </span>Regularization methods<a class="headerlink" href="#regularization-methods" title="Link to this heading">#</a></h2>
<p>Regularization techniques are fundamental to the field of machine learning and statistics, primarily used to prevent overfitting, improve model generalization, and handle multicollinearity among predictors. This section delves into three popular regularization methods: Lasso (L1 penalty), Ridge (L2 penalty), and Elastic Net (combination of L1 and L2 penalties).</p>
<section id="lasso-regression-l1-penalty">
<h3><span class="section-number">2.6.1. </span>Lasso Regression (L1 Penalty)<a class="headerlink" href="#lasso-regression-l1-penalty" title="Link to this heading">#</a></h3>
<p>Lasso (Least Absolute Shrinkage and Selection Operator) regression introduces a penalty term equal to the absolute value of the magnitude of coefficients to the loss function. The objective function of Lasso regression is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-71312f34-638b-4852-befa-274537dff5be">
<span class="eqno">(2.56)<a class="headerlink" href="#equation-71312f34-638b-4852-befa-274537dff5be" title="Permalink to this equation">#</a></span>\[\begin{equation}
3\text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} |\beta_j| \right\},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is a regularization parameter that controls the strength of the penalty.</p>
<ul class="simple">
<li><p>Promotes sparsity in the model coefficients, effectively performing feature selection.</p></li>
<li><p>Particularly useful when dealing with high-dimensional data or when feature selection is desired.</p></li>
<li><p>Can result in models that are easier to interpret due to fewer predictors.</p></li>
</ul>
</section>
<section id="ridge-regression-l2-penalty">
<h3><span class="section-number">2.6.2. </span>Ridge Regression (L2 Penalty)<a class="headerlink" href="#ridge-regression-l2-penalty" title="Link to this heading">#</a></h3>
<p>Ridge regression adds a penalty equal to the square of the magnitude of coefficients to the loss function. The formulation of Ridge regression is as follows:</p>
<div class="amsmath math notranslate nohighlight" id="equation-49f55094-8f39-41fa-a2bd-f86c5dfafc1e">
<span class="eqno">(2.57)<a class="headerlink" href="#equation-49f55094-8f39-41fa-a2bd-f86c5dfafc1e" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda \sum_{j=1}^{p} \beta_j^2 \right\},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda\)</span> is the regularization parameter.</p>
<ul class="simple">
<li><p>Reduces model complexity by shrinking coefficients, but does not necessarily reduce them to zero.</p></li>
<li><p>Particularly beneficial in situations with multicollinearity among predictors.</p></li>
<li><p>Helps to stabilize the coefficient estimates and improve model generalization.</p></li>
</ul>
</section>
<section id="elastic-net-combination-of-l1-and-l2-penalties">
<h3><span class="section-number">2.6.3. </span>Elastic Net (Combination of L1 and L2 Penalties)<a class="headerlink" href="#elastic-net-combination-of-l1-and-l2-penalties" title="Link to this heading">#</a></h3>
<p>Elastic Net combines the penalties of Lasso and Ridge, incorporating both the L1 and L2 penalty terms. The objective function for Elastic Net is:</p>
<div class="amsmath math notranslate nohighlight" id="equation-6ba35152-96eb-4bb6-bc85-87521ee2bb99">
<span class="eqno">(2.58)<a class="headerlink" href="#equation-6ba35152-96eb-4bb6-bc85-87521ee2bb99" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \text{minimize} \; \left\{ \frac{1}{2n} \sum_{i=1}^{n} (y_i - x_i^\top \beta)^2 + \lambda_1 \sum_{j=1}^{p} |\beta_j| + \frac{\lambda_2}{2} \sum_{j=1}^{p} \beta_j^2 \right\},
\end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\lambda_1\)</span> and <span class="math notranslate nohighlight">\(\lambda_2\)</span> are parameters that control the strength of the L1 and L2 penalties, respectively.</p>
<ul class="simple">
<li><p>Combines the feature selection properties of Lasso with the ridge regression’s ability to handle multicollinearity.</p></li>
<li><p>Encourages a grouping effect, where correlated predictors either enter or leave the model together.</p></li>
<li><p>Offers a balance between Lasso and Ridge regression by allowing for control over both penalties, making it versatile for various scenarios.</p></li>
<li><p>Useful in high-dimensional data scenarios where Lasso might suffer due to high correlations among predictors.</p></li>
</ul>
<p>Each of these regularization methods has its unique strengths and applications, and the choice among them should be guided by the specific characteristics of the data and the modeling objectives at hand.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">sklearn.linear_model</span> <span class="kn">import</span> <span class="n">Ridge</span><span class="p">,</span> <span class="n">Lasso</span>
<span class="kn">from</span> <span class="nn">sklearn.model_selection</span> <span class="kn">import</span> <span class="n">train_test_split</span>

<span class="c1"># Generate synthetic data</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">X</span><span class="p">,</span> <span class="n">y</span> <span class="o">=</span> <span class="n">make_regression</span><span class="p">(</span><span class="n">n_samples</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_features</span><span class="o">=</span><span class="mi">20</span><span class="p">,</span> <span class="n">noise</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="c1"># Split data into training and test sets</span>
<span class="n">X_train</span><span class="p">,</span> <span class="n">X_test</span><span class="p">,</span> <span class="n">y_train</span><span class="p">,</span> <span class="n">y_test</span> <span class="o">=</span> <span class="n">train_test_split</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">test_size</span><span class="o">=</span><span class="mf">0.2</span><span class="p">,</span> <span class="n">random_state</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Fit Lasso and Ridge regression models</span>
<span class="n">ridge_reg</span> <span class="o">=</span> <span class="n">Ridge</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>
<span class="n">lasso_reg</span> <span class="o">=</span> <span class="n">Lasso</span><span class="p">(</span><span class="n">alpha</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>

<span class="n">ridge_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>
<span class="n">lasso_reg</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X_train</span><span class="p">,</span> <span class="n">y_train</span><span class="p">)</span>

<span class="c1"># Predict and evaluate</span>
<span class="n">ridge_pred</span> <span class="o">=</span> <span class="n">ridge_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>
<span class="n">lasso_pred</span> <span class="o">=</span> <span class="n">lasso_reg</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X_test</span><span class="p">)</span>

<span class="n">ridge_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">ridge_pred</span><span class="p">)</span>
<span class="n">lasso_mse</span> <span class="o">=</span> <span class="n">mean_squared_error</span><span class="p">(</span><span class="n">y_test</span><span class="p">,</span> <span class="n">lasso_pred</span><span class="p">)</span>

<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Ridge MSE: </span><span class="si">{</span><span class="n">ridge_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Lasso MSE: </span><span class="si">{</span><span class="n">lasso_mse</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>

<span class="c1"># Plotting coefficients</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">ridge_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Ridge Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">lasso_reg</span><span class="o">.</span><span class="n">coef_</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Lasso Coefficients&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Ridge vs Lasso Coefficients&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Coefficient Index&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Coefficient Value&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Ridge MSE: 4.766846557940456
Lasso MSE: 0.10241540029791293
</pre></div>
</div>
<img alt="../_images/170720a0e57beefff736a8412b2a7c875ca30ff11d3d62749b5b580d93ea0809.png" src="../_images/170720a0e57beefff736a8412b2a7c875ca30ff11d3d62749b5b580d93ea0809.png" />
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="review_stats.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">1. </span>Probability Theory and Statistics</p>
      </div>
    </a>
    <a class="right-next"
       href="review_ML.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">3. </span>Machine Learning</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#a-preamble">2.1. A preamble</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#now-to-linear-regression">2.2. Now to linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ordinary-least-squares">2.2.1. Ordinary least squares</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#expected-value-and-variance-of-the-least-square-estimators">2.2.1.1. Expected value and variance of the least square estimators</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#estimation-of-sigma-2">2.2.1.2. Estimation of <span class="math notranslate nohighlight">\(\sigma^2\)</span></a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#hypothesis-testing-in-multiple-linear-regression">2.3. Hypothesis testing in multiple linear regression</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-for-significance-of-regression-anova">2.3.1. Tests for significance of regression (ANOVA)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#extra-sum-of-squares-method-tests-on-groups-of-coefficients">2.3.2. Extra sum of squares method (tests on groups of coefficients)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#tests-on-individual-regression-coefficients">2.3.3. Tests on individual regression coefficients</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals">2.3.4. Confidence intervals</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-intervals-on-individual-regression-coefficients">2.3.5. Confidence intervals on individual regression coefficients</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#joint-confidence-region-on-the-regression-coefficients-confidence-ellipsoid">2.3.5.1. Joint confidence region on the regression coefficients (confidence ellipsoid)</a><ul class="nav section-nav flex-column">
<li class="toc-h5 nav-item toc-entry"><a class="reference internal nav-link" href="#mahalanobis-distance">2.3.5.1.1. Mahalanobis distance</a></li>
</ul>
</li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#confidence-ellipsoid-we-can-define-an-ellipsoid-for-the-regression-coefficients-boldsymbol-beta-as">2.3.5.2. Confidence ellipsoid We can define an ellipsoid for the regression coefficients <span class="math notranslate nohighlight">\(\boldsymbol{\beta}\)</span> as</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#conditional-expectations-causality-observational-data-vs-designed-experiments">2.4. Conditional expectations/causality: observational data vs. designed experiments</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#model-selection-criteria-aic-and-bic">2.5. Model selection criteria: AIC and BIC</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#regularization-methods">2.6. Regularization methods</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#lasso-regression-l1-penalty">2.6.1. Lasso Regression (L1 Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#ridge-regression-l2-penalty">2.6.2. Ridge Regression (L2 Penalty)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#elastic-net-combination-of-l1-and-l2-penalties">2.6.3. Elastic Net (Combination of L1 and L2 Penalties)</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davide Cacciarelli
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>