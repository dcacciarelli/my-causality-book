
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>28. Multi-Armed Bandits &#8212; Applied Causal Inference with Examples from Electricity Markets</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/bandits';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="29. Design of Experiments" href="design_of_experiments.html" />
    <link rel="prev" title="27. A/B Testing" href="AB_testing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo_applied_causal_inference.png" class="logo__image only-light" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>
    <script>document.write(`<img src="../_static/logo_applied_causal_inference.png" class="logo__image only-dark" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Applied Causal Inference: Techniques and Applications in Electricity Markets
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Crash course on Stats and ML</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="review_stats.html">1. Probability Theory and Statistics</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_linear_models.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_ML.html">3. Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="motivation.html">4. Motivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide.html">5. What to expect from each chapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Basic Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="correlation_vs_causation.html">6. Correlation vs. Causation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAG.html">7. Causal Representations</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_dag_structures.html">8. Basic Causal Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">9. Definitions and Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II. Causal Discovery</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_discovery.html">10. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_direct_lingam.html">11. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_resit.html">12. Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_varlingam.html">13. Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="structural_breaks_example.html">14. Structural Breaks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III. Causal Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_inference.html">15. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="instrumental_variables.html">16. Instrumental Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="propensity_scores.html">17. Propensity Score Matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="double_machine_learning.html">18. Double Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="diff_in_diff.html">19. Difference-in-Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="interrupted_time_series.html">20. Interrupted Time Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IV. Interpretability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_interpretability.html">21. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="partial_dependency_plots.html">22. Partial Dependence Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="accumulated_local_effects.html">23. Accumulated Local Effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="impulse_response_functions.html">24. Impulse Response Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="shapley.html">25. Shapley Additive Explanations</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">V. Experiments and Data Collection</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_designs.html">26. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="AB_testing.html">27. A/B Testing</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">28. Multi-Armed Bandits</a></li>
<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html">29. Design of Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="active_learning.html">30. Active Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnotebooks/bandits.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/bandits.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Multi-Armed Bandits</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mab-vs-a-b-testing">28.1. MAB vs. A/B testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-high-level-example">28.2. Key concepts and high-level example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-algorithms-for-mab-problems">28.3. Key algorithms for MAB problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">28.3.1. Epsilon-greedy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">28.3.2. Upper confidence bound (UCB)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling">28.3.3. Thompson sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-three-algorithms-with-random-sampling">28.3.4. Comparing the three algorithms with random sampling</a></li>
</ul>
</li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="multi-armed-bandits">
<h1><span class="section-number">28. </span>Multi-Armed Bandits<a class="headerlink" href="#multi-armed-bandits" title="Link to this heading">#</a></h1>
<p>Multi-armed bandit (MAB) problems are a class of sequential decision-making problems that model the trade-off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one-armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls.</p>
<p>The MAB problem can be formalized as follows:</p>
<ul class="simple">
<li><p><strong>Arms</strong>: let <span class="math notranslate nohighlight">\(K\)</span> be the number of arms (choices or actions) available.</p></li>
<li><p><strong>Rewards</strong>: each arm <span class="math notranslate nohighlight">\(k \in \{1, 2, \ldots, K\}\)</span> provides a reward <span class="math notranslate nohighlight">\(r_k(t)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><strong>Objective</strong>: the objective is to maximize the cumulative reward over <span class="math notranslate nohighlight">\(T\)</span> rounds, <span class="math notranslate nohighlight">\(\sum_{t=1}^{T} r_k(t)\)</span>.</p></li>
</ul>
<p>Imagine you are a decision-maker in an electricity market, tasked with optimizing various strategies such as dynamic pricing, demand response programs, or marketing campaigns. Each strategy can be thought of as a slot machine (arm), each with an unknown probability of success (reward). You have a limited budget and need to decide how to allocate resources across these strategies to maximize your overall reward. The dilemma is that the more you explore different strategies to learn their effectiveness, the less you have left to exploit the best-performing strategy. This trade-off between exploration (trying different options to gather information) and exploitation (using known information to maximize reward) lies at the heart of the multi-armed bandit (MAB) problem.</p>
<p><strong>Practical applications</strong> of MAB problems in electricity markets might include:</p>
<ol class="arabic simple">
<li><p><strong>Dynamic pricing</strong>: suppose you need to determine the optimal pricing strategy for electricity during peak and off-peak hours. You could implement different pricing models (arms) and observe consumer reactions (rewards). Using MAB, you can dynamically adjust the pricing strategies based on observed data to maximize revenue without running prolonged inefficient experiments.</p></li>
<li><p><strong>Demand response programmes</strong>: consider various incentive schemes to encourage consumers to reduce usage during peak times. Each scheme can be tested (explored) initially, and based on which ones yield the highest reductions in usage (exploitation), more resources can be allocated to the most effective programs.</p></li>
<li><p><strong>Marketing campaigns</strong>: you may have multiple marketing strategies to promote energy-efficient appliances. Initially, you allocate equal resources to all strategies to see which performs best. As data comes in, you shift more resources to the campaigns that show higher engagement and conversion rates, optimizing your overall marketing budget.</p></li>
</ol>
<section id="mab-vs-a-b-testing">
<h2><span class="section-number">28.1. </span>MAB vs. A/B testing<a class="headerlink" href="#mab-vs-a-b-testing" title="Link to this heading">#</a></h2>
<p>Traditional methods for testing different options, such as A/B testing, involve splitting resources equally across different strategies (pure exploration), but this can be inefficient and costly, as it doesn’t adapt to the observed performance of the strategies. To this extent, the key <strong>limitations of A/B testing</strong> are:</p>
<ul class="simple">
<li><p><strong>Resource inefficiency</strong>: equally distributing resources among all strategies can waste time and opportunities on less-performing options.</p></li>
<li><p><strong>Costly</strong>: every test interaction involves costs related to market operations, consumer interactions, and potential financial impacts.</p></li>
<li><p><strong>Non-personalized</strong>: A/B testing typically identifies a winner for the majority, which may not be optimal for all segments of the market.</p></li>
</ul>
<p>On the other side, the <strong>advantages of MAB approaches</strong> include:</p>
<ul class="simple">
<li><p><strong>Dynamic allocation</strong>: MAB algorithms initially explore all options but gradually allocate more resources to the best-performing strategies, improving overall efficiency.</p></li>
<li><p><strong>Higher success rates</strong>: by continuously adapting to performance data, MAB approaches can increase the overall success rate of the strategies implemented.</p></li>
<li><p><strong>Contextual personalization</strong>: advanced MAB variants like contextual bandits tailor strategies to different market segments, enhancing personalization and engagement.</p></li>
</ul>
</section>
<section id="key-concepts-and-high-level-example">
<h2><span class="section-number">28.2. </span>Key concepts and high-level example<a class="headerlink" href="#key-concepts-and-high-level-example" title="Link to this heading">#</a></h2>
<p>The two key conepts in MAB problems are:</p>
<ol class="arabic simple">
<li><p><strong>Exploration vs. exploitation</strong>: where <strong>exploration</strong> means trying out different arms to gather more information about their rewards, and <strong>exploitation</strong> refers to choosing the arm that is currently believed to provide the highest reward.</p></li>
<li><p><strong>Regret</strong>: the difference between the reward obtained by the optimal arm and the reward obtained by the algorithm. The goal is to minimize regret over time.</p></li>
</ol>
<p>Let’s break down the MAB problem step by step, focusing on the concepts of exploration, reward, and how they are computed.</p>
<p><strong>Step-by-step illustration</strong></p>
<p><strong>Setup</strong></p>
<p>Imagine you have three slot machines (arms) in a casino, each with an unknown probability of payout (reward). Initially, you do not know which machine is the best, so you need to <strong>explore</strong> by trying out each machine. Over time, as you gather more information, you start to get an idea about the different machines and can <strong>exploit</strong> the available information by choosing the machine that seems to give the highest reward based on your observations.</p>
<p><strong>True reward probabilities</strong>:</p>
<ul class="simple">
<li><p>Slot Machine 1: 0.3</p></li>
<li><p>Slot Machine 2: 0.5</p></li>
<li><p>Slot Machine 3: 0.7</p></li>
</ul>
<p>These probabilities are unknown to you. Your goal is to find out which machine has the highest probability of giving a payout by trying them out.</p>
<p><strong>Step 1: initial exploration</strong></p>
<p>To start, you need to try each machine a few times to get an idea of their payouts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># True reward probabilities</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Number of times to pull each machine initially</span>
<span class="n">initial_pulls</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Simulate initial exploration</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">initial_rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_means</span><span class="p">)):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">initial_pulls</span><span class="p">):</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">initial_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

<span class="c1"># Print initial rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">initial_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rewards for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">rewards</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rewards for Machine 1: [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]
Rewards for Machine 2: [1, 0, 0, 1, 1, 1, 1, 0, 1, 1]
Rewards for Machine 3: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
</pre></div>
</div>
</div>
</div>
<p><strong>Step 2: compute average rewards</strong></p>
<p>After the initial exploration, compute the average reward for each machine to get an estimate of their payout probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate average rewards</span>
<span class="n">average_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">for</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="n">initial_rewards</span><span class="p">]</span>

<span class="c1"># Print average rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">avg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average reward for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">avg</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average reward for Machine 1: 0.30
Average reward for Machine 2: 0.70
Average reward for Machine 3: 0.90
</pre></div>
</div>
</div>
</div>
<p>The mean of the rewards obtained from the initial pulls for each machine. This gives an estimate of the payout probability for each machine.</p>
<p><strong>Step 3: exploitation</strong></p>
<p>Based on the average rewards, you start choosing the machine that seems to give the highest reward more often.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of additional rounds</span>
<span class="n">additional_rounds</span> <span class="o">=</span> <span class="mi">70</span>

<span class="c1"># Continue simulation</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="n">initial_rewards</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">additional_rounds</span><span class="p">)</span>
<span class="n">cumulative_rewards</span><span class="p">[:</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">for</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="n">initial_rewards</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">initial_pulls</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">additional_rounds</span><span class="p">):</span>
    <span class="c1"># Choose the machine with the highest average reward</span>
    <span class="n">best_machine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">)</span>
    
    <span class="c1"># Simulate pulling the best machine</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    
    <span class="c1"># Update average rewards</span>
    <span class="n">average_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">])</span>
    
    <span class="c1"># Update cumulative rewards</span>
    <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">reward</span>

<span class="c1"># Print final average rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">avg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final average reward for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">avg</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final average reward for Machine 1: 0.30
Final average reward for Machine 2: 0.64
Final average reward for Machine 3: 0.71
</pre></div>
</div>
</div>
</div>
<p>At each step, we define as <strong>best machine</strong> the machine with the highest average reward so far, and we continue pulling the best machine and update the average rewards based on the new observations.</p>
<p><strong>Step $: regret calculation</strong></p>
<p>Regret is a measure of how much worse our algorithm performs compared to if we had always chosen the best possible machine (the one with the highest true mean reward). If we knew in advance which machine had the highest probability of giving us a prize, we would always choose that machine. However, since we do not know this in advance, we have to try out all the machines to gather information. While we are trying out all the machines (exploring), we might choose the less optimal machines sometimes, which gives us less reward compared to the best machine.
Regret is the difference between the total reward we would have gotten by always choosing the best machine and the total reward we actually got by following our algorithm.</p>
<p>We have:</p>
<ul class="simple">
<li><p>Optimal reward: this is the reward we would have gotten if we had always chosen the best machine. It is calculated as the cumulative sum of the highest true mean reward over all rounds.</p></li>
<li><p>Algorithm reward: this is the reward we actually got by following our algorithm, which includes exploration and exploitation steps.</p></li>
<li><p>Regret: the difference between the optimal reward and the algorithm reward,</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># True means and optimal reward</span>
<span class="n">optimal_mean</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">true_means</span><span class="p">)</span>
<span class="n">optimal_cumulative_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">([</span><span class="n">optimal_mean</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">additional_rounds</span><span class="p">))</span>

<span class="c1"># Calculate regret</span>
<span class="n">regret</span> <span class="o">=</span> <span class="n">optimal_cumulative_rewards</span> <span class="o">-</span> <span class="n">cumulative_rewards</span>

<span class="c1"># Plot cumulative rewards and regret</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cumulative_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Algorithm Reward&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_cumulative_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Reward&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Regret&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Regret&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Regret Over Time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/e39fd570dc5a8b85da05d3fd80c48a1489edb91b9853c70fea7d436b76dff296.png" src="../_images/e39fd570dc5a8b85da05d3fd80c48a1489edb91b9853c70fea7d436b76dff296.png" />
</div>
</div>
<p><strong>How to read these graphs</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Cumulative reward</strong>:</p>
<ul class="simple">
<li><p>This graph shows the total reward accumulated over time by the algorithm.</p></li>
<li><p>The x-axis represents the number of rounds (time), and the y-axis represents the cumulative reward.</p></li>
<li><p>Initially, during the exploration phase, the rewards might increase slowly because the algorithm is trying out different arms to gather information.</p></li>
<li><p>As the algorithm gathers more information and starts to exploit the best-performing arm, the slope of the cumulative reward graph should increase, indicating a faster accumulation of rewards.</p></li>
<li><p>The optimal cumulative reward line represents the reward we would have accumulated if we had always chosen the best arm from the beginning. This line has a constant, steep slope, indicating the highest possible reward accumulation rate.</p></li>
</ul>
</li>
<li><p><strong>Regret</strong>:</p>
<ul class="simple">
<li><p>This graph shows the difference between the optimal cumulative reward and the cumulative reward obtained by the algorithm.</p></li>
<li><p>The x-axis represents the number of rounds (time), and the y-axis represents the regret.</p></li>
<li><p>At the beginning, the regret might increase rapidly because the algorithm is exploring and might be choosing suboptimal arms, leading to lower rewards compared to the optimal arm.</p></li>
<li><p>As the algorithm starts to exploit the best-performing arm more frequently, the rate at which regret increases should slow down. Ideally, the regret graph will start to flatten out, indicating that the algorithm is performing close to optimally.</p></li>
<li><p>The ideal scenario is to have a regret graph that flattens out as early as possible, showing that the algorithm quickly learned to choose the best arm.</p></li>
</ul>
</li>
</ol>
</section>
<section id="key-algorithms-for-mab-problems">
<h2><span class="section-number">28.3. </span>Key algorithms for MAB problems<a class="headerlink" href="#key-algorithms-for-mab-problems" title="Link to this heading">#</a></h2>
<p>In this section, we will explain three popular algorithms for solving the multi-armed bandit (MAB) problem: epsilon-greedy, upper confidence bound (UCB), and Thompson sampling. We will also compare their performance against a random strategy over multiple simulation runs.</p>
<section id="epsilon-greedy">
<h3><span class="section-number">28.3.1. </span>Epsilon-greedy<a class="headerlink" href="#epsilon-greedy" title="Link to this heading">#</a></h3>
<p>The Epsilon-Greedy algorithm balances exploration and exploitation by choosing a random arm with a small probability (<span class="math notranslate nohighlight">\(\epsilon\)</span>) and the best-known arm with a large probability (1 - <span class="math notranslate nohighlight">\(\epsilon\)</span>). Think of <span class="math notranslate nohighlight">\(\epsilon\)</span> as your curiosity factor. A higher <span class="math notranslate nohighlight">\(\epsilon\)</span> means you are more curious and willing to try different options even if you know some options perform well. A lower <span class="math notranslate nohighlight">\(\epsilon\)</span> means you are more inclined to stick with what you know works best.</p>
<p><strong>Mechanism</strong>:</p>
<ul class="simple">
<li><p><strong>Exploration</strong>: with a small probability (<span class="math notranslate nohighlight">\(\epsilon\)</span>), you choose a random arm to try out new options.</p></li>
<li><p><strong>Exploitation</strong>: with a large probability (1 - <span class="math notranslate nohighlight">\(\epsilon\)</span>), you choose the arm that has given you the highest average reward so far.</p></li>
</ul>
<p><strong>Formulation</strong>:</p>
<ul>
<li><p><strong>Estimated mean reward</strong>: for each arm <span class="math notranslate nohighlight">\(k\)</span>, you keep track of the average reward it has given you until this time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span>.</p></li>
<li><p><strong>Updating the Estimate</strong>: every time you pull an arm and get a reward, you update the average reward for that arm using the formula:</p>
<div class="amsmath math notranslate nohighlight" id="equation-2136edc2-4da6-46c0-93cb-859f5747f7bf">
<span class="eqno">(28.1)<a class="headerlink" href="#equation-2136edc2-4da6-46c0-93cb-859f5747f7bf" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{\mu}_k(t+1) = \hat{\mu}_k(t) + \frac{r_k(t) - \hat{\mu}_k(t)}{n_k(t)}
  \end{equation}\]</div>
</li>
</ul>
<p>where <span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span> is the current average reward for arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, <span class="math notranslate nohighlight">\(r_k(t)\)</span> is the reward you get from arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(n_k(t)\)</span> is the number of times you have pulled arm <span class="math notranslate nohighlight">\(k\)</span> up to time <span class="math notranslate nohighlight">\(t\)</span>.</p>
<p>Intuitively, when you receive a new reward for an arm, you need to update your estimate of the arm’s average reward. The goal is to make sure that this estimate becomes more accurate as you gather more data. To update the estimate of the average reward, we need to balance:</p>
<ul class="simple">
<li><p>New information: the reward you just observed provides new information about the arm’s performance.</p></li>
<li><p>Past experience the average reward you have observed so far reflects your past experience with that arm.</p></li>
</ul>
<p>The update formula balances these two by adjusting the current average reward (<span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span>) using the new reward (<span class="math notranslate nohighlight">\(r_k(t)\)</span>). The difference between the new reward and the current average reward is adjusted by <span class="math notranslate nohighlight">\(\frac{1}{n_k(t)}\)</span>. This fraction gets smaller as <span class="math notranslate nohighlight">\(n_k(t)\)</span>, the number of times the arm has been pulled, increases. The key reasons for this is that:</p>
<ul class="simple">
<li><p>Diminishing impact: the fraction <span class="math notranslate nohighlight">\(\frac{1}{n_k(t)}\)</span> ensures that the impact of new rewards diminishes over time. This means that early rewards have a bigger influence on your estimate, helping you quickly form an initial understanding of the arm’s performance.</p></li>
<li><p>As you pull the arm more times, you gather more information, and your estimate becomes more stable. New rewards should have less impact on the estimate because you already have a lot of information. Later rewards refine your estimate more subtly, preventing large swings in the estimate and ensuring that the estimate converges to the true average reward as you gather more data.</p></li>
</ul>
<p><strong>Simple example to illustrate how it works</strong></p>
<ul>
<li><p><strong>First Pull</strong>:</p>
<ul class="simple">
<li><p>Suppose you pull an arm for the first time and get a reward of 10.</p></li>
<li><p>Your initial estimate <span class="math notranslate nohighlight">\(\hat{\mu}_k(1)\)</span> is 10 because it’s the only data point you have.</p></li>
</ul>
</li>
<li><p><strong>Second Pull</strong>:</p>
<ul>
<li><p>You pull the same arm again and get a reward of 8.</p></li>
<li><p>You update your estimate:</p>
<div class="amsmath math notranslate nohighlight" id="equation-95a37017-e2b9-48bd-981e-db3454a20862">
<span class="eqno">(28.2)<a class="headerlink" href="#equation-95a37017-e2b9-48bd-981e-db3454a20862" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{\mu}_k(2) = 10 + \frac{8 - 10}{2} = 9
    \end{equation}\]</div>
</li>
<li><p>The new reward adjusts the estimate significantly because you only have two data points.</p></li>
</ul>
</li>
<li><p><strong>Hundredth Pull</strong>:</p>
<ul>
<li><p>After pulling the arm 99 times, suppose your current estimate is 9.5.</p></li>
<li><p>You pull the arm one more time and get a reward of 9.</p></li>
<li><p>You update your estimate:</p>
<div class="amsmath math notranslate nohighlight" id="equation-4cb4b0cb-7cf5-4f71-8162-faefe50b0770">
<span class="eqno">(28.3)<a class="headerlink" href="#equation-4cb4b0cb-7cf5-4f71-8162-faefe50b0770" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{\mu}_k(100) = 9.5 + \frac{9 - 9.5}{100} = 9.495
    \end{equation}\]</div>
</li>
<li><p>The new reward barely changes the estimate because you have a lot of data, making the estimate more stable.</p></li>
</ul>
</li>
</ul>
<p>By using this adjustment, you ensure that your estimate of the average reward for each arm becomes more accurate and stable over time, leading to better decision-making in the epsilon-greedy algorithm.</p>
<p><strong>Implementation</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">epsilon_greedy</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">n_arms</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
            <span class="c1"># Exploration: choose a random arm</span>
            <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="c1"># Exploitation: choose the best arm so far</span>
            <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">rewards</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">))</span>

        <span class="c1"># Simulate pulling the arm</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="upper-confidence-bound-ucb">
<h3><span class="section-number">28.3.2. </span>Upper confidence bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Link to this heading">#</a></h3>
<p>This algorithm selects arms based on the upper confidence bounds (UCBs) of their estimated rewards. The UCB is a combination of the estimated reward and a term that accounts for the uncertainty in the estimate. The key idea is to decide which options to try out by balancing between choosing the option that seems the best based on what you know so far and exploring less-tried options to discover their potential. It does this by considering both the estimated reward of each option and the uncertainty around that estimate.</p>
<p><strong>Mechanism</strong>:</p>
<ul class="simple">
<li><p>At each time step <span class="math notranslate nohighlight">\(t\)</span>, select the arm <span class="math notranslate nohighlight">\(k\)</span> that maximizes the upper confidence bound <span class="math notranslate nohighlight">\(UCB_k(t)\)</span>.</p></li>
</ul>
<p><strong>Formulation</strong>:</p>
<ul class="simple">
<li><p>The upper confidence bound for arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is given by:</p></li>
</ul>
<div class="amsmath math notranslate nohighlight" id="equation-ce4ef439-45d8-4acd-9d9c-d62bf965d60c">
<span class="eqno">(28.4)<a class="headerlink" href="#equation-ce4ef439-45d8-4acd-9d9c-d62bf965d60c" title="Permalink to this equation">#</a></span>\[\begin{equation}
    UCB_k(t) = \hat{\mu}_k(t) + c \sqrt{\frac{\ln t}{n_k(t)}}
\end{equation}\]</div>
<p>which is composed of two terms:</p>
<ul class="simple">
<li><p><strong>Estimated reward</strong>: the term <span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span> represents our current best guess of the mean reward for arm <span class="math notranslate nohighlight">\(k\)</span> based on the rewards we have observed so far. This is our current estimate of the average reward for arm <span class="math notranslate nohighlight">\(k\)</span>. It is updated every time we pull the arm and observe a reward, using the same updating mechanism as in the epsilon-greedy algorithm.</p></li>
<li><p><strong>Uncertainty term</strong>: The term <span class="math notranslate nohighlight">\(c \sqrt{\frac{\ln t}{n_k(t)}}\)</span> represents the uncertainty or confidence interval around the estimated reward. It ensures that arms with fewer pulls (higher uncertainty) are given a higher chance of being selected. Here, <span class="math notranslate nohighlight">\(n_k(t)\)</span> is the number of times arm <span class="math notranslate nohighlight">\(k\)</span> has been selected up to time <span class="math notranslate nohighlight">\(t\)</span>, and <span class="math notranslate nohighlight">\(c\)</span> is a confidence parameter that controls the degree of exploration. Let’s break down the components of this term to better understand how it works:</p>
<ul>
<li><p><span class="math notranslate nohighlight">\(\ln t\)</span>: The natural logarithm of the current time step <span class="math notranslate nohighlight">\(t\)</span>. As time progresses, this term grows, but at a decreasing rate.</p></li>
<li><p><span class="math notranslate nohighlight">\(\frac{1}{n_k(t)}\)</span>: the reciprocal of the number of times arm <span class="math notranslate nohighlight">\(k\)</span> has been pulled. This term decreases as we pull the arm more often.</p></li>
<li><p>Square root: the square root ensures that the uncertainty term grows more slowly as the number of pulls increases.</p></li>
<li><p>Confidence parameter <span class="math notranslate nohighlight">\(c\)</span>: this parameter controls how much weight we give to the uncertainty term. A higher <span class="math notranslate nohighlight">\(c\)</span> value means more exploration.</p></li>
</ul>
</li>
</ul>
<p>Why This Works:</p>
<ul class="simple">
<li><p><strong>Early stages</strong>: at the beginning of the process, <span class="math notranslate nohighlight">\(n_k(t)\)</span> is small for all arms, making the uncertainty term large. This encourages exploration of all arms to gather initial information.</p></li>
<li><p><strong>Later stages</strong>: as <span class="math notranslate nohighlight">\(n_k(t)\)</span> increases for an arm, the uncertainty term decreases. This means that the algorithm will increasingly favor arms with higher estimated rewards, but will still occasionally explore other arms to ensure they are not overlooked. Over time, the algorithm balances between exploiting machines with high estimated rewards and exploring machines with higher uncertainty to ensure it does not miss out on potentially better options.</p></li>
</ul>
<p><strong>Implementation</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">def</span> <span class="nf">ucb</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">n_arms</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">)</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
        <span class="n">ucb_values</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">)</span> <span class="o">+</span> <span class="n">c</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">counts</span> <span class="o">+</span> <span class="mf">1e-5</span><span class="p">))</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ucb_values</span><span class="p">)</span>

        <span class="c1"># Simulate pulling the arm</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="n">rewards</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
        <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>

    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="thompson-sampling">
<h3><span class="section-number">28.3.3. </span>Thompson sampling<a class="headerlink" href="#thompson-sampling" title="Link to this heading">#</a></h3>
<p>Thompson sampling balances between trying out different options (exploration) and sticking with the best-known option (exploitation) by using probability distributions to model the uncertainty of each option’s rewards.</p>
<p><strong>Mechanism</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Maintain a probability distribution</strong>: for each option (or arm), we keep track of a probability distribution that represents our belief about the likelihood of getting a reward from that option. In Thompson sampling, we use the Beta distribution for this purpose.</p></li>
<li><p><strong>Sample a value</strong>: from these distributions, we randomly sample a value for each option. These sampled values represent our current guess about the expected reward from each option. This step introduces randomness, ensuring that we occasionally try out arms that have fewer successes but might have potential.</p></li>
<li><p><strong>Choose the best option</strong>: we select the option with the highest sampled value. This means that we are more likely to choose options that have shown higher rewards in the past, but we still occasionally try out other options to gather more information about them.</p></li>
</ol>
<p><strong>Formulation</strong>:</p>
<ol class="arabic">
<li><p><strong>Beta distribution</strong>: the reward probability of each arm is assumed to follow a Beta distribution. The Beta distribution is a family of continuous probability distributions defined on the interval <code class="docutils literal notranslate"><span class="pre">[0,</span> <span class="pre">1]</span></code>, parameterized by two positive shape parameters, <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>. The distribution represents the posterior probability of the success rate of a Bernoulli trial (a trial with two outcomes, success or failure). The first parameter, <span class="math notranslate nohighlight">\(\alpha_k\)</span> represents the number of successes (rewards), while <span class="math notranslate nohighlight">\(\beta_k\)</span> represents the number of failures (no rewards) for arm <span class="math notranslate nohighlight">\(k\)</span>. For example, if we have an arm (option) with <span class="math notranslate nohighlight">\(\alpha = 5\)</span> and <span class="math notranslate nohighlight">\(\beta = 3\)</span>, the Beta distribution would give us a distribution of possible success probabilities for this arm, based on the 5 successes and 3 failures observed so far. When we get new data, we can easily update the Beta distribution with simple calculations. This is because the Beta distribution is a “conjugate prior” for the Bernoulli distribution, which means their mathematical properties align perfectly for easy updates.</p></li>
<li><p><strong>Updating the distribution</strong>: when we pull an arm <span class="math notranslate nohighlight">\(k \)</span> and observe a reward <span class="math notranslate nohighlight">\(r_k(t)\)</span>, we update the parameters of the Beta distribution as follows:</p>
<ul>
<li><p>If we get a reward (<span class="math notranslate nohighlight">\(r_k(t) = 1 \)</span>), we increase <span class="math notranslate nohighlight">\(\alpha_k\)</span> by 1, indicating one more success:</p>
<div class="amsmath math notranslate nohighlight" id="equation-523c0d5b-b3d3-4192-baa9-1dcd803acfb2">
<span class="eqno">(28.5)<a class="headerlink" href="#equation-523c0d5b-b3d3-4192-baa9-1dcd803acfb2" title="Permalink to this equation">#</a></span>\[\begin{equation}
         \alpha_k = \alpha_k + r_k(t)
     \end{equation}\]</div>
</li>
<li><p>If we do not get a reward (<span class="math notranslate nohighlight">\(r_k(t) = 0 \)</span>), we increase <span class="math notranslate nohighlight">\(\beta_k\)</span> by 1, indicating one more failure:</p>
<div class="amsmath math notranslate nohighlight" id="equation-cbc049e8-2d2a-473a-9c46-cbd8cf18e537">
<span class="eqno">(28.6)<a class="headerlink" href="#equation-cbc049e8-2d2a-473a-9c46-cbd8cf18e537" title="Permalink to this equation">#</a></span>\[\begin{equation}
         \beta_k = \beta_k + 1 - r_k(t)
     \end{equation}\]</div>
</li>
</ul>
</li>
</ol>
<p><strong>In practice</strong>:
Initially, both <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span> start at zero since we have not observed any rewards or failures yet. However, the Beta distribution with parameters <span class="math notranslate nohighlight">\(\alpha = 0\)</span> and <span class="math notranslate nohighlight">\(\beta = 0\)</span> is undefined because the Beta distribution requires positive parameters. So, we add 1 to each of these two values:</p>
<ul class="simple">
<li><p><strong>Initial prior</strong>: by starting with <span class="math notranslate nohighlight">\(\alpha = 1\)</span> and <span class="math notranslate nohighlight">\(\beta = 1\)</span>, we assume a weak prior belief that each arm has an equal probability of success and failure. This is often referred to as a “non-informative” or “uniform” prior.</p></li>
<li><p><strong>Practicality</strong>: it ensures that the Beta distribution is always defined, allowing us to sample from it even before any data is observed.</p></li>
<li><p><strong>Conjugate prior</strong>: When we update the Beta distribution with observed data, adding 1 ensures that our initial prior belief is combined with the observed data correctly.</p></li>
</ul>
<p>Suppose we have an arm that has been pulled 10 times, resulting in 7 successes and 3 failures:</p>
<ul class="simple">
<li><p>Successes: 7</p></li>
<li><p>Failures: 3</p></li>
</ul>
<p>For Thompson sampling:</p>
<ul class="simple">
<li><p>The <span class="math notranslate nohighlight">\(\alpha\)</span> parameter (successes) is <span class="math notranslate nohighlight">\(7 + 1 = 8\)</span></p></li>
<li><p>The <span class="math notranslate nohighlight">\(\beta\)</span> parameter (failures) is <span class="math notranslate nohighlight">\(3 + 1 = 4\)</span></p></li>
</ul>
<p>This gives us a Beta distribution with parameters <span class="math notranslate nohighlight">\(\alpha = 8\)</span> and <span class="math notranslate nohighlight">\(\beta = 4\)</span>, from which we can sample to estimate the probability of success for this arm.</p>
<p><strong>Implementation</strong>:</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Thompson Sampling algorithm</span>
<span class="k">def</span> <span class="nf">thompson_sampling</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">n_arms</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">)</span>
    <span class="n">successes</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">failures</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">total_rewards</span> <span class="o">=</span> <span class="p">[]</span>

    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">):</span>
        <span class="n">sampled_probs</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">successes</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">failures</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)]</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">sampled_probs</span><span class="p">)</span>

        <span class="c1"># Simulate pulling the arm</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">arms</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
        <span class="k">if</span> <span class="n">reward</span><span class="p">:</span>
            <span class="n">successes</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">failures</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
        <span class="n">total_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="comparing-the-three-algorithms-with-random-sampling">
<h3><span class="section-number">28.3.4. </span>Comparing the three algorithms with random sampling<a class="headerlink" href="#comparing-the-three-algorithms-with-random-sampling" title="Link to this heading">#</a></h3>
<p>Now, we can run a comprehensive example where we compare the performance of epsilon-greedy, UCB, and Thompson sampling against a random strategy.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Define the true reward probabilities for each arm</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.8</span><span class="p">]</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">100</span>
<span class="n">n_simulations</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Simulation to collect rewards</span>
<span class="k">def</span> <span class="nf">simulate</span><span class="p">(</span><span class="n">algorithm</span><span class="p">,</span> <span class="n">arms</span><span class="p">,</span> <span class="n">n_simulations</span><span class="o">=</span><span class="mi">100</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="mi">1000</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_simulations</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">))</span>
    <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_simulations</span><span class="p">):</span>
        <span class="n">rewards</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:]</span> <span class="o">=</span> <span class="n">algorithm</span><span class="p">(</span><span class="n">arms</span><span class="p">,</span> <span class="n">n_rounds</span><span class="o">=</span><span class="n">n_rounds</span><span class="p">,</span> <span class="o">**</span><span class="n">kwargs</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">rewards</span>

<span class="c1"># Run simulations</span>
<span class="n">rewards_random</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span><span class="n">epsilon_greedy</span><span class="p">,</span> <span class="n">true_means</span><span class="p">,</span> <span class="n">n_simulations</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">1.0</span><span class="p">)</span>  <span class="c1"># Pure exploration (random)</span>
<span class="n">rewards_epsilon_greedy</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span><span class="n">epsilon_greedy</span><span class="p">,</span> <span class="n">true_means</span><span class="p">,</span> <span class="n">n_simulations</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">epsilon</span><span class="o">=</span><span class="mf">0.1</span><span class="p">)</span>
<span class="n">rewards_ucb</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span><span class="n">ucb</span><span class="p">,</span> <span class="n">true_means</span><span class="p">,</span> <span class="n">n_simulations</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="mi">2</span><span class="p">)</span>
<span class="n">rewards_thompson</span> <span class="o">=</span> <span class="n">simulate</span><span class="p">(</span><span class="n">thompson_sampling</span><span class="p">,</span> <span class="n">true_means</span><span class="p">,</span> <span class="n">n_simulations</span><span class="p">,</span> <span class="n">n_rounds</span><span class="p">)</span>
</pre></div>
</div>
</div>
</div>
<p>We can the compare them in terms of <strong>cumulative reward</strong>, showing how the total reward accumulates over time for each algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate mean cumulative rewards</span>
<span class="n">mean_rewards_random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_random</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_rewards_epsilon_greedy</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_epsilon_greedy</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_rewards_ucb</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_ucb</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">mean_rewards_thompson</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards_thompson</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_rewards_random</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Random&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_rewards_epsilon_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Epsilon-Greedy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_rewards_ucb</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;UCB&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">mean_rewards_thompson</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Thompson Sampling&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward Comparison&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/c151f6ec889a3236b5975f1fef17aa0241136ee2a56669400bf1a6500764ec76.png" src="../_images/c151f6ec889a3236b5975f1fef17aa0241136ee2a56669400bf1a6500764ec76.png" />
</div>
</div>
<p>and here in terms of <strong>regret</strong>, showing the difference between the reward of the optimal machine and the reward obtained by each algorithm.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate regret</span>
<span class="n">optimal_mean</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">true_means</span><span class="p">)</span>
<span class="n">regret_random</span> <span class="o">=</span> <span class="n">optimal_mean</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_rewards_random</span>
<span class="n">regret_epsilon_greedy</span> <span class="o">=</span> <span class="n">optimal_mean</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_rewards_epsilon_greedy</span>
<span class="n">regret_ucb</span> <span class="o">=</span> <span class="n">optimal_mean</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_rewards_ucb</span>
<span class="n">regret_thompson</span> <span class="o">=</span> <span class="n">optimal_mean</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span> <span class="o">-</span> <span class="n">mean_rewards_thompson</span>

<span class="c1"># Plot regret</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret_random</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Random&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret_epsilon_greedy</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Epsilon-Greedy&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret_ucb</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;UCB&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;m&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret_thompson</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Thompson Sampling&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;b&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Regret&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cumulative Regret Comparison&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/2bf49d4e478ea502c2d579867e38585edb7c08bc556cf141c8fad9527c041120.png" src="../_images/2bf49d4e478ea502c2d579867e38585edb7c08bc556cf141c8fad9527c041120.png" />
</div>
</div>
<p>Through these plots, we can visually compare the efficiency and performance of different algorithms in balancing exploration and exploitation, highlighting the advantages of more sophisticated MAB strategies over a random approach.</p>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="AB_testing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">27. </span>A/B Testing</p>
      </div>
    </a>
    <a class="right-next"
       href="design_of_experiments.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">29. </span>Design of Experiments</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mab-vs-a-b-testing">28.1. MAB vs. A/B testing</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts-and-high-level-example">28.2. Key concepts and high-level example</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#key-algorithms-for-mab-problems">28.3. Key algorithms for MAB problems</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">28.3.1. Epsilon-greedy</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">28.3.2. Upper confidence bound (UCB)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling">28.3.3. Thompson sampling</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#comparing-the-three-algorithms-with-random-sampling">28.3.4. Comparing the three algorithms with random sampling</a></li>
</ul>
</li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davide Cacciarelli
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>