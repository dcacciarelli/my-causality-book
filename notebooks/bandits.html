
<!DOCTYPE html>


<html lang="en" data-content_root="../" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>28. Multi-Armed Bandits &#8212; Applied Causal Inference with Examples from Electricity Markets</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="../_static/styles/theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/bootstrap.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
<link href="../_static/styles/pydata-sphinx-theme.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />

  
  <link href="../_static/vendor/fontawesome/6.5.1/css/all.min.css?digest=8d27b9dea8ad943066ae" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="../_static/vendor/fontawesome/6.5.1/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="../_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="../_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="../_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae" />
<link rel="preload" as="script" href="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae" />
  <script src="../_static/vendor/fontawesome/6.5.1/js/all.min.js?digest=8d27b9dea8ad943066ae"></script>

    <script src="../_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="../_static/doctools.js?v=9a2dae69"></script>
    <script src="../_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="../_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="../_static/copybutton.js?v=f281be69"></script>
    <script src="../_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="../_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="../_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="../_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = 'notebooks/bandits';</script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="31. Design of Experiments" href="design_of_experiments.html" />
    <link rel="prev" title="27. A/B Testing" href="AB_testing.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a id="pst-skip-link" class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>
  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="../search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <header class="bd-header navbar navbar-expand-lg bd-navbar">
    </header>
  

  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">

  

<a class="navbar-brand logo" href="../intro.html">
  
  
  
  
  
    
    
      
    
    
    <img src="../_static/logo_applied_causal_inference.png" class="logo__image only-light" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>
    <script>document.write(`<img src="../_static/logo_applied_causal_inference.png" class="logo__image only-dark" alt="Applied Causal Inference with Examples from Electricity Markets - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">

 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="../intro.html">
                    Applied Causal Inference for Electricity Markets
                </a>
            </li>
        </ul>
        <p aria-level="2" class="caption" role="heading"><span class="caption-text">Crash course on Stats and ML</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="review_stats.html">1. Statistics and Probability</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_linear_models.html">2. Linear Regression</a></li>
<li class="toctree-l1"><a class="reference internal" href="review_ML.html">3. Machine Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Introduction</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="motivation.html">4. Motivation</a></li>
<li class="toctree-l1"><a class="reference internal" href="guide.html">5. What to expect from each chapter</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">I. Basic Concepts</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="correlation_vs_causation.html">6. Correlation vs. Causation</a></li>
<li class="toctree-l1"><a class="reference internal" href="DAG.html">7. What is a Causal Graph</a></li>
<li class="toctree-l1"><a class="reference internal" href="basic_dag_structures.html">8. Basic Causal Structures</a></li>
<li class="toctree-l1"><a class="reference internal" href="glossary.html">9. Glossary: Key Definitions and Terminology</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">II. Causal Discovery</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_discovery.html">10. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_direct_lingam.html">11. Linear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_resit.html">12. Nonlinear Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="semiparametric_varlingam.html">13. Time Series Models</a></li>
<li class="toctree-l1"><a class="reference internal" href="structural_breaks_example.html">14. Structural Breaks</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">III. Causal Inference</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_causal_inference.html">15. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="instrumental_variables.html">16. Instrumental Variables</a></li>
<li class="toctree-l1"><a class="reference internal" href="propensity_scores.html">17. Propensity Score Matching</a></li>
<li class="toctree-l1"><a class="reference internal" href="double_machine_learning.html">18. Double Machine Learning</a></li>
<li class="toctree-l1"><a class="reference internal" href="diff_in_diff.html">19. Difference-in-Differences</a></li>
<li class="toctree-l1"><a class="reference internal" href="interrupted_time_series.html">20. Interrupted Time Series</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">IV. Interpretability</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_interpretability.html">21. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="partial_dependency_plots.html">22. Partial Dependence Plots</a></li>
<li class="toctree-l1"><a class="reference internal" href="accumulated_local_effects.html">23. Accumulated Local Effects</a></li>
<li class="toctree-l1"><a class="reference internal" href="impulse_response_functions.html">24. Impulse Response Functions</a></li>
<li class="toctree-l1"><a class="reference internal" href="shapley.html">25. Shapley Values</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">V. Experiments and Data Collection</span></p>
<ul class="current nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="preface_designs.html">26. Overview</a></li>
<li class="toctree-l1"><a class="reference internal" href="AB_testing.html">27. A/B Testing</a></li>
<li class="toctree-l1 current active"><a class="current reference internal" href="#">28. Multi-Armed Bandits</a></li>


<li class="toctree-l1"><a class="reference internal" href="design_of_experiments.html">31. Design of Experiments</a></li>
<li class="toctree-l1"><a class="reference internal" href="active_learning.html">32. Active Learning</a></li>
</ul>
<p aria-level="2" class="caption" role="heading"><span class="caption-text">Other</span></p>
<ul class="nav bd-sidenav">
<li class="toctree-l1"><a class="reference internal" href="../bibliography.html">References</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/executablebooks/jupyter-book/issues/new?title=Issue%20on%20page%20%2Fnotebooks/bandits.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="../_sources/notebooks/bandits.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>



<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Multi-Armed Bandits</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">28. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">28.1. Multi-Armed Bandits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">28.2. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevance-in-electricity-markets">28.3. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">28.4. Mathematical Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">28.4.1. Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms">28.4.2. Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">28.4.3. Epsilon-Greedy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-intuition">28.4.3.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-details">28.4.3.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">28.4.4. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">28.4.4.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">28.4.4.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling">28.4.5. Thompson Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">28.4.5.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">28.4.5.2. Mathematical Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python">28.5. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-implementation">28.5.1. Step-by-Step Implementation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">29. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">29.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">29.2. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">29.3. Exploration vs. Exploitation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">29.3.1. High-Level Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-example">29.3.2. Practical Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulating-the-example">29.3.3. Simulating the Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">29.3.4. Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-regret">29.3.5. Understanding Regret</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">29.3.6. Mathematical Formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">29.4. Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">29.4.1. Epsilon-Greedy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">29.4.1.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">29.4.1.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">29.4.2. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">29.4.2.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">29.4.2.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">29.4.3. Thompson Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">29.4.3.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">29.4.3.2. Mathematical Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">29.5. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">29.5.1. Step-by-Step Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">29.5.2. Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">29.5.3. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">30. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">30.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">30.2. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">30.3. Mathematical Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">30.3.1. Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">30.3.2. Algorithms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">30.4. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">30.4.1. Step-by-Step Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">30.4.2. Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">30.4.3. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="multi-armed-bandits">
<h1><span class="section-number">28. </span>Multi-Armed Bandits<a class="headerlink" href="#multi-armed-bandits" title="Link to this heading">#</a></h1>
<p>Multi-armed bandit (MAB) problems are a class of sequential decision-making problems that model the trade-off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one-armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls.</p>
<p>The MAB problem can be formalized as follows:</p>
<ul class="simple">
<li><p><strong>Arms</strong>: let <span class="math notranslate nohighlight">\(K\)</span> be the number of arms (choices or actions) available.</p></li>
<li><p><strong>Rewards</strong>: each arm <span class="math notranslate nohighlight">\(k \in \{1, 2, \ldots, K\}\)</span> provides a reward <span class="math notranslate nohighlight">\(r_k(t)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><strong>Objective</strong>: the objective is to maximize the cumulative reward over <span class="math notranslate nohighlight">\(T\)</span> rounds, <span class="math notranslate nohighlight">\(\sum_{t=1}^{T} r_k(t)\)</span>.</p></li>
</ul>
<p>Imagine you are a decision-maker in an electricity market, tasked with optimizing various strategies such as dynamic pricing, demand response programs, or marketing campaigns. Each strategy can be thought of as a slot machine (arm), each with an unknown probability of success (reward). You have a limited budget and need to decide how to allocate resources across these strategies to maximize your overall reward. The dilemma is that the more you explore different strategies to learn their effectiveness, the less you have left to exploit the best-performing strategy. This trade-off between exploration (trying different options to gather information) and exploitation (using known information to maximize reward) lies at the heart of the multi-armed bandit (MAB) problem.</p>
<p><strong>Practical applications</strong> of MAB problems in electricity markets might include:</p>
<ol class="arabic simple">
<li><p><strong>Dynamic pricing</strong>: suppose you need to determine the optimal pricing strategy for electricity during peak and off-peak hours. You could implement different pricing models (arms) and observe consumer reactions (rewards). Using MAB, you can dynamically adjust the pricing strategies based on observed data to maximize revenue without running prolonged inefficient experiments.</p></li>
<li><p><strong>Demand response programmes</strong>: consider various incentive schemes to encourage consumers to reduce usage during peak times. Each scheme can be tested (explored) initially, and based on which ones yield the highest reductions in usage (exploitation), more resources can be allocated to the most effective programs.</p></li>
<li><p><strong>Marketing campaigns</strong>: you may have multiple marketing strategies to promote energy-efficient appliances. Initially, you allocate equal resources to all strategies to see which performs best. As data comes in, you shift more resources to the campaigns that show higher engagement and conversion rates, optimizing your overall marketing budget.</p></li>
</ol>
<p><strong>MAB vs. A/B testing</strong>
Traditional methods for testing different options, such as A/B testing, involve splitting resources equally across different strategies (pure exploration), but this can be inefficient and costly, as it doesn’t adapt to the observed performance of the strategies. To this extent, the key <strong>limitations of A/B testing</strong> are:</p>
<ul class="simple">
<li><p><strong>Resource inefficiency</strong>: equally distributing resources among all strategies can waste time and opportunities on less-performing options.</p></li>
<li><p><strong>Costly</strong>: every test interaction involves costs related to market operations, consumer interactions, and potential financial impacts.</p></li>
<li><p><strong>Non-personalized</strong>: A/B testing typically identifies a winner for the majority, which may not be optimal for all segments of the market.</p></li>
</ul>
<p>On the other side, the <strong>advantages of MAB approaches</strong> include:</p>
<ul class="simple">
<li><p><strong>Dynamic allocation</strong>: MAB algorithms initially explore all options but gradually allocate more resources to the best-performing strategies, improving overall efficiency.</p></li>
<li><p><strong>Higher success rates</strong>: by continuously adapting to performance data, MAB approaches can increase the overall success rate of the strategies implemented.</p></li>
<li><p><strong>Contextual personalization</strong>: advanced MAB variants like contextual bandits tailor strategies to different market segments, enhancing personalization and engagement.</p></li>
</ul>
<p>The two <strong>key conepts</strong> in MAB problems are:</p>
<ol class="arabic simple">
<li><p><strong>Exploration vs. exploitation</strong>: where <strong>exploration</strong> means trying out different arms to gather more information about their rewards, and <strong>exploitation</strong> refers to choosing the arm that is currently believed to provide the highest reward.</p></li>
<li><p><strong>Regret</strong>: the difference between the reward obtained by the optimal arm and the reward obtained by the algorithm. The goal is to minimize regret over time.</p></li>
</ol>
<p>Let’s break down the MAB problem step by step, focusing on the concepts of exploration, reward, and how they are computed.</p>
<p><strong>Step-by-step illustration</strong></p>
<p><strong>Setup</strong></p>
<p>Imagine you have three slot machines (arms) in a casino, each with an unknown probability of payout (reward). Initially, you do not know which machine is the best, so you need to <strong>explore</strong> by trying out each machine. Over time, as you gather more information, you start to get an idea about the different machines and can <strong>exploit</strong> the available information by choosing the machine that seems to give the highest reward based on your observations.</p>
<p><strong>True reward probabilities</strong>:</p>
<ul class="simple">
<li><p>Slot Machine 1: 0.3</p></li>
<li><p>Slot Machine 2: 0.5</p></li>
<li><p>Slot Machine 3: 0.7</p></li>
</ul>
<p>These probabilities are unknown to you. Your goal is to find out which machine has the highest probability of giving a payout by trying them out.</p>
<p><strong>Step 1: initial exploration</strong></p>
<p>To start, you need to try each machine a few times to get an idea of their payouts.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># True reward probabilities</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>

<span class="c1"># Number of times to pull each machine initially</span>
<span class="n">initial_pulls</span> <span class="o">=</span> <span class="mi">10</span>

<span class="c1"># Simulate initial exploration</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">42</span><span class="p">)</span>
<span class="n">initial_rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">true_means</span><span class="p">)):</span>
    <span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
    <span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">initial_pulls</span><span class="p">):</span>
        <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
        <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    <span class="n">initial_rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>

<span class="c1"># Print initial rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">initial_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Rewards for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">rewards</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Rewards for Machine 1: [0, 0, 0, 0, 1, 1, 1, 0, 0, 0]
Rewards for Machine 2: [1, 0, 0, 1, 1, 1, 1, 0, 1, 1]
Rewards for Machine 3: [1, 1, 1, 1, 1, 0, 1, 1, 1, 1]
</pre></div>
</div>
</div>
</div>
<p><strong>Step 2: compute average rewards</strong></p>
<p>After the initial exploration, compute the average reward for each machine to get an estimate of their payout probabilities.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Calculate average rewards</span>
<span class="n">average_rewards</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">for</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="n">initial_rewards</span><span class="p">]</span>

<span class="c1"># Print average rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">avg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Average reward for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">avg</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Average reward for Machine 1: 0.30
Average reward for Machine 2: 0.70
Average reward for Machine 3: 0.90
</pre></div>
</div>
</div>
</div>
<p>The mean of the rewards obtained from the initial pulls for each machine. This gives an estimate of the payout probability for each machine.</p>
<p><strong>Step 3: exploitation</strong></p>
<p>Based on the average rewards, you start choosing the machine that seems to give the highest reward more often.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Number of additional rounds</span>
<span class="n">additional_rounds</span> <span class="o">=</span> <span class="mi">90</span>

<span class="c1"># Continue simulation</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="n">initial_rewards</span><span class="o">.</span><span class="n">copy</span><span class="p">()</span>
<span class="n">cumulative_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">additional_rounds</span><span class="p">)</span>
<span class="n">cumulative_rewards</span><span class="p">[:</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">([</span><span class="nb">sum</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span> <span class="k">for</span> <span class="n">rewards</span> <span class="ow">in</span> <span class="n">initial_rewards</span><span class="p">])</span><span class="o">.</span><span class="n">repeat</span><span class="p">(</span><span class="n">initial_pulls</span><span class="p">)</span>

<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">additional_rounds</span><span class="p">):</span>
    <span class="c1"># Choose the machine with the highest average reward</span>
    <span class="n">best_machine</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">)</span>
    
    <span class="c1"># Simulate pulling the best machine</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">reward</span><span class="p">)</span>
    
    <span class="c1"># Update average rewards</span>
    <span class="n">average_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">mean</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="n">best_machine</span><span class="p">])</span>
    
    <span class="c1"># Update cumulative rewards</span>
    <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">t</span><span class="p">]</span> <span class="o">=</span> <span class="n">cumulative_rewards</span><span class="p">[</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">t</span> <span class="o">-</span> <span class="mi">1</span><span class="p">]</span> <span class="o">+</span> <span class="n">reward</span>

<span class="c1"># Print final average rewards</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">avg</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">average_rewards</span><span class="p">):</span>
    <span class="nb">print</span><span class="p">(</span><span class="sa">f</span><span class="s2">&quot;Final average reward for Machine </span><span class="si">{</span><span class="n">i</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="mi">1</span><span class="si">}</span><span class="s2">: </span><span class="si">{</span><span class="n">avg</span><span class="si">:</span><span class="s2">.2f</span><span class="si">}</span><span class="s2">&quot;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Final average reward for Machine 1: 0.30
Final average reward for Machine 2: 0.64
Final average reward for Machine 3: 0.70
</pre></div>
</div>
</div>
</div>
<p>At each step, we define as <strong>best machine</strong> the machine with the highest average reward so far, and we continue pulling the best machine and update the average rewards based on the new observations.</p>
<p><strong>Step 4: regret calculation</strong></p>
<p>Regret is a measure of how much worse our algorithm performs compared to if we had always chosen the best possible machine (the one with the highest true mean reward). If we knew in advance which machine had the highest probability of giving us a prize, we would always choose that machine. However, since we do not know this in advance, we have to try out all the machines to gather information. While we are trying out all the machines (exploring), we might choose the less optimal machines sometimes, which gives us less reward compared to the best machine.
Regret is the difference between the total reward we would have gotten by always choosing the best machine and the total reward we actually got by following our algorithm.</p>
<p>We have:</p>
<ul class="simple">
<li><p>Optimal reward: this is the reward we would have gotten if we had always chosen the best machine. It is calculated as the cumulative sum of the highest true mean reward over all rounds.</p></li>
<li><p>Algorithm reward: this is the reward we actually got by following our algorithm, which includes exploration and exploitation steps.</p></li>
<li><p>Regret: the difference between the optimal reward and the algorithm reward,</p></li>
</ul>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># True means and optimal reward</span>
<span class="n">optimal_mean</span> <span class="o">=</span> <span class="nb">max</span><span class="p">(</span><span class="n">true_means</span><span class="p">)</span>
<span class="n">optimal_cumulative_rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">cumsum</span><span class="p">([</span><span class="n">optimal_mean</span><span class="p">]</span> <span class="o">*</span> <span class="p">(</span><span class="n">initial_pulls</span> <span class="o">*</span> <span class="mi">3</span> <span class="o">+</span> <span class="n">additional_rounds</span><span class="p">))</span>

<span class="c1"># Calculate regret</span>
<span class="n">regret</span> <span class="o">=</span> <span class="n">optimal_cumulative_rewards</span> <span class="o">-</span> <span class="n">cumulative_rewards</span>

<span class="c1"># Plot cumulative rewards and regret</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">cumulative_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Algorithm Reward&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;c&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">optimal_cumulative_rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Optimal Reward&#39;</span><span class="p">,</span> <span class="n">linestyle</span><span class="o">=</span><span class="s1">&#39;--&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">c</span><span class="o">=</span><span class="s1">&#39;k&#39;</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>

<span class="n">plt</span><span class="o">.</span><span class="n">subplot</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">regret</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Regret&#39;</span><span class="p">,</span> <span class="n">color</span><span class="o">=</span><span class="s1">&#39;r&#39;</span><span class="p">,</span> <span class="n">lw</span><span class="o">=</span><span class="mi">3</span><span class="p">,</span> <span class="n">alpha</span><span class="o">=</span><span class="mf">.8</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Regret&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Regret Over Time&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/17360ca7493663169c893c9b68486a20102d7624efa7f39eb0dc21d7ccfa2176.png" src="../_images/17360ca7493663169c893c9b68486a20102d7624efa7f39eb0dc21d7ccfa2176.png" />
</div>
</div>
<p><strong>How to read these graphs</strong>:</p>
<ol class="arabic simple">
<li><p><strong>Cumulative reward</strong>:</p>
<ul class="simple">
<li><p>This graph shows the total reward accumulated over time by the algorithm.</p></li>
<li><p>The x-axis represents the number of rounds (time), and the y-axis represents the cumulative reward.</p></li>
<li><p>Initially, during the exploration phase, the rewards might increase slowly because the algorithm is trying out different arms to gather information.</p></li>
<li><p>As the algorithm gathers more information and starts to exploit the best-performing arm, the slope of the cumulative reward graph should increase, indicating a faster accumulation of rewards.</p></li>
<li><p>The optimal cumulative reward line represents the reward we would have accumulated if we had always chosen the best arm from the beginning. This line has a constant, steep slope, indicating the highest possible reward accumulation rate.</p></li>
</ul>
</li>
<li><p><strong>Regret</strong>:</p>
<ul class="simple">
<li><p>This graph shows the difference between the optimal cumulative reward and the cumulative reward obtained by the algorithm.</p></li>
<li><p>The x-axis represents the number of rounds (time), and the y-axis represents the regret.</p></li>
<li><p>At the beginning, the regret might increase rapidly because the algorithm is exploring and might be choosing suboptimal arms, leading to lower rewards compared to the optimal arm.</p></li>
<li><p>As the algorithm starts to exploit the best-performing arm more frequently, the rate at which regret increases should slow down. Ideally, the regret graph will start to flatten out, indicating that the algorithm is performing close to optimally.</p></li>
<li><p>The ideal scenario is to have a regret graph that flattens out as early as possible, showing that the algorithm quickly learned to choose the best arm.</p></li>
</ul>
</li>
</ol>
<p>Sure! Let’s provide a high-level intuition followed by the mathematical details for each of the three algorithms: Epsilon-Greedy, Upper Confidence Bound (UCB), and Thompson Sampling.</p>
<hr class="docutils" />
<section id="id1">
<h2><span class="section-number">28.1. </span>Multi-Armed Bandits<a class="headerlink" href="#id1" title="Link to this heading">#</a></h2>
</section>
<section id="introduction">
<h2><span class="section-number">28.2. </span>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">#</a></h2>
<p>Multi-armed bandit (MAB) problems are a class of sequential decision-making problems that model the trade-off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one-armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls. In the context of electricity markets, multi-armed bandit algorithms can be used for various purposes, such as optimizing dynamic pricing strategies, demand response programs, and marketing campaigns.</p>
</section>
<section id="relevance-in-electricity-markets">
<h2><span class="section-number">28.3. </span>Relevance in Electricity Markets<a class="headerlink" href="#relevance-in-electricity-markets" title="Link to this heading">#</a></h2>
<p>In electricity markets, decisions need to be made in real-time under uncertainty. Multi-armed bandit algorithms provide a powerful framework for optimizing these decisions by learning from past actions and balancing the trade-off between exploring new strategies and exploiting known profitable ones. For instance, they can help in:</p>
<ul class="simple">
<li><p><strong>Dynamic Pricing</strong>: Adjusting prices based on consumer behavior to maximize revenue.</p></li>
<li><p><strong>Demand Response</strong>: Determining the most effective incentives to reduce peak demand.</p></li>
<li><p><strong>Marketing Campaigns</strong>: Identifying the most effective promotional strategies to increase customer engagement.</p></li>
</ul>
</section>
<section id="mathematical-formulation">
<h2><span class="section-number">28.4. </span>Mathematical Formulation<a class="headerlink" href="#mathematical-formulation" title="Link to this heading">#</a></h2>
<p>The MAB problem can be formalized as follows:</p>
<ul class="simple">
<li><p><strong>Arms</strong>: Let <span class="math notranslate nohighlight">\(K\)</span> be the number of arms (choices or actions) available.</p></li>
<li><p><strong>Rewards</strong>: Each arm <span class="math notranslate nohighlight">\(k \in \{1, 2, \ldots, K\}\)</span> provides a reward <span class="math notranslate nohighlight">\(r_k(t)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><strong>Objective</strong>: The objective is to maximize the cumulative reward over <span class="math notranslate nohighlight">\(T\)</span> rounds, (\sum_{t=1}^{T} r_k(t)$.</p></li>
</ul>
<section id="key-concepts">
<h3><span class="section-number">28.4.1. </span>Key Concepts<a class="headerlink" href="#key-concepts" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Exploration vs. Exploitation</strong>:</p>
<ul class="simple">
<li><p><strong>Exploration</strong>: Trying out different arms to gather more information about their rewards.</p></li>
<li><p><strong>Exploitation</strong>: Choosing the arm that is currently believed to provide the highest reward.</p></li>
</ul>
</li>
<li><p><strong>Regret</strong>:</p>
<ul class="simple">
<li><p><strong>Regret</strong> is the difference between the reward obtained by the optimal arm and the reward obtained by the algorithm.</p></li>
<li><p>The goal is to minimize regret over time.</p></li>
</ul>
</li>
</ol>
</section>
<section id="algorithms">
<h3><span class="section-number">28.4.2. </span>Algorithms<a class="headerlink" href="#algorithms" title="Link to this heading">#</a></h3>
<p>Several popular algorithms have been proposed to solve MAB problems, including:</p>
</section>
<section id="epsilon-greedy">
<h3><span class="section-number">28.4.3. </span>Epsilon-Greedy<a class="headerlink" href="#epsilon-greedy" title="Link to this heading">#</a></h3>
<section id="high-level-intuition">
<h4><span class="section-number">28.4.3.1. </span>High-Level Intuition<a class="headerlink" href="#high-level-intuition" title="Link to this heading">#</a></h4>
<p>Epsilon-Greedy is a simple yet effective algorithm that balances exploration and exploitation. The basic idea is to explore a random arm with a small probability (epsilon) and exploit the best-known arm with the remaining probability.</p>
</section>
<section id="mathematical-details">
<h4><span class="section-number">28.4.3.2. </span>Mathematical Details<a class="headerlink" href="#mathematical-details" title="Link to this heading">#</a></h4>
<ul>
<li><p><strong>Mechanism</strong>:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\(\epsilon\)</span>, choose a random arm (exploration).</p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, choose the arm with the highest estimated reward (exploitation).</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>Let <span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span> be the estimated mean reward of arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p>Update rule for the estimate:</p>
<div class="amsmath math notranslate nohighlight" id="equation-b1f10d32-8ad3-4882-9015-6431c683d70c">
<span class="eqno">(28.1)<a class="headerlink" href="#equation-b1f10d32-8ad3-4882-9015-6431c683d70c" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \hat{\mu}_k(t+1) = \hat{\mu}_k(t) + \frac{r_k(t) - \hat{\mu}_k(t)}{n_k(t)}
    \end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(r_k(t)\)</span> is the observed reward and <span class="math notranslate nohighlight">\(n_k(t)\)</span> is the number of times arm <span class="math notranslate nohighlight">\(k\)</span> has been selected up to time <span class="math notranslate nohighlight">\(t\)</span>.</p>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="upper-confidence-bound-ucb">
<h3><span class="section-number">28.4.4. </span>Upper Confidence Bound (UCB)<a class="headerlink" href="#upper-confidence-bound-ucb" title="Link to this heading">#</a></h3>
<section id="id2">
<h4><span class="section-number">28.4.4.1. </span>High-Level Intuition<a class="headerlink" href="#id2" title="Link to this heading">#</a></h4>
<p>The UCB algorithm selects arms based on the principle of optimism in the face of uncertainty. It balances exploration and exploitation by choosing the arm with the highest upper confidence bound, which accounts for both the estimated reward and the uncertainty in the estimate.</p>
</section>
<section id="id3">
<h4><span class="section-number">28.4.4.2. </span>Mathematical Details<a class="headerlink" href="#id3" title="Link to this heading">#</a></h4>
<ul>
<li><p><strong>Mechanism</strong>:</p>
<ul class="simple">
<li><p>Select the arm <span class="math notranslate nohighlight">\(k\)</span> that maximizes the upper confidence bound <span class="math notranslate nohighlight">\(UCB_k(t)\)</span>.</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>The upper confidence bound for arm <span class="math notranslate nohighlight">\(k\)</span> at time <span class="math notranslate nohighlight">\(t\)</span> is given by:</p>
<div class="amsmath math notranslate nohighlight" id="equation-0ccd9ff1-517c-4ee2-9c56-4fec8c90382f">
<span class="eqno">(28.2)<a class="headerlink" href="#equation-0ccd9ff1-517c-4ee2-9c56-4fec8c90382f" title="Permalink to this equation">#</a></span>\[\begin{equation}
    UCB_k(t) = \hat{\mu}_k(t) + c \sqrt{\frac{\ln t}{n_k(t)}}
    \end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\hat{\mu}_k(t)\)</span> is the estimated mean reward, <span class="math notranslate nohighlight">\(n_k(t)\)</span> is the number of times arm <span class="math notranslate nohighlight">\(k\)</span> has been selected, and <span class="math notranslate nohighlight">\(c\)</span> is a confidence parameter.</p>
</li>
</ul>
</li>
</ul>
</section>
</section>
<section id="thompson-sampling">
<h3><span class="section-number">28.4.5. </span>Thompson Sampling<a class="headerlink" href="#thompson-sampling" title="Link to this heading">#</a></h3>
<section id="id4">
<h4><span class="section-number">28.4.5.1. </span>High-Level Intuition<a class="headerlink" href="#id4" title="Link to this heading">#</a></h4>
<p>Thompson Sampling is a Bayesian approach that balances exploration and exploitation by sampling from the posterior distribution of the reward probabilities for each arm. It chooses the arm with the highest sampled value, which inherently balances the trade-off.</p>
</section>
<section id="id5">
<h4><span class="section-number">28.4.5.2. </span>Mathematical Details<a class="headerlink" href="#id5" title="Link to this heading">#</a></h4>
<ul>
<li><p><strong>Mechanism</strong>:</p>
<ul class="simple">
<li><p>Maintain a probability distribution (posterior) for the expected reward of each arm.</p></li>
<li><p>Sample a value from the posterior distribution for each arm and select the arm with the highest sampled value.</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>Assume a Beta distribution for the reward probabilities:</p>
<div class="amsmath math notranslate nohighlight" id="equation-30c8920b-6d7f-4480-b975-c2e11b18a110">
<span class="eqno">(28.3)<a class="headerlink" href="#equation-30c8920b-6d7f-4480-b975-c2e11b18a110" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \theta_k \sim \text{Beta}(\alpha_k, \beta_k)
    \end{equation}\]</div>
<p>where <span class="math notranslate nohighlight">\(\alpha_k\)</span> and <span class="math notranslate nohighlight">\(\beta_k\)</span> are the parameters of the Beta distribution.</p>
</li>
<li><p>Update the parameters based on observed rewards:</p>
<div class="amsmath math notranslate nohighlight" id="equation-e3f2eb14-d741-4293-868e-4994c388b114">
<span class="eqno">(28.4)<a class="headerlink" href="#equation-e3f2eb14-d741-4293-868e-4994c388b114" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \alpha_k = \alpha_k + r_k(t)
    \end{equation}\]</div>
<div class="amsmath math notranslate nohighlight" id="equation-4902f598-9044-4d69-91d5-69b5e7884faa">
<span class="eqno">(28.5)<a class="headerlink" href="#equation-4902f598-9044-4d69-91d5-69b5e7884faa" title="Permalink to this equation">#</a></span>\[\begin{equation}
    \beta_k = \beta_k + 1 - r_k(t)
    \end{equation}\]</div>
</li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="example-in-python">
<h2><span class="section-number">28.5. </span>Example in Python<a class="headerlink" href="#example-in-python" title="Link to this heading">#</a></h2>
<p>Let’s implement a simple multi-armed bandit problem using the Epsilon-Greedy, UCB, Thompson Sampling, and Random strategies.</p>
<section id="step-by-step-implementation">
<h3><span class="section-number">28.5.1. </span>Step-by-Step Implementation<a class="headerlink" href="#step-by-step-implementation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Define the Environment</strong>:</p>
<ul class="simple">
<li><p>Simulate the reward probabilities for each arm.</p></li>
</ul>
</li>
<li><p><strong>Implement the Algorithms</strong>:</p>
<ul class="simple">
<li><p>Initialize the estimates and counts for each arm.</p></li>
<li><p>Iterate through multiple rounds, selecting arms based on each strategy.</p></li>
</ul>
</li>
<li><p><strong>Plot the Results</strong>:</p>
<ul class="simple">
<li><p>Visualize the cumulative rewards and the number of times each arm is selected.</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward probabilities for each arm</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Initialize parameters for all algorithms</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Upper Confidence Bound (UCB) parameters</span>
<span class="n">ucb_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">ucb_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Thompson Sampling parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Simulation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Epsilon-Greedy</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">])</span>

    <span class="c1"># UCB</span>
    <span class="n">ucb_values</span> <span class="o">=</span> <span class="n">ucb_means</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ucb_counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ucb_values</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">])</span>

    <span class="c1"># Thompson Sampling</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">alpha</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">beta</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">])</span>

    <span class="c1"># Random Sampling</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">])</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">strategy</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">strategy</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Multi-Armed Bandit: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected for each strategy</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>

<span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">strategies</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
    <span class="n">ax</span>
</pre></div>
</div>
<p>Several popular algorithms have been proposed to solve MAB problems, including:</p>
<ul class="simple">
<li><p><strong>Epsilon-Greedy</strong>: with probability <span class="math notranslate nohighlight">\(\epsilon\)</span>, choose a random arm (exploration), and with probability <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, choose the arm with the highest estimated reward (exploitation).</p></li>
<li><p><strong>Upper Confidence Bound (UCB)</strong>: selects arms based on the upper confidence bounds of their estimated rewards.</p></li>
<li><p><strong>Thompson Sampling</strong>: uses Bayesian methods to update beliefs about the reward distributions and selects arms based on the posterior probabilities.</p></li>
</ul>
<p>Sure, let’s enhance the tutorial to provide a more pedagogic introduction to the concept of exploration vs. exploitation. We’ll use a practical example to demonstrate the trade-off, and show how it impacts the decision-making process and regret.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="id6">
<h1><span class="section-number">29. </span>Multi-Armed Bandits<a class="headerlink" href="#id6" title="Link to this heading">#</a></h1>
<section id="id7">
<h2><span class="section-number">29.1. </span>Introduction<a class="headerlink" href="#id7" title="Link to this heading">#</a></h2>
<p>Multi-armed bandit (MAB) problems are a class of sequential decision-making problems that model the trade-off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one-armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls. In the context of electricity markets, multi-armed bandit algorithms can be used for various purposes, such as optimizing dynamic pricing strategies, demand response programs, and marketing campaigns.</p>
</section>
<section id="id8">
<h2><span class="section-number">29.2. </span>Relevance in Electricity Markets<a class="headerlink" href="#id8" title="Link to this heading">#</a></h2>
<p>In electricity markets, decisions need to be made in real-time under uncertainty. Multi-armed bandit algorithms provide a powerful framework for optimizing these decisions by learning from past actions and balancing the trade-off between exploring new strategies and exploiting known profitable ones. For instance, they can help in:</p>
<ul class="simple">
<li><p><strong>Dynamic Pricing</strong>: Adjusting prices based on consumer behavior to maximize revenue.</p></li>
<li><p><strong>Demand Response</strong>: Determining the most effective incentives to reduce peak demand.</p></li>
<li><p><strong>Marketing Campaigns</strong>: Identifying the most effective promotional strategies to increase customer engagement.</p></li>
</ul>
</section>
<section id="exploration-vs-exploitation">
<h2><span class="section-number">29.3. </span>Exploration vs. Exploitation<a class="headerlink" href="#exploration-vs-exploitation" title="Link to this heading">#</a></h2>
<section id="id9">
<h3><span class="section-number">29.3.1. </span>High-Level Intuition<a class="headerlink" href="#id9" title="Link to this heading">#</a></h3>
<p>Exploration vs. exploitation is the core trade-off in multi-armed bandit problems:</p>
<ul class="simple">
<li><p><strong>Exploration</strong>: Trying out different arms (choices or actions) to gather more information about their rewards.</p></li>
<li><p><strong>Exploitation</strong>: Choosing the arm that is currently believed to provide the highest reward based on the gathered information.</p></li>
</ul>
</section>
<section id="practical-example">
<h3><span class="section-number">29.3.2. </span>Practical Example<a class="headerlink" href="#practical-example" title="Link to this heading">#</a></h3>
<p>Let’s illustrate this with a simple example involving three slot machines (arms), each with a different but unknown probability of payout (reward). Initially, we don’t know which machine is the best, so we need to explore by trying out each machine. Over time, as we gather more information, we start to exploit by choosing the machine that seems to give the highest reward based on our observations.</p>
</section>
<section id="simulating-the-example">
<h3><span class="section-number">29.3.3. </span>Simulating the Example<a class="headerlink" href="#simulating-the-example" title="Link to this heading">#</a></h3>
<p>We’ll simulate a scenario with three arms, each with a different probability of payout. We’ll track how often we choose each arm and the cumulative reward over time.</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">3</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="p">[</span><span class="mf">0.3</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.7</span><span class="p">]</span>  <span class="c1"># True reward probabilities for each arm</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">100</span>

<span class="c1"># Initialize parameters</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>
<span class="n">selected_arms</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Epsilon-Greedy algorithm</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># Exploration: choose a random arm</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Exploitation: choose the best arm so far</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>
    
    <span class="c1"># Simulate pulling the arm</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Update estimates and counts</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Accumulate rewards</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>
    <span class="n">selected_arms</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">arm</span><span class="p">)</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Arm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Times Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="analysis">
<h3><span class="section-number">29.3.4. </span>Analysis<a class="headerlink" href="#analysis" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Cumulative Rewards</strong>:</p>
<ul class="simple">
<li><p>The plot shows how the total reward accumulates over time. Initially, exploration leads to variability in the cumulative reward, but over time, the algorithm converges towards the best arm, leading to more consistent rewards.</p></li>
</ul>
</li>
<li><p><strong>Number of Times Each Arm Was Selected</strong>:</p>
<ul class="simple">
<li><p>This plot illustrates the balance between exploration and exploitation. The arm with the highest estimated reward gets selected more frequently, but the other arms are also tried occasionally due to the exploration factor.</p></li>
</ul>
</li>
</ol>
</section>
<section id="understanding-regret">
<h3><span class="section-number">29.3.5. </span>Understanding Regret<a class="headerlink" href="#understanding-regret" title="Link to this heading">#</a></h3>
<p><strong>Regret</strong> is the difference between the reward obtained by always choosing the best arm and the reward obtained by the algorithm. In practical terms, regret measures the opportunity cost of not always exploiting the best option due to exploration. Minimizing regret over time is a key objective in MAB problems.</p>
</section>
<section id="id10">
<h3><span class="section-number">29.3.6. </span>Mathematical Formulation<a class="headerlink" href="#id10" title="Link to this heading">#</a></h3>
<p>The MAB problem can be formalized as follows:</p>
<ul class="simple">
<li><p><strong>Arms</strong>: Let ( K ) be the number of arms (choices or actions) available.</p></li>
<li><p><strong>Rewards</strong>: Each arm ( k \in {1, 2, \ldots, K} ) provides a reward ( r_k(t) ) at time ( t ).</p></li>
<li><p><strong>Objective</strong>: The objective is to maximize the cumulative reward over ( T ) rounds, (\sum_{t=1}^{T} r_k(t) ).</p></li>
</ul>
</section>
</section>
<section id="id11">
<h2><span class="section-number">29.4. </span>Algorithms<a class="headerlink" href="#id11" title="Link to this heading">#</a></h2>
<p>Several popular algorithms have been proposed to solve MAB problems, including:</p>
<section id="id12">
<h3><span class="section-number">29.4.1. </span>Epsilon-Greedy<a class="headerlink" href="#id12" title="Link to this heading">#</a></h3>
<section id="id13">
<h4><span class="section-number">29.4.1.1. </span>High-Level Intuition<a class="headerlink" href="#id13" title="Link to this heading">#</a></h4>
<p>Epsilon-Greedy is a simple yet effective algorithm that balances exploration and exploitation. The basic idea is to explore a random arm with a small probability (epsilon) and exploit the best-known arm with the remaining probability.</p>
</section>
<section id="id14">
<h4><span class="section-number">29.4.1.2. </span>Mathematical Details<a class="headerlink" href="#id14" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Mechanism</strong>:</p>
<ul>
<li><p>With probability ( \epsilon ), choose a random arm (exploration).</p></li>
<li><p>With probability ( 1 - \epsilon ), choose the arm with the highest estimated reward (exploitation).</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>Let ( \hat{\mu}_k(t) ) be the estimated mean reward of arm ( k ) at time ( t ).</p></li>
<li><p>Update rule for the estimate:
[
\hat{\mu}_k(t+1) = \hat{\mu}_k(t) + \frac{r_k(t) - \hat{\mu}_k(t)}{n_k(t)}
]
where ( r_k(t) ) is the observed reward and ( n_k(t) ) is the number of times arm ( k ) has been selected up to time ( t ).</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id15">
<h3><span class="section-number">29.4.2. </span>Upper Confidence Bound (UCB)<a class="headerlink" href="#id15" title="Link to this heading">#</a></h3>
<section id="id16">
<h4><span class="section-number">29.4.2.1. </span>High-Level Intuition<a class="headerlink" href="#id16" title="Link to this heading">#</a></h4>
<p>The UCB algorithm selects arms based on the principle of optimism in the face of uncertainty. It balances exploration and exploitation by choosing the arm with the highest upper confidence bound, which accounts for both the estimated reward and the uncertainty in the estimate.</p>
</section>
<section id="id17">
<h4><span class="section-number">29.4.2.2. </span>Mathematical Details<a class="headerlink" href="#id17" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Mechanism</strong>:</p>
<ul>
<li><p>Select the arm ( k ) that maximizes the upper confidence bound ( UCB_k(t) ).</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>The upper confidence bound for arm ( k ) at time ( t ) is given by:
[
UCB_k(t) = \hat{\mu}_k(t) + c \sqrt{\frac{\ln t}{n_k(t)}}
]
where ( \hat{\mu}_k(t) ) is the estimated mean reward, ( n_k(t) ) is the number of times arm ( k ) has been selected, and ( c ) is a confidence parameter.</p></li>
</ul>
</li>
</ul>
</section>
</section>
<section id="id18">
<h3><span class="section-number">29.4.3. </span>Thompson Sampling<a class="headerlink" href="#id18" title="Link to this heading">#</a></h3>
<section id="id19">
<h4><span class="section-number">29.4.3.1. </span>High-Level Intuition<a class="headerlink" href="#id19" title="Link to this heading">#</a></h4>
<p>Thompson Sampling is a Bayesian approach that balances exploration and exploitation by sampling from the posterior distribution of the reward probabilities for each arm. It chooses the arm with the highest sampled value, which inherently balances the trade-off.</p>
</section>
<section id="id20">
<h4><span class="section-number">29.4.3.2. </span>Mathematical Details<a class="headerlink" href="#id20" title="Link to this heading">#</a></h4>
<ul class="simple">
<li><p><strong>Mechanism</strong>:</p>
<ul>
<li><p>Maintain a probability distribution (posterior) for the expected reward of each arm.</p></li>
<li><p>Sample a value from the posterior distribution for each arm and select the arm with the highest sampled value.</p></li>
</ul>
</li>
<li><p><strong>Formulation</strong>:</p>
<ul>
<li><p>Assume a Beta distribution for the reward probabilities:
[
\theta_k \sim \text{Beta}(\alpha_k, \beta_k)
]
where ( \alpha_k ) and ( \beta_k ) are the parameters of the Beta distribution.</p></li>
<li><p>Update the parameters based on observed rewards:
[
\alpha_k = \alpha_k + r_k(t)
]
[
\beta_k = \beta_k + 1 - r_k(t)
]</p></li>
</ul>
</li>
</ul>
</section>
</section>
</section>
<section id="id21">
<h2><span class="section-number">29.5. </span>Example in Python<a class="headerlink" href="#id21" title="Link to this heading">#</a></h2>
<p>Let’s implement a simple multi-armed bandit problem using the Epsilon-Greedy, UCB, Thompson Sampling, and Random strategies.</p>
<section id="id22">
<h3><span class="section-number">29.5.1. </span>Step-by-Step Implementation<a class="headerlink" href="#id22" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Define the Environment</strong>:</p>
<ul class="simple">
<li><p>Simulate the reward probabilities for each arm.</p></li>
</ul>
</li>
<li><p><strong>Implement the Algorithms</strong>:</p>
<ul class="simple">
<li><p>Initialize the estimates and counts for each arm.</p></li>
<li><p>Iterate through multiple rounds, selecting arms based on each strategy.</p></li>
</ul>
</li>
<li><p><strong>Plot the Results</strong>:</p>
<ul class="simple">
<li><p>Visualize the cumulative rewards and the number of times each arm is selected.</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward probabilities for each arm</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Initialize parameters for all algorithms</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span>

<span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Upper Confidence Bound (UCB) parameters</span>
<span class="n">ucb_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">ucb_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Thompson Sampling parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Simulation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Epsilon-Greedy</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># Exploration</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>  <span class="c1"># Exploitation</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">])</span>

    <span class="c1"># UCB</span>
    <span class="n">ucb_values</span> <span class="o">=</span> <span class="n">ucb_means</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ucb_counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ucb_values</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">])</span>

    <span class="c1"># Thompson Sampling</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">alpha</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">beta</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">])</span>

    <span class="c1"># Random Sampling</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">])</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">strategy</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">strategy</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Multi-Armed Bandit: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected for each strategy</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>

<span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">strategies</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">%</span><span class="mi">2</span><span class="p">]</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Arm&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Times Selected&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="explanation">
<h3><span class="section-number">29.5.2. </span>Explanation<a class="headerlink" href="#explanation" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Environment Setup</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">true_means</span></code> defines the true reward probabilities for each arm.</p></li>
</ul>
</li>
<li><p><strong>Algorithm Implementation</strong>:</p>
<ul class="simple">
<li><p><strong>Epsilon-Greedy</strong>: Balances exploration and exploitation by choosing random arms with probability ( \epsilon ) and the best-known arm with probability ( 1 - \epsilon ).</p></li>
<li><p><strong>UCB</strong>: Selects arms based on the upper confidence bounds of their estimated rewards, balancing exploration and exploitation by considering the uncertainty in the estimates.</p></li>
<li><p><strong>Thompson Sampling</strong>: Uses Bayesian methods to update beliefs about the reward distributions and selects arms based on the posterior probabilities.</p></li>
<li><p><strong>Random Sampling</strong>: Chooses arms randomly without any learning, serving as a baseline for comparison.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>The cumulative reward plot shows how the total reward accumulates over time for each strategy.</p></li>
<li><p>The bar plots show the number of times each arm was selected by each strategy, indicating how exploration and exploitation were balanced.</p></li>
</ul>
</li>
</ol>
</section>
<section id="conclusion">
<h3><span class="section-number">29.5.3. </span>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">#</a></h3>
<p>Multi-armed bandit algorithms are powerful tools for making sequential decisions under uncertainty. By balancing exploration and exploitation, these algorithms can optimize decisions in various contexts, including dynamic pricing, demand response, and marketing campaigns in electricity markets. This tutorial demonstrated the basic principles and implementation of Epsilon-Greedy, UCB, and Thompson Sampling algorithms, providing a foundation for more advanced techniques and applications.</p>
<hr class="docutils" />
<p>This comprehensive tutorial introduces the key concepts and algorithms for solving MAB problems, using practical examples and visualizations to aid understanding.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward probabilities for each arm</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Initialize parameters for all algorithms</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="mi">0</span><span class="p">}</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;ucb&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;thompson&#39;</span><span class="p">:</span> <span class="p">[],</span> <span class="s1">&#39;random&#39;</span><span class="p">:</span> <span class="p">[]}</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>

<span class="c1"># Upper Confidence Bound (UCB) parameters</span>
<span class="n">ucb_counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">ucb_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Thompson Sampling parameters</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>

<span class="c1"># Simulation</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="c1"># Epsilon-Greedy</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">])</span>

    <span class="c1"># UCB</span>
    <span class="n">ucb_values</span> <span class="o">=</span> <span class="n">ucb_means</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="mi">2</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">t</span><span class="p">)</span> <span class="o">/</span> <span class="p">(</span><span class="n">ucb_counts</span> <span class="o">+</span> <span class="mi">1</span><span class="p">))</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">ucb_values</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">ucb_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">ucb_counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;ucb&#39;</span><span class="p">])</span>

    <span class="c1"># Thompson Sampling</span>
    <span class="n">theta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">theta</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">alpha</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">beta</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span> <span class="o">-</span> <span class="n">reward</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;thompson&#39;</span><span class="p">])</span>

    <span class="c1"># Random Sampling</span>
    <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    <span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">]</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_rewards</span><span class="p">[</span><span class="s1">&#39;random&#39;</span><span class="p">])</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="n">rewards</span><span class="p">:</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">[</span><span class="n">strategy</span><span class="p">],</span> <span class="n">label</span><span class="o">=</span><span class="n">strategy</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Multi-Armed Bandit: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected for each strategy</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">axs</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">12</span><span class="p">,</span> <span class="mi">10</span><span class="p">))</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s1">&#39;Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>

<span class="n">strategies</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;epsilon_greedy&#39;</span><span class="p">,</span> <span class="s1">&#39;ucb&#39;</span><span class="p">,</span> <span class="s1">&#39;thompson&#39;</span><span class="p">,</span> <span class="s1">&#39;random&#39;</span><span class="p">]</span>
<span class="k">for</span> <span class="n">i</span><span class="p">,</span> <span class="n">strategy</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">strategies</span><span class="p">):</span>
    <span class="n">ax</span> <span class="o">=</span> <span class="n">axs</span><span class="p">[</span><span class="n">i</span><span class="o">//</span><span class="mi">2</span><span class="p">,</span> <span class="n">i</span><span class="o">%</span><span class="k">2</span>]
    <span class="n">ax</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_xlabel</span><span class="p">(</span><span class="s1">&#39;Arm&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Times Selected&#39;</span><span class="p">)</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="n">strategy</span><span class="o">.</span><span class="n">capitalize</span><span class="p">())</span>
    <span class="n">ax</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">tight_layout</span><span class="p">(</span><span class="n">rect</span><span class="o">=</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="mf">0.03</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mf">0.95</span><span class="p">])</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/abd4a66b64178f4ea96cba188b93c267445174cb58f7988a41ee5b10683dfb2d.png" src="../_images/abd4a66b64178f4ea96cba188b93c267445174cb58f7988a41ee5b10683dfb2d.png" />
<img alt="../_images/96b5d787c2a4c53d671eeb7abfb85371b0c5f9cd841a2b79b93011b13f12d245.png" src="../_images/96b5d787c2a4c53d671eeb7abfb85371b0c5f9cd841a2b79b93011b13f12d245.png" />
</div>
</div>
<p>Sure! Let’s create a comprehensive chapter on multi-armed bandits.</p>
</section>
</section>
</section>
<hr class="docutils" />
<section class="tex2jax_ignore mathjax_ignore" id="id23">
<h1><span class="section-number">30. </span>Multi-Armed Bandits<a class="headerlink" href="#id23" title="Link to this heading">#</a></h1>
<section id="id24">
<h2><span class="section-number">30.1. </span>Introduction<a class="headerlink" href="#id24" title="Link to this heading">#</a></h2>
<p>Multi-armed bandit (MAB) problems are a class of sequential decision-making problems that model the trade-off between exploration and exploitation. The name comes from the metaphor of a gambler facing multiple slot machines (one-armed bandits), each with a different probability of payout. The gambler’s objective is to maximize their total reward over a series of pulls. In the context of electricity markets, multi-armed bandit algorithms can be used for various purposes, such as optimizing dynamic pricing strategies, demand response programs, and marketing campaigns.</p>
</section>
<section id="id25">
<h2><span class="section-number">30.2. </span>Relevance in Electricity Markets<a class="headerlink" href="#id25" title="Link to this heading">#</a></h2>
<p>In electricity markets, decisions need to be made in real-time under uncertainty. Multi-armed bandit algorithms provide a powerful framework for optimizing these decisions by learning from past actions and balancing the trade-off between exploring new strategies and exploiting known profitable ones. For instance, they can help in:</p>
<ul class="simple">
<li><p><strong>Dynamic Pricing</strong>: Adjusting prices based on consumer behavior to maximize revenue.</p></li>
<li><p><strong>Demand Response</strong>: Determining the most effective incentives to reduce peak demand.</p></li>
<li><p><strong>Marketing Campaigns</strong>: Identifying the most effective promotional strategies to increase customer engagement.</p></li>
</ul>
</section>
<section id="id26">
<h2><span class="section-number">30.3. </span>Mathematical Formulation<a class="headerlink" href="#id26" title="Link to this heading">#</a></h2>
<p>The MAB problem can be formalized as follows:</p>
<ul class="simple">
<li><p><strong>Arms</strong>: Let <span class="math notranslate nohighlight">\(K\)</span> be the number of arms (choices or actions) available.</p></li>
<li><p><strong>Rewards</strong>: Each arm <span class="math notranslate nohighlight">\(k \in \{1, 2, \ldots, K\}\)</span> provides a reward <span class="math notranslate nohighlight">\(r_k(t)\)</span> at time <span class="math notranslate nohighlight">\(t\)</span>.</p></li>
<li><p><strong>Objective</strong>: The objective is to maximize the cumulative reward over <span class="math notranslate nohighlight">\(T\)</span> rounds, (\sum_{t=1}^{T} r_k(t)$.</p></li>
</ul>
<section id="id27">
<h3><span class="section-number">30.3.1. </span>Key Concepts<a class="headerlink" href="#id27" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Exploration vs. Exploitation</strong>:</p>
<ul class="simple">
<li><p><strong>Exploration</strong>: Trying out different arms to gather more information about their rewards.</p></li>
<li><p><strong>Exploitation</strong>: Choosing the arm that is currently believed to provide the highest reward.</p></li>
</ul>
</li>
<li><p><strong>Regret</strong>:</p>
<ul class="simple">
<li><p><strong>Regret</strong> is the difference between the reward obtained by the optimal arm and the reward obtained by the algorithm.</p></li>
<li><p>The goal is to minimize regret over time.</p></li>
</ul>
</li>
</ol>
</section>
<section id="id28">
<h3><span class="section-number">30.3.2. </span>Algorithms<a class="headerlink" href="#id28" title="Link to this heading">#</a></h3>
<p>Several algorithms can solve MAB problems, including:</p>
<ul class="simple">
<li><p><strong>Epsilon-Greedy</strong>: With probability <span class="math notranslate nohighlight">\(\epsilon\)</span>, choose a random arm (exploration), and with probability <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, choose the arm with the highest estimated reward (exploitation).</p></li>
<li><p><strong>Upper Confidence Bound (UCB)</strong>: Selects arms based on the upper confidence bounds of their estimated rewards.</p></li>
<li><p><strong>Thompson Sampling</strong>: Uses Bayesian methods to update beliefs about the reward distributions and selects arms based on the posterior probabilities.</p></li>
</ul>
</section>
</section>
<section id="id29">
<h2><span class="section-number">30.4. </span>Example in Python<a class="headerlink" href="#id29" title="Link to this heading">#</a></h2>
<p>Let’s implement a simple multi-armed bandit problem using the Epsilon-Greedy algorithm.</p>
<section id="id30">
<h3><span class="section-number">30.4.1. </span>Step-by-Step Implementation<a class="headerlink" href="#id30" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Define the Environment</strong>:</p>
<ul class="simple">
<li><p>Simulate the reward probabilities for each arm.</p></li>
</ul>
</li>
<li><p><strong>Implement the Epsilon-Greedy Algorithm</strong>:</p>
<ul class="simple">
<li><p>Initialize the estimates and counts for each arm.</p></li>
<li><p>Iterate through multiple rounds, selecting arms based on the epsilon-greedy strategy.</p></li>
</ul>
</li>
<li><p><strong>Plot the Results</strong>:</p>
<ul class="simple">
<li><p>Visualize the cumulative rewards and the number of times each arm is selected.</p></li>
</ul>
</li>
</ol>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward probabilities for each arm</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Initialize estimates and counts</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Epsilon-Greedy algorithm</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># Exploration: choose a random arm</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Exploitation: choose the best arm so far</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>
    
    <span class="c1"># Simulate pulling the arm</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Update estimates and counts</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Accumulate rewards</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Arm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Times Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</section>
<section id="id31">
<h3><span class="section-number">30.4.2. </span>Explanation<a class="headerlink" href="#id31" title="Link to this heading">#</a></h3>
<ol class="arabic simple">
<li><p><strong>Environment Setup</strong>:</p>
<ul class="simple">
<li><p><code class="docutils literal notranslate"><span class="pre">true_means</span></code> defines the true reward probabilities for each arm.</p></li>
</ul>
</li>
<li><p><strong>Epsilon-Greedy Algorithm</strong>:</p>
<ul class="simple">
<li><p>With probability <span class="math notranslate nohighlight">\(\epsilon\)</span>, the algorithm explores by choosing a random arm.</p></li>
<li><p>With probability <span class="math notranslate nohighlight">\(1 - \epsilon\)</span>, the algorithm exploits by choosing the arm with the highest estimated reward.</p></li>
<li><p>The algorithm updates the reward estimates and keeps track of the total reward and the number of times each arm is selected.</p></li>
</ul>
</li>
<li><p><strong>Visualization</strong>:</p>
<ul class="simple">
<li><p>The cumulative reward plot shows how the total reward accumulates over time.</p></li>
<li><p>The bar plot shows the number of times each arm was selected, indicating the balance between exploration and exploitation.</p></li>
</ul>
</li>
</ol>
</section>
<section id="id32">
<h3><span class="section-number">30.4.3. </span>Conclusion<a class="headerlink" href="#id32" title="Link to this heading">#</a></h3>
<p>Multi-armed bandit algorithms are powerful tools for making sequential decisions under uncertainty. By balancing exploration and exploitation, these algorithms can optimize decisions in various contexts, including dynamic pricing, demand response, and marketing campaigns in electricity markets. This tutorial demonstrated the basic principles and implementation of the Epsilon-Greedy algorithm, providing a foundation for more advanced techniques and applications.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the environment</span>
<span class="n">n_arms</span> <span class="o">=</span> <span class="mi">5</span>
<span class="n">true_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>  <span class="c1"># True reward probabilities for each arm</span>

<span class="c1"># Epsilon-Greedy parameters</span>
<span class="n">epsilon</span> <span class="o">=</span> <span class="mf">0.1</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>

<span class="c1"># Initialize estimates and counts</span>
<span class="n">estimated_means</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">counts</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
<span class="n">total_reward</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="p">[]</span>

<span class="c1"># Epsilon-Greedy algorithm</span>
<span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="n">n_rounds</span> <span class="o">+</span> <span class="mi">1</span><span class="p">):</span>
    <span class="k">if</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">epsilon</span><span class="p">:</span>
        <span class="c1"># Exploration: choose a random arm</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">choice</span><span class="p">(</span><span class="n">n_arms</span><span class="p">)</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="c1"># Exploitation: choose the best arm so far</span>
        <span class="n">arm</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">estimated_means</span><span class="p">)</span>
    
    <span class="c1"># Simulate pulling the arm</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">rand</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">true_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Update estimates and counts</span>
    <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="o">+=</span> <span class="p">(</span><span class="n">reward</span> <span class="o">-</span> <span class="n">estimated_means</span><span class="p">[</span><span class="n">arm</span><span class="p">])</span> <span class="o">/</span> <span class="n">counts</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span>
    
    <span class="c1"># Accumulate rewards</span>
    <span class="n">total_reward</span> <span class="o">+=</span> <span class="n">reward</span>
    <span class="n">rewards</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">total_reward</span><span class="p">)</span>

<span class="c1"># Plot cumulative rewards</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">rewards</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Round&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Cumulative Reward&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Cumulative Reward Over Time&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Plot the number of times each arm was selected</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="n">plt</span><span class="o">.</span><span class="n">bar</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="n">n_arms</span><span class="p">),</span> <span class="n">counts</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Arm&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Number of Times Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Epsilon-Greedy: Number of Times Each Arm Was Selected&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">grid</span><span class="p">(</span><span class="kc">True</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/a595289c70cf388a178ac2a5a40d9753619ce8d44ac596448a4fddc23a377ab6.png" src="../_images/a595289c70cf388a178ac2a5a40d9753619ce8d44ac596448a4fddc23a377ab6.png" />
<img alt="../_images/73c81ead8fa3a6fcec8cf623784654b305eac70541e954be24f4f10a0d22f115.png" src="../_images/73c81ead8fa3a6fcec8cf623784654b305eac70541e954be24f4f10a0d22f115.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">random</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="c1"># Define the arms (e.g., product IDs or recommendation strategies)</span>
<span class="n">arms</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;Product A&#39;</span><span class="p">,</span> <span class="s1">&#39;Product B&#39;</span><span class="p">,</span> <span class="s1">&#39;Product C&#39;</span><span class="p">]</span>

<span class="c1"># Function to simulate user interaction and reward (1 if purchased, 0 otherwise)</span>
<span class="k">def</span> <span class="nf">simulate_reward</span><span class="p">(</span><span class="n">arm</span><span class="p">):</span>
    <span class="c1"># Simulated reward probabilities for each product</span>
    <span class="n">reward_probabilities</span> <span class="o">=</span> <span class="p">{</span><span class="s1">&#39;Product A&#39;</span><span class="p">:</span> <span class="mf">0.1</span><span class="p">,</span> <span class="s1">&#39;Product B&#39;</span><span class="p">:</span> <span class="mf">0.15</span><span class="p">,</span> <span class="s1">&#39;Product C&#39;</span><span class="p">:</span> <span class="mf">0.05</span><span class="p">}</span>
    <span class="k">return</span> <span class="mi">1</span> <span class="k">if</span> <span class="n">random</span><span class="o">.</span><span class="n">random</span><span class="p">()</span> <span class="o">&lt;</span> <span class="n">reward_probabilities</span><span class="p">[</span><span class="n">arm</span><span class="p">]</span> <span class="k">else</span> <span class="mi">0</span>

<span class="c1"># Initialize priors (alpha and beta) for each arm</span>
<span class="n">alpha</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>
<span class="n">beta</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>

<span class="c1"># Function to select an arm using Thompson Sampling</span>
<span class="k">def</span> <span class="nf">thompson_sampling</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">):</span>
    <span class="n">samples</span> <span class="o">=</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">])</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))]</span>
    <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">samples</span><span class="p">)</span>

<span class="c1"># Simulate rounds of experimentation</span>
<span class="n">n_rounds</span> <span class="o">=</span> <span class="mi">1000</span>
<span class="n">rewards</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))</span>

<span class="k">for</span> <span class="n">_</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_rounds</span><span class="p">):</span>
    <span class="n">chosen_arm</span> <span class="o">=</span> <span class="n">thompson_sampling</span><span class="p">(</span><span class="n">alpha</span><span class="p">,</span> <span class="n">beta</span><span class="p">)</span>
    <span class="n">reward</span> <span class="o">=</span> <span class="n">simulate_reward</span><span class="p">(</span><span class="n">arms</span><span class="p">[</span><span class="n">chosen_arm</span><span class="p">])</span>
    
    <span class="c1"># Update priors based on observed reward</span>
    <span class="k">if</span> <span class="n">reward</span> <span class="o">==</span> <span class="mi">1</span><span class="p">:</span>
        <span class="n">alpha</span><span class="p">[</span><span class="n">chosen_arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">else</span><span class="p">:</span>
        <span class="n">beta</span><span class="p">[</span><span class="n">chosen_arm</span><span class="p">]</span> <span class="o">+=</span> <span class="mi">1</span>
    
    <span class="n">rewards</span><span class="p">[</span><span class="n">chosen_arm</span><span class="p">]</span> <span class="o">+=</span> <span class="n">reward</span>

<span class="c1"># Print results</span>
<span class="n">total_rewards</span> <span class="o">=</span> <span class="n">rewards</span>
<span class="n">priors_alpha</span> <span class="o">=</span> <span class="n">alpha</span>
<span class="n">priors_beta</span> <span class="o">=</span> <span class="n">beta</span>

<span class="c1"># Plot the posterior distributions</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1000</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">figure</span><span class="p">(</span><span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mi">6</span><span class="p">))</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">)):</span>
    <span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">beta</span><span class="p">(</span><span class="n">alpha</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">beta</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">size</span><span class="o">=</span><span class="mi">1000</span><span class="p">),</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;Arm </span><span class="si">{</span><span class="n">arms</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">title</span><span class="p">(</span><span class="s1">&#39;Posterior Distributions of Arms&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">xlabel</span><span class="p">(</span><span class="s1">&#39;Reward Probability&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylabel</span><span class="p">(</span><span class="s1">&#39;Density&#39;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">()</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1"># Calculate the mean reward for each arm</span>
<span class="n">mean_rewards</span> <span class="o">=</span> <span class="n">rewards</span> <span class="o">/</span> <span class="p">(</span><span class="n">alpha</span> <span class="o">+</span> <span class="n">beta</span> <span class="o">-</span> <span class="mi">2</span><span class="p">)</span>
<span class="n">mean_rewards_per_arm</span> <span class="o">=</span> <span class="p">{</span><span class="n">arms</span><span class="p">[</span><span class="n">i</span><span class="p">]:</span> <span class="n">mean_rewards</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">arms</span><span class="p">))}</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Mean Rewards Per Arm:&quot;</span><span class="p">,</span> <span class="n">mean_rewards_per_arm</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Total Rewards:&quot;</span><span class="p">,</span> <span class="n">total_rewards</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Priors (Alpha):&quot;</span><span class="p">,</span> <span class="n">priors_alpha</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;Priors (Beta):&quot;</span><span class="p">,</span> <span class="n">priors_beta</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/f5d7e76d96f86d0a79cd9866e301e232a05e5e8a56a54afea5fa138018b3503e.png" src="../_images/f5d7e76d96f86d0a79cd9866e301e232a05e5e8a56a54afea5fa138018b3503e.png" />
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>Mean Rewards Per Arm: {&#39;Product A&#39;: 0.0, &#39;Product B&#39;: 0.16862326574172892, &#39;Product C&#39;: 0.02702702702702703}
Total Rewards: [  0. 158.   1.]
Priors (Alpha): [  1. 159.   2.]
Priors (Beta): [ 27. 780.  37.]
</pre></div>
</div>
</div>
</div>
</section>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./notebooks"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  
<div class="prev-next-area">
    <a class="left-prev"
       href="AB_testing.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title"><span class="section-number">27. </span>A/B Testing</p>
      </div>
    </a>
    <a class="right-next"
       href="design_of_experiments.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title"><span class="section-number">31. </span>Design of Experiments</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">


  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#">28. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id1">28.1. Multi-Armed Bandits</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#introduction">28.2. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#relevance-in-electricity-markets">28.3. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-formulation">28.4. Mathematical Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#key-concepts">28.4.1. Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#algorithms">28.4.2. Algorithms</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#epsilon-greedy">28.4.3. Epsilon-Greedy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#high-level-intuition">28.4.3.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#mathematical-details">28.4.3.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#upper-confidence-bound-ucb">28.4.4. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id2">28.4.4.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id3">28.4.4.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#thompson-sampling">28.4.5. Thompson Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id4">28.4.5.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id5">28.4.5.2. Mathematical Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#example-in-python">28.5. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#step-by-step-implementation">28.5.1. Step-by-Step Implementation</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id6">29. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id7">29.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id8">29.2. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#exploration-vs-exploitation">29.3. Exploration vs. Exploitation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id9">29.3.1. High-Level Intuition</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#practical-example">29.3.2. Practical Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#simulating-the-example">29.3.3. Simulating the Example</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#analysis">29.3.4. Analysis</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#understanding-regret">29.3.5. Understanding Regret</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id10">29.3.6. Mathematical Formulation</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id11">29.4. Algorithms</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id12">29.4.1. Epsilon-Greedy</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id13">29.4.1.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id14">29.4.1.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id15">29.4.2. Upper Confidence Bound (UCB)</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id16">29.4.2.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id17">29.4.2.2. Mathematical Details</a></li>
</ul>
</li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id18">29.4.3. Thompson Sampling</a><ul class="nav section-nav flex-column">
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id19">29.4.3.1. High-Level Intuition</a></li>
<li class="toc-h4 nav-item toc-entry"><a class="reference internal nav-link" href="#id20">29.4.3.2. Mathematical Details</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id21">29.5. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id22">29.5.1. Step-by-Step Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#explanation">29.5.2. Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#conclusion">29.5.3. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
<li class="toc-h1 nav-item toc-entry"><a class="reference internal nav-link" href="#id23">30. Multi-Armed Bandits</a><ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id24">30.1. Introduction</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id25">30.2. Relevance in Electricity Markets</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id26">30.3. Mathematical Formulation</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id27">30.3.1. Key Concepts</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id28">30.3.2. Algorithms</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#id29">30.4. Example in Python</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id30">30.4.1. Step-by-Step Implementation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id31">30.4.2. Explanation</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#id32">30.4.3. Conclusion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Davide Cacciarelli
</p>

  </div>
  
  <div class="footer-item">
    

  <p class="copyright">
    
      © Copyright 2023.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="../_static/scripts/bootstrap.js?digest=8d27b9dea8ad943066ae"></script>
<script src="../_static/scripts/pydata-sphinx-theme.js?digest=8d27b9dea8ad943066ae"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>